Figure 2.2: Data Movement across Buses between Components

It’s a Multiprocessing System Architecture

There is a further thought to be made here outside of “that’s nice.” When we
look at even a single-processor, single-core, single-threaded system, you
should realize that we have a multiprocessing environment even if there is
just one CPU. Typically, several of these data flows are happening

concurrently:
Figure two point two: Data Movement across Buses between Components.

The diagram, titled "Figure two point two: Data Movement across Buses between Components," illustrates a typical computer system architecture. At the top, a `C P U` is connected to a central `North Bridge` component via a `C P U Bus`. From the `North Bridge`, a `Hublink Bus` connects to what appear to be `Ram` modules. An `A G P Device` is also connected to the `North Bridge` via an `A G P Bus`, with data flow indicated by curved arrows between them. The `North Bridge` is further connected to a `South Bridge`. From the `South Bridge`, a `P C I Bus` extends, connecting to a `Network Card` and a `Sound Card`, with data movement depicted by curved arrows. Additionally, an `L P C Bus` connects the `South Bridge` to an `S I O` component.

It's a Multiprocessing System Architecture.

There is a further thought to be made here outside of "that's nice." When we look at even a single processor, single core, single threaded system, you should realize that we have a multiprocessing environment even if there is just one `C P U`. Typically, several of these data flows are happening concurrently.
The presented architectural diagram illustrates a classic personal computer system's core interconnect structure, emphasizing the pathways for data movement between various components via a hierarchy of buses. At the apex of this hierarchy, positioned in the upper central region, is the C P U, or Central Processing Unit, which communicates with the system's primary interconnect component, the North Bridge, via a dedicated C P U Bus. This C P U Bus, historically known as the Front Side Bus or F S B, is characterized by its high speed and bandwidth, crucial for the C P U's rapid access to main memory and other performance-critical resources managed by the North Bridge.

The North Bridge, depicted centrally in the upper half of the diagram, acts as the primary memory controller and high-speed I O hub. It is responsible for mediating communication between the C P U, the main system Ram, and the A G P Device. The Ram modules, shown to the right of the North Bridge, are connected directly to it, signifying the North Bridge's integral role as the memory controller, ensuring efficient data transfers between the C P U and the system's volatile storage. Below the C P U Bus, extending to the left from the North Bridge, is the A G P Bus, a specialized high-speed interface designed to provide a direct, dedicated data path for graphics processing units, often referred to as A G P Devices, to access system memory with minimal latency. This direct connection bypasses the general purpose P C I bus, offering superior performance for graphical applications that require intensive data streaming.

Below the North Bridge, located centrally in the lower half of the diagram, is the South Bridge. These two chipset components, the North Bridge and South Bridge, are interconnected by a high-speed proprietary link, labeled here as a Hublink Bus. This link facilitates data exchange between the high-speed components managed by the North Bridge and the slower peripherals handled by the South Bridge. The South Bridge functions as an I O Controller Hub, managing a wide array of slower peripherals and I O interfaces. Extending to the right from the South Bridge is the P C I Bus, a general purpose expansion bus to which various peripheral devices, such as a Network Card and a Sound Card, are connected. The P C I Bus supports multiple devices, allowing them to share its bandwidth, which is significantly lower than that of the C P U Bus or A G P Bus. These devices request data transfers from the C P U or Ram, which are then arbitrated and routed through the South Bridge and North Bridge.

Further extending downwards from the South Bridge is the L P C Bus, or Low Pin Count Bus. This bus is designed to connect even slower, legacy I O devices, such as the S I O, or Super I O chip, which typically handles peripheral functions like serial ports, parallel ports, and keyboard and mouse controllers. The hierarchical bus architecture, with dedicated high-speed buses for critical components and general-purpose buses for a multitude of peripherals, is a fundamental design principle for managing the diverse performance requirements of system components while minimizing contention.

The overarching principle demonstrated by this architectural model is that of a multiprocessing environment, even in systems with a single C P U. While a single-processor, single-core, single-threaded C P U executes instructions sequentially, the system as a whole is capable of extensive concurrency at the hardware level. Multiple data flows occur simultaneously across different buses and through different components. For instance, the C P U might be fetching data from Ram via the C P U Bus, while concurrently, the A G P Device is performing direct memory access over the A G P Bus to update a frame buffer, and a Network Card is receiving data packets over the P C I Bus, which are then buffered and eventually processed by the C P U. This concurrent activity necessitates sophisticated bus arbitration mechanisms, interrupt handling, and direct memory access controllers within the North and South Bridges to efficiently manage resource sharing and prevent bottlenecks, ensuring that various system operations can proceed in parallel without significant performance degradation. The architectural design aims to balance the need for high-speed access for performance-critical components with the requirement to support a broad range of slower peripheral devices, all while managing the inherent concurrency of multiple, independent data transfer operations.
