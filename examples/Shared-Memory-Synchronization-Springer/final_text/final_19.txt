
Read Copy Update, often abbreviated as R C U, is a sophisticated method for managing concurrent access to shared data in multi-processor systems, particularly in environments where reading operations are far more frequent than writing operations. At its heart, R C U is a non-blocking synchronization technique that allows multiple readers to access shared data simultaneously without the need for traditional locking mechanisms, which can introduce contention and reduce performance. This approach significantly lowers the overhead for readers, making it an ideal choice for systems where data is predominantly read rather than modified.

The core idea behind R C U is the concept of a grace period. When a writer wants to update a data structure, it does not modify the existing data directly. Instead, it creates a new version of the data structure, applies its changes to this new version, and then atomically replaces the pointer to the old version with a pointer to the new one. This replacement is done in such a way that it appears to happen instantaneously from the perspective of other threads. However, the old version of the data cannot be immediately discarded because there may still be readers that are in the process of accessing it. The system must wait for a grace period to elapse, during which all readers that were active at the time of the update have completed their operations. Only after this grace period can the old data be safely reclaimed, either through garbage collection or explicit memory deallocation.

To enforce this grace period, a common implementation uses a global counter, often referred to as C, and a set of per-thread counters, stored in an array S indexed by thread identifier. The global counter C is incremented each time a writer completes an update, effectively serving as a timestamp for the latest modification. Each thread maintains an entry in the S array that reflects the value of C when it began its read operation. If a thread is not currently performing a read, its entry in S is set to zero. When a writer needs to ensure that all previous readers have completed their access to the old data, it iterates through the S array, waiting for each entry to either be zero or to have a value greater than or equal to the current value of C. This waiting period constitutes the grace period and ensures that no reader is still accessing the old version of the data.

An important optimization in this design is the placement of each entry in the S array on a separate cache line. This prevents false sharing, a phenomenon where unrelated data items residing in the same cache line cause unnecessary cache invalidations across processors. By isolating each thread's counter in its own cache line, the system minimizes cache coherence traffic and improves overall performance.

Memory ordering is another critical aspect of R C U, especially on modern processors that employ weak memory models. When a thread starts or ends a read operation, it must update its corresponding entry in the S array. This update must be performed in a way that ensures proper memory ordering, typically by using memory barriers or fences. For example, when a thread begins a read, it may use a Read Or Store operation or follow the update with a Write Or Read fence to ensure that the update to S is visible to other threads before the read operation proceeds. Similarly, when a thread finishes a read, it updates its S entry, often using a Read Or Write store or by following the update with a Read Or Read fence. These fences ensure that the writer can correctly observe the reader's entry and exit from its critical section.

One of the most expensive operations in this context is the Write Or Read fence, which enforces strict ordering between memory writes and reads across all processors. This type of fence often involves a global memory barrier that flushes write buffers and ensures that all memory operations are visible to other processors. To avoid the performance cost of this fence in common scenarios, some implementations use alternative mechanisms, such as POSIX signals, to interrupt readers and achieve the grace period through operating system-level inter-process communication rather than direct memory ordering constraints.

Read Copy Update is particularly effective for data structures that can be updated by changing a single pointer. For example, in a linked list, a writer can create a new node, update its pointers to reflect the desired changes, and then atomically replace the pointer to the old node with a pointer to the new one. Readers that were already traversing the list can continue to access the old version until the grace period has expired, at which point the old node can be safely reclaimed. This approach ensures that readers incur minimal overhead while maintaining consistency and correctness.

However, R C U can also be extended to more complex data structures, such as trees, where multiple pointers may need to be updated simultaneously. In such cases, the writer must carefully manage the updates to ensure that readers always see a consistent view of the data. For example, when rebalancing a binary tree, the writer can create a new version of the affected subtree, apply the necessary changes, and then atomically replace the pointer to the old subtree with a pointer to the new one. This technique, known as subtree replacement, allows the writer to make complex changes without blocking readers, who can continue to traverse the old version of the tree until the grace period has expired.

One of the key trade-offs in using R C U is the overhead it imposes on writers. Because writers must create new copies of data structures and wait for grace periods before reclaiming old memory, the cost of writing can be significantly higher than the cost of reading. This overhead is particularly noticeable in scenarios where writes are frequent or where the data structures being modified are large. To mitigate this, some systems combine R C U with other synchronization techniques, such as sequence locks, to balance the overhead between readers and writers.

Sequence locks are a type of synchronization primitive that allows readers to check whether a write has occurred during their read operation. If a write has occurred, the reader can retry its operation using the updated data. This approach is particularly useful in scenarios where readers may need to access multiple related data items and where the order of access is important. By combining R C U with sequence locks, systems can achieve a balance between low reader overhead and efficient writer updates.

A practical example of R C U in action is its use in managing page tables in operating system kernels. Page tables are critical data structures that map virtual memory addresses to physical memory addresses. When a page fault occurs, the operating system must update the page table to establish the correct mapping. Using R C U, the operating system can update the page table without blocking other threads that are performing memory accesses. The writer creates a new version of the page table, applies the necessary changes, and then atomically replaces the pointer to the old page table with a pointer to the new one. Readers that were already traversing the page table can continue to use the old version until the grace period has expired, at which point the old page table can be safely reclaimed.

In some cases, a fault handler may need to modify more than a single word in memory, which can introduce additional synchronization challenges. For example, if a fault handler is modifying a page table entry while a major update, such as an munmap operation, is in progress, it may end up modifying an obsolete version of the data. To handle this, the fault handler can check a sequence lock before and after its operation. If the sequence lock has changed, it indicates that a writer has modified the data, and the fault handler can retry its operation using the updated data. Alternatively, if the fault handler cannot safely retry, it can temporarily acquire the writer's sequence lock to ensure a stable view of the data.

Ultimately, Read Copy Update provides a powerful mechanism for managing concurrent access to shared data in systems where reads are the dominant operation. By allowing readers to proceed without explicit locks and deferring the cost of synchronization to writers, R C U achieves high scalability and performance. However, it also introduces complexity in managing memory reclamation and ensuring consistency across concurrent operations. When used appropriately, R C U can significantly improve the performance of concurrent systems, particularly in environments where data is read far more frequently than it is modified.
