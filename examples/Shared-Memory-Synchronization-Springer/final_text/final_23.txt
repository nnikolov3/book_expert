
Operating systems must manage how multiple threads interact with shared resources, especially when those threads are running concurrently. One of the core challenges in this domain is ensuring that threads can access shared data without interfering with each other in ways that lead to inconsistency or deadlock. This is where synchronization mechanisms like spin locks and atomic operations come into play.

Let’s begin with a specific example of a spin lock that uses two flags to coordinate between a user thread and the kernel. The first flag, known as the “do not preempt me” flag, is set by the user thread to indicate that it does not want to be interrupted. This is important when the thread is in a critical section of code—such as when it is trying to acquire a lock—and cannot afford to be paused or switched out by the operating system. The second flag, known as the “kernel wanted to preempt me” flag, is set by the kernel. This flag tells the user thread that the kernel had intended to interrupt it, but did not do so because the first flag was already active.

Now, imagine a thread trying to acquire a spin lock. The process begins by setting the “do not preempt me” flag to true. This signals to the kernel that the thread is in a sensitive state and should not be interrupted. Then, the thread enters a loop where it repeatedly attempts to acquire the lock using a test and set operation. The test and set operation is an atomic instruction that checks whether the lock is available and, if so, marks it as taken in a single, uninterruptible step.

The loop continues as long as the test and set operation fails to acquire the lock. This failure means that another thread is currently holding the lock. While the thread is spinning—repeatedly checking for the lock to become available—it keeps the “do not preempt me” flag set to true. This ensures that the kernel does not interrupt the thread during this busy-waiting phase.

However, there is a possibility that the kernel might still want to interrupt the thread, perhaps to give CPU time to another thread or to handle a higher-priority task. If the kernel does attempt to interrupt the thread, it sets the “kernel wanted to preempt me” flag to true. When the spinning thread detects this, it voluntarily yields the CPU. This is a form of cooperative multitasking, where the thread gives up its current time slice to allow other threads to run. This helps the system scheduler make better decisions about which thread should run next.

This mechanism is designed to balance the needs of the user thread, which wants to avoid interruption while waiting for a lock, and the kernel, which must manage system-wide scheduling and fairness. However, this basic model has limitations. For instance, if a thread holds a lock for too long, it can cause other threads to wait indefinitely, potentially leading to system unresponsiveness. To address this, researchers have proposed enhancements such as Solaris’s schedctl interface, which allows the kernel to make more nuanced scheduling decisions based on the behavior of threads.

Another important consideration is what happens when a thread that is holding a lock gets interrupted. If the thread is paused for a long time, other threads waiting for the lock can be blocked indefinitely. Kontothanassis and colleagues proposed extending the kernel interface to allow a thread to pass a lock to another thread atomically. This means that the lock can be transferred directly from one thread to another without going through an intermediate unlocked state, reducing the chances of contention and improving performance.

He and others expanded on this idea by introducing queue-based locks. In these systems, when a thread releases a lock, it can estimate how long the next thread in line has been waiting. If the waiting time exceeds a certain threshold, it suggests that the next thread might have been interrupted or is otherwise unavailable. In such cases, the lock is not passed directly, and the releasing thread can either resume its own execution or attempt to pass the lock to the next eligible thread in the queue.

This approach helps reduce lock contention and improves overall system efficiency by ensuring that locks are only passed to threads that are likely to be ready to use them. It also helps prevent situations where a thread is preempted while holding a lock, which can cause other threads to stall.

Now, let’s shift focus to another important aspect of synchronization: minimizing the use of kernel resources. In traditional locking mechanisms, every lock requires a kernel-level condition queue to manage threads that are waiting for the lock. However, this can be inefficient, especially when many locks are created but rarely used.

Linux’s futexes and Solaris’s lwp_park-based mutexes address this by keeping lists of blocked threads in user-level memory. When a thread waits on a lock, it is simply descheduled by the kernel, and a note is made in its context block indicating what it is waiting for. This reduces the amount of kernel memory required for synchronization and allows the system to support a much larger number of locks.

The designers of the NT kernel, which underlies Microsoft’s operating systems starting with Windows two thousand, recognized the importance of conserving kernel resources. Unlike Unix-based systems, which tend to allocate kernel structures only when necessary, the Windows API includes a large number of standard library routines that declare internal locks. Many of these locks are never actually used in a typical program, so allocating kernel resources for them upfront would be wasteful.

To solve this, the NT kernel delays allocating a kernel queue for a lock until a thread actually tries to acquire it. This means that kernel memory is only used for locks that are actively being contested. However, this approach introduces a new problem: if the kernel runs out of memory when trying to allocate a queue, it can result in a run-time exception. This was a significant issue in Windows two thousand, where such exceptions could lead to system instability.

Windows XP introduced a solution called keyed events. These allow multiple logically distinct conditions to share a single kernel-level queue. Each call to wait or set must specify both an event and a thirty-two-bit key. Threads waiting in the queue are tagged with the key they provided, and a set operation will only awaken a thread with a matching key. This reduces the number of kernel queues needed and avoids run-time exceptions when memory is low.

In Windows Vista, this system was further improved by replacing the linked list used for per-address-space queues with a hash table. This allowed for faster lookups based on the key, improving performance. Vista also introduced a new family of synchronization objects, including the slim reader-writer lock, or SRWL. Like futexes and lwp_park-based mutexes, these objects keep their state in user-level memory and avoid entering kernel mode unless absolutely necessary.

When a thread does need to block, it always uses the per-address-space queue, which helps reduce contention and improve scalability. The overall goal of these techniques is to minimize the overhead associated with synchronization, especially in systems with many concurrent threads.

Now, let’s move on to nonblocking algorithms, which offer an alternative to traditional lock-based synchronization. These algorithms aim to avoid the problems associated with locks, such as deadlock, livelock, and indefinite blocking. Instead of using locks, they rely on atomic operations like Compare And Swap, or CAS, to ensure that concurrent updates to shared data are performed correctly.

A simple example of a nonblocking algorithm is an atomic counter. This counter uses the CAS primitive to increment its value. The process works like this: the thread reads the current value of the counter into a variable called old. It then computes a new value by adding the desired increment to old. It attempts to update the counter using CAS, which checks whether the current value is still equal to old. If it is, the counter is updated to new. If not, it means another thread has modified the counter in the meantime, so the operation is retried.

This retry loop is a common pattern in nonblocking algorithms. It ensures that the operation eventually succeeds, even in the presence of concurrent modifications. However, it also introduces the possibility of livelock, where multiple threads repeatedly retry their operations without making progress. To avoid this, nonblocking algorithms often include mechanisms to ensure that at least one thread always makes progress.

One of the classic examples of a nonblocking data structure is the Treiber stack, a lock-free stack implementation. The Treiber stack uses a top-of-stack pointer and a sequence count to avoid the ABA problem. The ABA problem occurs when a memory location’s value changes from A to B and back to A before a thread can perform a CAS operation. If the thread only checks for A, the CAS might incorrectly succeed, leading to corrupted state.

To solve this, the Treiber stack embeds a sequence count in the top-of-stack pointer. Each time the pointer is updated, the sequence count is incremented. This ensures that even if the pointer value returns to A, the sequence count will have changed, and the CAS operation will fail correctly.

The stack’s push and pop operations both use retry loops. The push operation takes a node and attempts to update the top pointer using CAS. The pop operation attempts to remove the top node and update the pointer to the next node in the stack. Both operations continue retrying until they succeed, ensuring that the stack remains consistent even under concurrent access.

Memory management is another critical aspect of nonblocking algorithms. When nodes are removed from a data structure, they must be safely deallocated to avoid use-after-free errors. One approach is to use a type-preserving allocator, which ensures that a block of memory is only reused for objects of the same type and alignment. This reduces the likelihood of the ABA problem by ensuring that if a memory address is reused, it is for an object that serves the same purpose in the data structure.

In more advanced implementations, memory managers use thread-local pools of free nodes. When a thread needs a new node, it first checks its local pool. If the pool is empty, it obtains a batch of nodes from a central backup pool. This reduces contention on the global pool and improves cache locality, leading to better performance.

Finally, let’s consider the implementation of nonblocking linked lists. Inserting and deleting nodes in a singly linked list requires updating pointers in adjacent nodes. In a naive implementation, this can lead to race conditions if multiple threads perform operations concurrently. For example, if one thread is inserting a node while another is deleting a node, the list can become inconsistent.

The Harris and Michael approach addresses this by using a two-step deletion process. First, a node is logically marked for deletion, often by setting a flag or modifying its value. Then, the pointer from the previous node is updated to skip over the marked node. This process uses CAS operations to ensure that updates are atomic and that the list remains consistent even under concurrent access.

These techniques illustrate the complexity and importance of synchronization in modern operating systems. Whether using spin locks, kernel-level queues, or nonblocking algorithms, the goal is always to ensure that concurrent threads can access shared resources safely and efficiently, without introducing unnecessary overhead or risking system instability.
