
In a program that is free of data races, the behavior of memory accesses appears as if they are executed in a single, global sequence. This property is known as sequential consistency. It means that all threads in the program agree on the order in which memory operations occur, and this order must align with the sequence of operations within each individual thread. Additionally, if a program uses transactions—units of computation that execute as if they are atomic, or indivisible—then the sequence of these transactions must also be consistent with the overall order of memory operations.

One of the key challenges in integrating transactional memory into programming languages is ensuring that transactions are handled correctly at both compile time and runtime. A subroutine, or function, that is called within a transaction must be transaction safe. This means that the subroutine must not perform any operations that could violate the atomicity or isolation of the transaction. If a subroutine is not transaction safe, calling it within a transaction could lead to unpredictable or incorrect behavior. For example, if the subroutine interacts with external resources like files or network connections, or if it throws an exception, the transaction may not be able to roll back cleanly or maintain its invariants.

Exceptions, in particular, pose a complex issue in transactional memory systems. When an exception is raised within a transaction, it must be propagated in a way that respects the transactional boundaries. If the exception indicates an error that requires restoring the program state to a consistent condition, the transaction must be able to roll back and ensure that any invariants are preserved. However, this raises a fundamental question: how can an exception be raised within a transaction if the transaction is considered to have never occurred? Some researchers argue that transactions should be fundamentally atomic, meaning that any speculative execution—where operations are tentatively performed and later confirmed or rolled back—should be an implementation detail rather than part of the language semantics.

In two thousand nine, Guerraoui and Kapalka explored the issue of forward progress in transactional memory systems. Forward progress refers to the ability of transactions to complete without being indefinitely delayed due to conflicts or contention. They introduced the concept of progressiveness, which describes how well a transactional memory system ensures that transactions can commit. A system is weakly progressive if a transaction that does not encounter any conflicts is guaranteed to commit. A system is strongly progressive if conflicts between transactions are limited to a single variable or object, and if progressiveness is ensured, it implies that speculation is not only a performance optimization but also part of the system's formal behavior. The definition of a conflict becomes important in this context, as does the possibility of false conflicts—situations where transactions appear to conflict due to the way operations are grouped or mapped to hardware.

The distinction between strong and weak isolation in transactional memory is also significant. Strong isolation ensures that all memory operations, including individual loads and stores, are serialized, meaning they appear to occur in a single, global order. Most hardware transactional memory systems aim for strong atomicity, which provides this level of isolation. However, at the language level, ensuring strong atomicity can be more challenging, especially in software implementations. The difference between strong and weak atomicity becomes apparent when considering data races—interleavings of memory accesses by different threads that can lead to incorrect behavior. Data races between transactional and nontransactional accesses, or even between two nontransactional accesses, are considered bugs. If such races are a major concern, the distinction between strong atomicity and other forms of isolation becomes crucial for diagnosing and debugging concurrent programs.

Progressiveness is largely independent of nonblocking progress, which refers to the ability of threads to make progress without being blocked by other threads. A nonblocking transactional memory system may allow a thread to make progress within the implementation of software transactional memory, or STM, while still not providing progressiveness at the level of complete transactions. One of the strongest arguments for nonblocking STM is its ability to support event-driven code, where a handler may need to make progress within the STM implementation even when a main thread is in the middle of a conflicting transaction.

When transactions are added to a programming language, it is often necessary to include additional features to support more complex use cases. One such feature is nesting, which allows transactions to be composed within other transactions. This is a key advantage of transactions over lock-based synchronization, which can lead to problems like deadlock. The simplest way to implement nesting is through a flattening approach, where inner transactions are merged into the outer transaction, so they either all commit or all abort. Most commercial hardware transactional memory systems support a form of nested transactions, often with a limit on the depth of nesting.

For performance reasons, it may be desirable to allow transactions to abort and retry while retaining the work they have already done. This is known as closed nesting, and it requires that the system support both aborting and not retrying a transaction. Such behavior can also be implemented in languages that provide explicit abort commands. Additionally, for both performance and flexibility, it is useful to allow multiple transactions to cooperate on computationally intensive tasks, committing their results atomically. In some cases, it may even be beneficial to allow an inner transaction to commit independently of the outer transaction, although this can potentially violate serializability and must be handled carefully.

Condition synchronization is another important topic in transactional memory. Like lock-based critical sections, transactions may depend on certain preconditions being true. However, unlike in lock-based systems, a transaction cannot wait for a condition to become true while remaining isolated from other threads. This is similar to nonblocking operations, which cannot wait and still be nonblocking. One potential solution is to require that transactions be total, meaning their preconditions are always true, but allow them to commit reservation notices instead of waiting. For example, if a dequeue operation on a transactional queue finds no data, it can enqueue a reservation for the data it expects, allowing other threads to proceed. The surrounding code can then wait for this reservation to be satisfied in normal, nontransactional code.

Another approach, proposed by Smaragdakis and colleagues in two thousand seven, is to suspend a transaction at a conditional wait, making the sections of the transaction before and after the wait individually atomic but not jointly atomic. This requires that any invariants maintained by the transaction are true at the point of suspension. If a wait is nested within a called routine, the fact that the routine may wait must be part of its interface.

The most elegant solution to condition synchronization in transactions is the retry primitive introduced by Harris and colleagues in two thousand five. When a transaction cannot proceed, it can abort and schedule a retry at a later time. This is similar to conditional critical regions and provides an efficient mechanism for software transactional memory. The transaction can proceed optimistically, assuming that the data it reads will not be modified by other transactions. If a modification does occur, or if the condition is still not met, the transaction can abort and retry. This mechanism relies on the concept of visible readers, which are transactions that monitor memory locations for changes. The synchronization mechanism for conditional execution can share implementation details with the abort mechanism for visible readers.

Beyond condition synchronization, speculation can be used in other ways. For example, try blocks can be implemented to roll back to their original state when an exception occurs, rather than simply stopping. These are known as try all blocks and are supported in hardware on systems like Power eight. Another application of speculation is in the parallelization of semantically sequential loops. In such loops, each iteration is treated as a transaction, and conflicts are resolved in favor of earlier iterations. No iteration can commit until all previous iterations have committed. This approach is supported in hardware by systems like Blue Gene Q, which provide ordered speculation to improve performance.

Privatization is another important concept in transactional memory. It refers to making a data structure private to a specific thread, typically when the structure is removed from a shared container. Once privatized, the structure no longer requires synchronization for access. However, privatization is not inherently race-free and may require compiler support to ensure correctness. Two types of races can occur: one where a transaction performs cleanup operations after its serialization point, interfering with nontransactional reads, and another where a doomed transaction reads data written nontransactionally by the thread that now owns the privatized data.

To address these issues, some systems use data partitioning or cloning to manage privatization. Data can be categorized as always private or sometimes shared, and privatized versions of shared data can be created using explicit cloning. Modern software transactional memory systems aim to be privatization safe by ensuring that any data accessed transactionally is either exclusively private or managed within a transactional context.

Compilers play a crucial role in implementing transactional memory. They can assist by instrumenting transactions, performing load and store operations, and inserting validation checks. Compilers can also optimize transactional code by cloning code paths for nontransactional execution, sandboxing dangerous operations, and eliminating redundant instrumentation. This helps improve performance while maintaining correctness.

Debugging and performance analysis of transactional programs present unique challenges. Transactions must appear atomic to other threads, which makes it difficult to examine program state at fine granularities. Researchers have proposed methods to differentiate between debugging within atomic blocks and debugging the transactional memory implementation itself. Without specialized tools, programmers often struggle to debug transactional programs, as conventional debuggers typically only support debugging of transactional memory operations, not the underlying mechanisms.

Several programming languages have developed compilers with transactional memory extensions, including Java, C#, C, C++, Clojure, and Haskell. Among these, Clojure and Haskell are considered to have mature implementations, while C++ is expected to be the first mainstream language to include transactional memory extensions in its standard.
