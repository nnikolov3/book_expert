
Queued spin locks are a type of synchronization mechanism designed to manage access to shared resources in multi-threaded environments, especially in systems with multiple processors. Unlike simple spin locks, where threads repeatedly try to acquire a lock and can cause high contention and cache line invalidations, queued spin locks improve fairness and efficiency by organizing waiting threads into a queue.

At the core of the queued spin lock mechanism is a data structure called a qnode, which represents a thread that is waiting for the lock. Each qnode contains two main components: a pointer to the previous qnode in the queue, and an atomic boolean variable called "successor must wait." This variable is used to signal to the next thread in line when it can proceed to acquire the lock.

The lock class manages the queue of qnodes. It maintains an atomic pointer called "tail," which always points to the last qnode added to the queue. When a new lock is created, the tail is initialized to a new qnode. This initial qnode has its previous pointer set to null, indicating it is the first in the queue, and its "successor must wait" variable is set to false, meaning no thread is currently waiting behind it.

The lock class provides an "acquire" method that takes a pointer to a qnode, which we'll call "p." Inside this method, the "successor must wait" field of the qnode pointed to by "p" is set to true using an atomic operation with sequential consistency. This ensures that any changes are properly synchronized across processors.

Next, a pointer called "pred" is declared and assigned the result of an atomic swap operation on the tail pointer. This swap operation does two things at once: it updates the global tail pointer to point to the current thread's qnode, effectively adding it to the queue, and it returns the previous value of the tail pointer, which becomes the predecessor of the current thread's qnode.

The thread then enters a spin loop, continuously checking the "successor must wait" field of its predecessor's qnode. It keeps spinning as long as this value is true, again using sequential consistency to ensure proper synchronization. This means the thread waits until its predecessor allows it to proceed. Once the loop exits, a memory fence operation is performed with acquire or release acquire semantics. This ensures that memory operations are properly ordered and that the thread sees the most up-to-date state of shared memory before proceeding into its critical section.

The lock class also includes a "release" method that takes a double pointer to a qnode, which we'll call "pp." Inside this method, a pointer named "pred" is declared and assigned the value of the previous pointer of the qnode that "pp" points to. The "successor must wait" field of the qnode pointed to by "pp" is then set to false using an atomic operation with release acquire consistency. This signals to the next waiting thread that it can now proceed to acquire the lock.

Finally, the "pp" pointer is updated to point to "pred," effectively transferring ownership of the predecessor's qnode to the current thread. This allows the thread to reuse the qnode for future lock acquisitions.

The CLH queued lock is illustrated in two figures. One shows the structure of the lock and how qnodes are linked together. The other shows the operation of the lock, with an "R" indicating that a thread is free to run its critical section, and a "W" indicating that it must wait. Dashed boxes represent qnodes that are no longer needed by the waiting threads and can be reused by the thread that is releasing the lock.

There is also a variant of the CLH lock that uses a standard interface, making it a direct replacement for traditional locks. This variant retains the scalability and reduced cache contention benefits of the original CLH lock but simplifies the interface by not requiring the caller to pass a qnode when releasing the lock.

Hemlock is a simplified and scalable version of the CLH lock, introduced by Dice and Kogan. Unlike the CLH lock, Hemlock does not use qnodes. Instead, each thread uses a single status word in shared memory to link into the queue of any lock it is waiting for. This status word is used to track the thread's position in the queue and to coordinate with other threads.

When a thread wants to acquire a lock, it performs an atomic swap operation on the lock's tail pointer, replacing it with the address of its own status word. If the return value is null, it means the lock was free, and the thread has successfully acquired it. If the return value is not null, it indicates the address of the predecessor thread's status word, and the current thread begins spinning on that status word, waiting for a signal that it can proceed.

To release the lock, the thread attempts an atomic compare-and-swap operation on the tail pointer. If this operation succeeds, it means no other threads are waiting, and the lock is simply released. If the operation fails, it means there are waiting threads, and the current thread must signal its successor. It does this by writing the address of the current lock into its own status word. This helps distinguish between cases where a thread holds multiple locks and multiple successors are waiting.

To prevent a situation where a successor misses the signal to proceed, the release method waits for a handshake. The successor confirms that it has noticed the signal, ensuring that the release is properly acknowledged. If a thread holds many locks, this can cause a burst of memory coherence activity as multiple successors check their status words. However, in the common case where a thread holds only one lock, the memory traffic is similar to that of the CLH lock.

To reduce the overhead of these handshakes, Dice and Kogan suggest an unusual but effective approach: instead of using simple memory loads to check the status of a predecessor or successor, threads should use compare-and-swap operations in both the acquire and release methods. This is because compare-and-swap is an atomic read-modify-write operation that provides stronger memory ordering guarantees. It allows a thread to directly attempt to modify the state of a lock, rather than just observe it, which can reduce contention and improve performance in high-concurrency situations.

The Hemlock algorithm is designed to minimize cache contention in multi-processor systems. It uses atomic operations on pointers and a per-thread status array to manage access to shared resources. The algorithm defines a "status" type as an atomic pointer to a lock object. There is also an array called "ts," indexed by thread identifier, which holds these status pointers. Initially, all entries in this array are set to null, representing that the thread is not waiting for any lock.

The lock class contains an atomic pointer called "tail," which is also initialized to null. This tail pointer represents the last element in the conceptual queue of waiting threads. The acquire function for the lock performs several steps. First, a status pointer called "pred" is assigned the result of an atomic swap operation on the tail pointer. The swap replaces the tail with the address of the current thread's status entry, and returns the previous value of the tail, which becomes the predecessor.

Next, the function checks if "pred" is not null. If it is not null, the thread enters a spin loop, continuously checking the value stored in the predecessor's status pointer. It keeps spinning as long as this value is not equal to the current lock object, using relaxed memory ordering. Once the loop exits, the predecessor's status pointer is set to null, completing a handshake. Finally, a memory fence instruction is executed to ensure strong memory ordering.

The release function checks whether an atomic compare-and-swap operation on the tail pointer succeeds. This operation attempts to replace the value at the current thread's status entry with null, using strong memory ordering. If it succeeds, it means no other threads are waiting, and the lock is released. If it fails, it means the tail pointer was not pointing to the current thread's status entry, so the thread must signal its successor.

To do this, the thread stores the address of the current lock object into its own status entry. Then, it enters a spin loop, continuously checking its status entry until it sees a null value. This ensures that the successor has acknowledged the signal before the release is complete.

The choice of spin lock implementation depends heavily on the specific machine architecture and the workload. For systems with a small number of threads, basic test-and-set locks or ticket locks can work well. However, ticket locks may not perform well on Non-Uniform Memory Access machines or when thread preemption is involved. The main issue with simpler locks is that their performance degrades quickly as contention increases. With more threads competing for the same lock, cache line invalidations and memory bus contention increase, leading to performance bottlenecks.

Queue-based locks like the Mellor-Crummey-Scott or Craig-Landin-Hagersten locks address this by allowing each waiting thread to spin on a local memory location instead of a single shared variable. On Non-Uniform Memory Access machines, where memory access times vary depending on the processor's proximity to the memory, Craig-Landin-Hagersten locks are generally preferred. While Mellor-Crummey-Scott locks also use local spinning, their queue management can require additional operations or introduce extra levels of indirection, which may lead to remote memory accesses and performance penalties.

The Hemlock algorithm is a fast, space-efficient, and highly scalable queue-based spin lock. It uses distributed spinning and carefully chosen atomic operations with precise memory ordering to minimize contention and improve performance. It is well-suited for modern multi-core and Non-Uniform Memory Access systems.

The standard acquire-release interface for locks can be extended to support special use cases. For example, a timeout parameter can be added to the acquire operation, allowing a thread to specify the maximum time it is willing to wait for the lock. The operation returns a boolean value indicating whether the lock was successfully acquired or if the timeout expired.

Another extension is the trylock operation, which attempts to acquire the lock immediately. It returns true if the lock is acquired and false if it is already held by another thread. The standard interface assumes that each lock acquisition is followed by a corresponding release. However, in some cases, it may be useful for a thread to acquire the same lock multiple times, as long as it releases it the same number of times before any other thread can acquire it. This is known as reentrant lock acquisition.

A reentrant lock can be implemented by adding an owner field and a counter to a basic mutual exclusion lock. The owner field stores the identity of the thread currently holding the lock, and the counter tracks how many times that thread has acquired the lock. When a thread tries to acquire the lock, it checks if it is already the owner. If not, it acquires the underlying lock, sets itself as the owner, and increments the counter. When releasing the lock, the thread decrements the counter and only releases the underlying lock when the counter reaches zero.

Another optimization technique is locality-conscious locking, which biases lock acquisition toward threads that are physically closer to the most recent holder. This reduces the cost of transferring the lock between processors on Non-Uniform Memory Access machines. By minimizing cache coherence operations and remote memory accesses, this approach improves system throughput, especially for highly contended locks. It takes advantage of the fact that communication between processors and cores has varying costs depending on their physical proximity, and optimizes lock acquisition to favor threads with lower communication costs, enhancing overall performance and scalability.
