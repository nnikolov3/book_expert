
In the world of concurrent computing, ensuring that data remains intact and behaves predictably when accessed by multiple threads or processes is a major challenge. This is especially true in systems where memory is shared among different parts of a program. One of the key ideas in this area is the concept of *safety*, which refers to how operations on shared data appear to be ordered in time. This ordering is crucial for maintaining consistency and correctness in programs that run multiple tasks simultaneously.

One widely used method for managing concurrent access to shared data is called *two-phase locking*. This technique ensures that all operations happen in a strict, sequential order, as if they were executed one after another without any overlap. This is known as *strict serializability*. It means that if one operation finishes before another starts in real time, the first one must appear to come before the second in the final sequence of events. This is a very strong guarantee and is often used in systems where correctness is critical, such as databases.

However, not all systems require such a strict ordering. Some implementations follow a weaker form of consistency called *plain serializability*. In this model, the system only needs to behave as if the operations were executed in some valid order, but that order doesn't have to match the actual real-time sequence. This allows for more flexibility and potentially better performance, but at the cost of predictability.

To understand the difference, imagine two operations: one that adds a value to a counter and another that reads the counter. In a strictly serializable system, if the add operation finishes before the read starts, the read must return the updated value. In a plain serializable system, the read might return the old value, as long as there's *some* order that makes the result valid. This can lead to behavior that seems counterintuitive to a programmer, but it can also allow the system to run faster.

Now, let's look at a different model called *quiescent consistency*. This model is based on the idea of *quiescent intervals*, which are periods when no operations are actively being performed on a particular object. During these intervals, the system must ensure that all previous operations have completed and that any new operations start fresh. So, if one operation ends and there's a quiescent period before the next one begins, the system must reflect the result of the first operation before starting the second.

Quiescent consistency is a *local* property, meaning it applies to individual objects rather than the entire system. This is different from models like *linearizability*, which provide global guarantees about the order of operations across the entire system. One limitation of quiescent consistency is that during non-quiescent periods—when operations are actively happening—it doesn't necessarily preserve the order in which operations were issued by individual threads or the real-time sequence of events across threads.

To give a concrete example of how quiescent consistency might be implemented, consider a system where a single lock is used to manage access to an object. When a thread wants to perform an operation, it acquires the lock, but instead of executing the operation immediately, it *stages* it—meaning it queues the operation for later execution. If the result of the operation isn't needed right away, this staging allows the system to batch multiple operations together. Before releasing the lock, the thread must execute all the staged operations. This approach is similar to a technique called *flat combining*, where a single thread processes a group of operations on behalf of others, reducing the overhead of acquiring and releasing locks repeatedly.

Now, let's compare different consistency models. There are several important criteria that define how operations are ordered and how they relate to real time. These include *sequential consistency*, *linearizability*, *serializability*, *strict serializability*, and *quiescent consistency*. Each of these models offers different guarantees about the order of operations and how they align with real time.

For example, *linearizability* and *strict serializability* both ensure that the order of operations matches real time. This means that if one operation finishes before another starts, the system must reflect that order. In contrast, *plain serializability* and *sequential consistency* only guarantee that the operations could have happened in *some* valid order, not necessarily the real-time order. *Quiescent consistency* only enforces real-time ordering during quiescent intervals.

Another important distinction is whether a model supports *multi-object atomicity*. This refers to the ability to treat a group of operations on different objects as a single, indivisible unit. *Serializability* and *strict serializability* provide this capability, making them suitable for systems where multiple objects must be updated together in a consistent way. On the other hand, *linearizability* and *quiescent consistency* are primarily local properties and do not inherently support this kind of atomicity across multiple objects.

This leads us to the concept of *composability*, which is the ability to combine individual operations into larger, atomic units. In distributed systems and transactional memory environments, this is a major challenge. Even if each individual operation is atomic or serializable, combining them doesn't automatically preserve those properties. For example, two linearizable operations might not form a linearizable composite operation unless the system is specifically designed to handle such combinations.

This is especially difficult in systems that use *speculation*, such as some transactional memory implementations. These systems try to execute operations optimistically, assuming they won't conflict, and roll back if they do. While this can improve performance, it makes it harder to support composable operations without falling back to more conservative locking strategies.

Now, let's shift focus to *liveness*, which is another fundamental property in concurrent systems. While *safety* ensures that the system never enters an invalid state, *liveness* ensures that the system continues to make progress. In other words, safety is about avoiding bad outcomes, and liveness is about eventually achieving good ones.

Liveness includes properties like *deadlock freedom*, *starvation freedom*, and *progress*. For example, a system must ensure that threads don't get stuck waiting forever for a resource, and that every thread eventually gets a chance to proceed. A method is considered *blocking* if it can cause a thread to wait indefinitely for another thread to release a resource, such as a lock. This is common in lock-based algorithms, where threads must wait for exclusive access to shared data.

To avoid these problems, *non-blocking algorithms* are used. These algorithms ensure that the system continues to make progress even if some threads fail or are delayed. They typically rely on low-level atomic operations like *compare and swap* to manage shared state without locks. This makes them more resilient to failures and better suited for high-concurrency environments.

There are different levels of non-blocking progress. The strongest is *wait freedom*, which guarantees that every thread will complete its operation within a bounded number of its own steps, regardless of what other threads are doing. This ensures that no thread will be indefinitely delayed, even if other threads are slow or fail.

A slightly weaker form is *lock freedom*, which guarantees that at least one thread will make progress within a bounded number of system-wide steps. However, individual threads may still be delayed indefinitely, so lock freedom does not guarantee fairness.

The weakest form is *obstruction freedom*, which only guarantees progress if a thread runs without interference from others. If multiple threads are competing for the same resource, an obstruction-free algorithm may still suffer from repeated failures and retries.

Many non-blocking algorithms use a technique called *helping*, where one thread assists another in completing its operation. This helps prevent situations where a thread gets stuck and blocks progress for others. For example, in a wait-free counter implementation, each thread maintains its own local count, and the total is computed by summing all the local counts. This allows each thread to increment its own value without interfering with others, and the total can be computed at any time.

Now, let's look at *fairness*, which is the property that every thread attempting an operation will eventually complete it. While *wait freedom* inherently ensures fairness, *lock freedom* and *obstruction freedom* do not. This means that even if the system as a whole is making progress, some threads may still be delayed indefinitely.

In practice, fairness depends on the underlying system's ability to schedule threads and ensure that they continue to run. This includes factors like the hardware, the operating system, and the programming language runtime. Without some level of fairness, a system could be technically correct but practically unusable because certain threads never get a chance to run.

One example of how fairness is enforced is through *weak fairness*, which guarantees that any thread waiting for a condition that is continuously true will eventually proceed. This is important in systems that use busy-waiting or spin loops, where a thread might otherwise wait forever if the scheduler doesn't give it a chance to run.

To illustrate this, consider a simple program with two threads and a shared boolean variable. One thread waits for the variable to become true, while the other waits for it to become false before setting it to true. If weak fairness holds, the system must eventually schedule the second thread, which will set the variable to true, allowing the first thread to proceed. This ensures that both threads complete their tasks, even though they are using a potentially inefficient spin-waiting pattern.

In summary, concurrent systems must balance between strong consistency guarantees and performance. Models like strict serializability and linearizability provide strong, predictable behavior but can be slow. Weaker models like quiescent consistency and plain serializability offer better performance but may allow unintuitive behavior. Non-blocking algorithms provide robustness and fault tolerance, but they come with their own trade-offs in terms of complexity and fairness. Ultimately, the choice of model and algorithm depends on the specific requirements of the application, including how much correctness and performance are needed.
