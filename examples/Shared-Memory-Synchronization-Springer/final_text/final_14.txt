
In multi-processor computing environments, especially those where memory access times vary depending on the location of the data relative to the processor core—known as Non-Uniform Memory Access, or NUMA—managing shared data structures efficiently becomes a critical concern. When a thread on one core needs to access a shared data structure, the relevant portion of memory, known as a cache line, must be transferred into the cache of that core. This transfer incurs a performance cost, and that cost is significantly lower if the data currently resides in the same NUMA cluster as the core attempting to access it. This underscores the importance of maintaining data locality—keeping data close to the threads that use it most frequently.

In two thousand two, Radovic and Hagersten observed that maintaining locality is especially important when dealing with synchronization mechanisms like locks. Locks are used to protect shared data from being accessed simultaneously by multiple threads, which could lead to data corruption. They introduced a lock design called the RH lock, tailored for a two-cluster NUMA system. This lock conceptually uses two test-and-set operations—one for each cluster. A test-and-set operation is a special kind of atomic instruction that reads the current value of a memory location and writes a new value to it, all in one indivisible step. This ensures that no other thread can interfere during the operation, maintaining consistency across processors.

The RH lock could be extended to systems with more than two clusters, but the memory space required would grow proportionally with the number of clusters, which Radovic and Hagersten considered inefficient. To address this, they later proposed the Hierarchical Backoff, or HBO, lock. Instead of using a separate test-and-set for each cluster, the HBO lock uses a single test-and-set lock combined with a compare-and-swap operation. The lock variable itself keeps track of which cluster currently holds the lock. Threads that are in the same cluster as the lock use one set of backoff parameters—meaning they wait for shorter intervals before trying again—while threads from other clusters use longer backoff intervals. This increases the likelihood that threads within the same cluster will acquire the lock quickly when it becomes available.

However, both the RH and HBO locks are inherently unfair in their design. A test-and-set lock does not guarantee that every thread will eventually get a turn—it can theoretically leave some threads waiting indefinitely, a problem known as starvation. In practice, the RH and HBO locks may be even less fair than a basic test-and-set lock. Ideally, a locking mechanism should allow developers to balance fairness and locality, ensuring that threads get timely access to shared resources while still benefiting from performance gains due to data proximity.

To address this need, Dice and colleagues introduced a NUMA-aware locking design in two thousand twelve that can be applied to a wide range of underlying lock types, including FIFO queued locks. Their approach, called cohort locking, uses two layers of locking: a global lock that tracks which cluster currently owns the lock, and a local lock for each cluster that tracks which thread within that cluster holds the lock. This design allows for efficient coordination between clusters while maintaining fine-grained control within each cluster.

The global lock must allow one thread to acquire it and a different thread to release it, which is not typical for many lock implementations. The local lock, on the other hand, must be able to determine whether any other thread in the same cluster is waiting for the lock when it is released. These are the only special requirements for the cohort locking mechanism, which can otherwise work with any existing lock type. Experimental results have shown that using Mellor-Crummey-Scott, or MCS, locks at both the global and local levels provides particularly strong performance and fairness, while still preserving data locality.

The techniques discussed so far improve performance by controlling the order in which threads acquire locks. However, another approach is to control which threads perform the operations that are protected by the lock. By assigning operations that access similar data to the same thread, the system can reduce cache misses and improve overall efficiency. This kind of locality-conscious work allocation can significantly boost performance in systems that distribute fine-grained tasks among multiple worker threads.

Another important optimization technique is lazy initialization, where the creation of an object or resource is delayed until it is actually needed. This conserves system resources like memory and CPU time by avoiding the creation of objects that may never be used. A basic thread-safe way to implement lazy initialization involves a shared pointer that starts as null and a lock. When a function is called to retrieve the object, it first acquires the lock, checks if the object has already been created, and if not, creates it and assigns it to the shared pointer before releasing the lock.

This simple approach, however, has a drawback: it requires acquiring the lock every time the function is called, even after the object has already been initialized. To reduce this overhead, the double-checked locking idiom was developed. This pattern checks the shared pointer once before acquiring the lock and again after acquiring it. If the pointer is already set, the thread can proceed without acquiring the lock. This reduces contention and improves performance.

However, this idiom relies on precise memory ordering to work correctly. Without proper synchronization, the thread initializing the object might set the pointer before the object is fully constructed, or another thread might read uninitialized data. On systems with relaxed memory models, the cost of ensuring correct memory ordering can be nearly as high as acquiring the lock in the first place, making the optimization less effective. On systems with a Total Store Order, or TSO, memory model—such as most modern x eighty six processors—the overhead of memory ordering is minimal, making this optimization much more beneficial.

Despite its potential performance benefits, double-checked locking is notoriously error-prone. It requires careful handling of memory barriers and compiler optimizations, and mistakes can lead to subtle and hard-to-diagnose bugs. As a result, many modern programming languages and operating systems provide higher-level, safer mechanisms for one-time initialization. For example, Windows Vista introduced the InitOnce API, which handles all the necessary synchronization and memory ordering guarantees internally, allowing developers to safely initialize resources without manually managing locks.

Another optimization is asymmetric locking, which is useful in situations where a data structure is accessed primarily by a single designated thread, but occasionally by others. The idea is to optimize for the common case—where the designated thread accesses the data—by making its lock operations as fast as possible, ideally even lock-free. For the rare cases where other threads access the data, the locking mechanism still ensures correctness, but with more overhead.

One way to implement asymmetric locking is by adapting classic synchronization algorithms like Peterson's Algorithm, which ensures mutual exclusion between two threads using only atomic read and write operations. In this context, Peterson's algorithm is modified to distinguish between the preferred thread and all other threads. The preferred thread can acquire and release the lock with minimal overhead, while the other threads follow a more complex synchronization path, often involving handshake operations that ensure proper memory visibility and ordering.

This handshake mechanism is crucial for correctness on systems with relaxed memory models. It typically involves specific atomic operations or sequences of memory barriers that ensure that changes made by one thread are properly observed by others. While this path is slower, it is used infrequently, so the overall performance remains high. This technique is especially useful in environments like the Java Virtual Machine, where objects are often associated with a specific thread and require synchronization during garbage collection or when interacting with native code. The HotSpot JVM, for example, uses biased locking to optimize access to objects that are primarily used by a single thread, reducing the overhead of synchronization in common cases.

Synchronization in concurrent computing is fundamentally about coordinating the execution of multiple threads to ensure correctness and data consistency. One common synchronization technique is busy wait synchronization, where a thread repeatedly checks a condition instead of yielding its time to the operating system. This is often implemented using a spin lock, where the thread spins in a loop waiting for a flag to change.

A basic implementation of this uses an atomic Boolean variable, initially set to false. The atomic nature of the variable ensures that operations on it are indivisible, preventing race conditions that could occur if multiple threads tried to modify it simultaneously. The flag provides two main operations: set and await. The set operation changes the flag to true, and the await operation causes a thread to wait until the flag becomes true.

When a thread calls the set method, it performs a release store, which ensures that all memory writes made before setting the flag are visible to other threads after they read the flag. When another thread calls await, it enters a loop that repeatedly checks the flag. Once the flag becomes true, the thread proceeds, but before doing so, it executes a memory fence operation—also known as a memory barrier—to ensure that all subsequent memory reads will see the writes made before the flag was set.

This pairing of release and acquire semantics is essential in systems with weak memory models, where the order of memory operations is not guaranteed unless explicitly enforced. The flag is often used in scenarios where one thread initializes a shared data structure and signals completion by setting the flag, while other threads wait for the flag before accessing the structure.

Handshaking mechanisms can also be implemented in various other ways, such as using cross-core interrupts, thread migration, or forced un mapping of memory pages. Cross-core interrupts are hardware signals that one core sends to another, often used for urgent communication, but they are expensive due to the overhead of interrupt handling. Thread migration involves moving a thread from one core to another, which can help with load balancing or data locality but introduces costs related to cache warming and context switching. Forced un mapping of memory pages is a more extreme approach that prevents non-preferred threads from accessing certain data, but it can lead to page faults and significant performance penalties.

Dice and colleagues studied these handshaking techniques and found that they are rarely beneficial in general-purpose computing due to their high overhead. They are only justified in cases where non-preferred thread access is extremely rare. However, they also observed that modern hardware inherently provides memory coherence through cache protocols like MESI, which automatically ensure that writes from one core become visible to others. Additionally, modern processors support atomic operations at subword levels, allowing fine-grained synchronization without the need for complex software-based handshaking in most cases.

This hardware-level support for memory coherence and atomicity significantly simplifies concurrent programming, reducing the need for costly software synchronization mechanisms in many scenarios. As a result, developers can focus on higher-level design patterns and optimizations that balance performance, fairness, and correctness in multi-threaded applications.
