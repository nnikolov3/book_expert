
Queued spin locks are a sophisticated class of synchronization mechanisms designed to manage access to shared resources in multi-processor systems. Their primary goal is to reduce contention among processors and ensure fairness in lock acquisition. Traditional spin locks, such as simple test-and-set locks, suffer from high cache invalidation traffic because all waiting processors repeatedly attempt to access the same memory location. This leads to performance degradation due to excessive cache coherence traffic.

To address this issue, queued spin locks distribute the waiting threads across different memory locations, thereby reducing contention and improving scalability. One of the early approaches to this problem was proposed by Anderson, who introduced a design where the queue is implemented as an array of flag words. Each thread is assigned a specific element in this array to spin on, effectively spreading out the contention. When a thread wants to acquire the lock, it knows its assigned index in the array. Upon releasing the lock, the thread performs an atomic fetch and increment operation on a shared counter, which determines the next thread in a circular order to receive the lock. The releasing thread then updates the flag corresponding to its original position in the array.

Another early design, developed by Graunke and Thakkar, also uses an array-based queue but introduces an atomic swap operation to manage the queue. In this scheme, an additional tail element is used to track the end of the queue. When a thread wants to join the queue, it performs a swap operation that effectively enqueues it by updating the tail pointer. This allows each thread to determine its designated spin location without requiring a shared counter.

A significant advancement in queued spin locks came with the introduction of linked list-based designs. These were inspired by the QOLB hardware primitive from the Wisconsin Multicube project and the IEEE SCI standard. Researchers such as Goodman, Aboulenein, and Mellor Crummey and Scott developed queue-based spin locks that use dynamically linked nodes instead of fixed-size arrays. This approach eliminates the need for a static upper bound on the number of threads and reduces the memory overhead. Instead of requiring memory proportional to the product of the number of threads and locks, these designs only require memory proportional to the sum of threads and locks.

One of the most well-known implementations of this idea is the MCS lock, named after Mellor Crummey and Scott. The MCS lock uses a queue implemented as a linked list of nodes, where each thread allocates its own node, or qnode, when attempting to acquire the lock. Each qnode contains a pointer to the next node in the queue and a flag indicating whether the thread is waiting.

The lock itself is represented by a pointer to the qnode at the tail of the queue. If the lock is free, this pointer is null. When a thread wants to acquire the lock, it initializes its qnode, sets its next pointer to null, and performs an atomic swap operation to insert its qnode at the tail of the queue. If the swap returns a null value, the thread has successfully acquired the lock. If it returns a non-null value, that value refers to the qnode of the previous thread in the queue, and the current thread must update the previous thread's next pointer to point to its own qnode.

When the thread holding the lock finishes its critical section, it executes the release method. This involves checking the next pointer in its own qnode to find its successor in the queue. If there is a successor, the releasing thread modifies the waiting flag in the successor's qnode to signal that the lock is now available. This allows the successor to stop spinning and proceed with its critical section. If there is no successor, the lock is simply set to null, indicating it is now free.

The MCS lock has several important advantages. Threads join the queue in a wait-free manner using the swap operation, and they receive the lock in first-in-first-out order. Each thread spins on its own qnode, which eliminates contention for shared cache lines. This is especially beneficial in NUMA architectures, where memory access times vary depending on the location of the memory relative to the processor. Because each thread provides its own qnode, it can place it in local memory, reducing remote memory access overhead.

The MCS lock also has a constant time overhead for passing the lock from one thread to the next, and its memory usage scales linearly with the number of threads and locks. However, the original MCS lock requires both swap and compare-and-swap operations. While compare-and-swap can be used to emulate swap, this changes the queue entry from wait-free to lock-free, which means a thread could potentially be starved if other threads keep acquiring the lock before it.

One limitation of the MCS lock is that it requires each thread to pass a qnode pointer when calling the acquire and release functions. Traditional spin locks, such as test-and-set or ticket locks, only require a reference to the lock itself. This makes it more difficult to integrate the MCS lock into existing codebases that were designed for simpler lock interfaces. To address this, a variant of the MCS lock was developed as part of the K42 project at IBM Research. This variant embeds some of the qnode management logic directly into the lock's state, allowing it to be used with a simpler API.

In this variant, the lock is represented by a qnode that contains two null pointers when the lock is free. One pointer serves as the tail of the queue, and the other is the next pointer for the first waiting thread. This allows a newly arriving thread to use a compare-and-swap operation to replace a null tail pointer, effectively managing the initial queuing without requiring an external qnode pointer.

The K42 variant of the MCS lock maintains the efficiency and fairness of the original design while simplifying the interface for integration with legacy systems. It retains the ability to scale well in highly concurrent environments and continues to provide first-in-first-out ordering of threads.

Another important queued spin lock design is the CLH lock, named after Craig, Landin, and Hagersten. The CLH lock is designed to minimize the number of remote memory accesses during lock acquisition, achieving constant overhead regardless of the number of processors. Like the MCS lock, it uses qnodes, but the structure and operation differ.

In the CLH lock, each thread attempting to acquire the lock creates its own qnode and enqueues itself by atomically linking its qnode into a system-wide logical queue. The thread then spins on a flag in its predecessor's qnode. This spinning is efficient because the predecessor's qnode is likely to be in the current thread's local cache or a nearby memory region, reducing the need for remote memory access.

When a thread releases the lock, it simply modifies the state of its own qnode, setting a flag that indicates the next thread in the queue can proceed. This notifies the successor, which has been spinning on the predecessor's qnode, that the lock is now available. The CLH lock ensures first-in-first-out fairness, as threads acquire the lock in the order they requested it.

One potential drawback of the original CLH design is an extra handshake required between threads. A newly arriving thread must write its qnode address into its predecessor's qnode, and the predecessor must wait for that write to complete before releasing the lock. To address this, an optimization was introduced that allows each thread to spin on a memory location local to its own processor, improving performance on NUMA architectures.

Further refinements of the CLH lock include the LH lock and the M lock. The LH lock is conceptually similar to the CLH lock but differs in the mechanisms used to pass qnodes during acquire and release operations. The M lock optimizes the case where the lock is not contended, reducing the number of cache misses. While the M lock may use compare-and-swap operations to resolve contention, the CLH lock primarily relies on atomic swap operations, making it more efficient in some scenarios.

Craig's original work on the CLH lock also explored extensions to improve its performance on NUMA systems. By introducing an additional level of indirection, the design can eliminate remote spinning, making it suitable for non-uniform memory access architectures without requiring compare-and-swap operations. This extension also allows for more flexible ordering of threads, such as prioritizing certain threads over others, while still maintaining the efficiency benefits of localized spinning.

These queue-based spin lock designs—MCS, K42 variant, and CLH—represent significant advancements in synchronization for concurrent systems. They address the limitations of traditional spin locks by reducing contention, improving scalability, and ensuring fairness. Each design has its own strengths and trade-offs, making them suitable for different types of systems and workloads. The MCS lock excels in environments where wait-free entry and localized spinning are critical, while the CLH lock provides efficient lock acquisition with minimal remote memory access. The K42 variant simplifies integration with existing codebases while preserving the performance benefits of the MCS design.

In summary, queued spin locks are essential tools for managing concurrency in modern multi-processor systems. They provide efficient, scalable, and fair mechanisms for controlling access to shared resources, ensuring that threads can execute critical sections without unnecessary delays or contention.
