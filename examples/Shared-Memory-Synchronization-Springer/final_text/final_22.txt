
In this section, we explore advanced language constructs that help manage concurrency and parallelism effectively. These constructs are essential for writing programs that can handle multiple tasks simultaneously, especially when those tasks share resources or data.

One of the core ideas discussed is the bounded buffer, a data structure used to coordinate the flow of information between threads. A bounded buffer acts like a queue with a fixed maximum size. It ensures that one thread can insert data while another thread removes it, without either of them overstepping the buffer’s capacity or accessing invalid data. To implement this, the buffer maintains two positions: one for the next available spot to insert data, and another for the next element to be removed. When inserting, the data is placed at the current empty position, and then that position is updated to the next available slot. If the end of the buffer is reached, the position wraps around to the beginning, ensuring efficient use of the fixed space. Similarly, when removing, the data at the current full position is retrieved, and then that position is advanced, again wrapping around when necessary.

However, when multiple threads try to access the buffer at the same time, problems can arise. Without proper control, two threads might attempt to insert or remove data simultaneously, leading to inconsistent or corrupted data. To prevent this, the concept of conditional critical regions is introduced. These are sections of code that can only be executed by one thread at a time, and only if a certain condition is met. For example, a thread can only insert data if the buffer is not full, and only remove data if the buffer is not empty. The system ensures that only one thread can be inside such a region at any moment, and that other threads wait until the condition they need is satisfied.

This leads to a broader discussion of how programming languages can manage concurrency more generally. One optimization involves tracking which variables a condition depends on. If a condition is waiting for a change in a specific variable, the system can pause the thread until that variable is actually modified, rather than constantly checking it. This reduces unnecessary computation and improves efficiency.

Another important rule is that conditions should only depend on the internal state of the object they are associated with, not on external parameters passed into methods. This allows the system to manage the conditions more efficiently, without having to keep track of the entire context in which a thread was called. When a thread leaves a critical region, the system checks if any waiting thread now meets its condition and, if so, resumes that thread.

Futures are another powerful tool for managing parallelism. A future represents a value that will be computed later, allowing a program to continue executing other tasks in the meantime. This is especially useful in algorithms that can be broken into independent subtasks, such as sorting large arrays using quicksort. Each subtask can be assigned to a future, and the program can proceed without waiting for each one to finish immediately. When the result of a future is needed, the program waits only if the computation hasn’t completed yet.

In languages like Java, futures are implemented using objects such as FutureTask. A thread can start a computation in the background and then retrieve the result later using a method called get. However, because Java is not purely functional, care must be taken to ensure that shared data is accessed safely. Some proposals suggest using techniques similar to transactional memory to make futures more robust and predictable in such environments.

Instead of using busy waiting—where a thread repeatedly checks if a condition is met—many systems use scheduler-based synchronization. In this approach, a thread that cannot proceed simply yields control and waits to be notified when the condition might be true. This notification can come through signals or interrupts, and it avoids wasting processor time. This kind of synchronization is central to what is known as series-parallel execution, where tasks are structured so that some parts run in sequence and others in parallel.

A common example of this is found in systems like Cilk, which provide constructs for spawning multiple threads and then synchronizing them. For instance, a loop might spawn a set of parallel tasks, and then a sync operation ensures that all of those tasks complete before the program continues. This structure allows a main thread to fork off child threads at the start of a loop iteration and then join them back at the end. The Cilk runtime system is designed to make these operations as efficient as possible, minimizing the overhead of managing parallel tasks.

These systems often use a technique called work stealing, where each thread maintains its own queue of tasks. When a thread finishes its current work, it looks for tasks in other threads’ queues and "steals" one to keep the processor busy. This helps balance the workload and ensures that all available processors are used effectively.

Proper synchronization is crucial to avoid data races, which occur when multiple threads access the same memory location without coordination, leading to unpredictable results. Barrier synchronization is one way to prevent this. A barrier ensures that all threads reach a certain point in the program before any of them can proceed. This is particularly important in parallel loops, where different iterations might depend on each other’s results.

Some languages, like Fortran, provide built-in constructs for parallel loops, such as the forall loop. These constructs allow the compiler to manage the parallel execution and synchronization automatically. Similarly, OpenMP provides compiler directives that can be added to C or Fortran code to indicate which loops should be executed in parallel, along with implicit synchronization at the end of the loop.

The discussion also covers task graphs, which are visual representations of how parallel tasks are organized. Three main types are described: fork and join, spawn and sync, and split and merge. In the fork and join model, a single task splits into multiple subtasks, which must all complete before the original task can continue. This is useful for divide-and-conquer algorithms. The spawn and sync model is more flexible, allowing a task to create new tasks and then explicitly wait for them to finish. The split and merge model is often used in recursive algorithms, where a problem is divided into subproblems, solved in parallel, and then combined.

Phasers are a more advanced synchronization mechanism that generalize the idea of barriers. They allow threads to register whether they are waiting for a signal or are responsible for sending one. This makes it possible to implement more flexible synchronization patterns, such as fuzzy barriers, where threads don’t all have to wait for each other at the same time.

Finally, the text explores how synchronization is handled at the system level, particularly the interaction between user threads and kernel threads. User threads are managed by the application or runtime system, while kernel threads are managed by the operating system. Synchronization between threads often involves switching between user mode and kernel mode, which can be expensive due to the overhead of saving and restoring thread state.

One challenge is inopportune preemption, where a thread is interrupted while holding a lock on a shared resource. This can cause other threads to wait unnecessarily, leading to a phenomenon known as a convoy, where many threads line up waiting for a single resource. To reduce this effect, some systems use temporary non-preemption, where the kernel is informed that a thread is about to enter a critical section and is given a chance to adjust scheduling decisions accordingly.

In summary, this section provides a deep dive into the mechanisms that programming languages and runtime systems use to manage concurrency and parallelism. From bounded buffers and futures to work stealing and phasers, these tools help ensure that programs can scale efficiently across multiple processors while maintaining correctness and avoiding common pitfalls like data races and resource contention.
