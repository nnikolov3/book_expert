
In concurrent programming, ensuring that multiple threads can safely access shared resources without conflict is a foundational challenge. One of the key mechanisms for achieving this is mutual exclusion, which guarantees that only one thread can execute within a critical section at any given time. This discussion explores two important algorithms—Lamport's Bakery Algorithm and Peterson's Algorithm—along with their implementation details, memory ordering constraints, and performance considerations.

Let's begin with Lamport's Bakery Algorithm, a software-based solution for mutual exclusion that works with any number of threads. The algorithm is inspired by the way customers in a bakery take numbered tickets to determine the order in which they are served. Each thread, when attempting to enter a critical section, first selects a ticket number. The thread with the smallest number gets to enter the critical section first.

To implement this, the algorithm uses two shared arrays: the choosing array and the number array. The choosing array is a boolean array that indicates whether a thread is currently in the process of selecting a ticket number. The number array holds the actual ticket numbers for each thread. Initially, all entries in the choosing array are false, and all entries in the number array are zero.

When a thread wants to acquire the lock, it first sets its choosing flag to true. This operation is followed by a read memory fence, which ensures that this update becomes visible to all other threads before the current thread proceeds. Then, the thread scans the number array to find the highest ticket number currently held by any other thread. It adds one to that value to determine its own ticket number and stores it in its own position in the number array. This store operation is accompanied by a read-write memory fence, which ensures that the ticket number is properly recorded and visible to other threads before any further actions are taken.

Next comes the synchronization phase. The thread iterates through all other threads and performs two spin loops for each. The first spin loop checks whether the other thread is still in the process of choosing its ticket. If so, the current thread waits. This check uses a read fence to ensure that the current thread sees the most up-to-date value of the other thread's choosing flag.

Once the other thread has finished choosing, the current thread enters the second spin loop. It repeatedly checks the other thread's ticket number. If that number is zero, it means the other thread is not currently contending for the lock. If the other thread's number is greater than the current thread's number, or if the numbers are equal but the other thread's identifier is higher, the current thread can proceed. Otherwise, it must wait. This ensures a strict ordering of threads based on their ticket numbers and identifiers.

When the thread is ready to release the lock, it simply sets its ticket number back to zero. This operation is accompanied by a read-write memory fence to ensure that the release is properly observed by other threads.

Now let's turn to Peterson's Algorithm, which is designed for mutual exclusion between two threads. While conceptually simple, its correct behavior depends heavily on precise memory ordering. In modern systems, both compilers and processors can reorder memory operations for optimization purposes. Without explicit memory barriers, these reorderings can break the assumptions of the algorithm.

In Peterson's Algorithm, a thread attempting to acquire the lock sets a flag indicating its interest and then yields priority to the other thread. It then checks whether the other thread is interested and whether it has priority. If both threads are interested and the current thread has priority, it can enter the critical section.

To ensure correctness, the algorithm uses acquire and release memory fences. An acquire fence ensures that all memory operations before the fence are completed and visible before the thread proceeds into the critical section. A release fence ensures that all memory operations within the critical section are completed and visible before the thread releases the lock.

Lamport's Fast Algorithm is another approach to mutual exclusion, optimized for the common case where the lock is not heavily contended. In this algorithm, a thread first declares its intent to acquire the lock by setting a flag in a trying array. It then attempts to claim a shared variable x by storing its own identifier into it.

If another thread has already claimed a second shared variable y, the current thread must back off. It sets its trying flag back to false and waits until y becomes null. This waiting is done through a spin loop that repeatedly checks the value of y.

If y is null, the thread proceeds to claim y by storing its identifier into it. It then checks whether x still contains its own identifier. If not, it means another thread has successfully claimed x after this thread did, and the current thread must back off again.

In this case, the thread resets its trying flag and waits for all other threads to clear their trying flags. This waiting is done through a loop that checks each thread's trying flag. Once all flags are cleared, the thread executes a read fence to ensure it sees the most up-to-date values of y before checking it again.

If y is still not equal to the current thread's identifier, it means another thread has successfully claimed y. The current thread must wait until y becomes null again and then restart the entire acquisition process.

Only after passing all these checks can the thread proceed into the critical section. Before doing so, it executes a read-write memory fence to ensure that all prior operations are visible before any operations within the critical section begin.

When releasing the lock, the thread first sets y to null with a read-write memory fence, ensuring that all operations within the critical section are visible to other threads. Then, it sets its trying flag back to false with a write memory fence, ensuring that this update is globally visible.

One important characteristic of this algorithm is its performance under contention. In the best case, when no other threads are contending for the lock, the acquisition time is constant. However, in the worst case, when many threads are contending, the acquisition time grows linearly with the number of threads. This is because the algorithm may need to check and wait for every other thread's trying flag to clear.

Spin locks, in general, are synchronization mechanisms where a thread repeatedly checks whether a lock is available rather than yielding its execution. This avoids the overhead of context switching, making spin locks suitable for short critical sections on multi-processor systems.

However, basic spin locks can suffer from performance issues under contention. For example, the test-and-set lock involves repeated atomic read-modify-write operations on a shared variable, which can generate significant cache coherence traffic. To reduce this, the test-and-test-and-set lock introduces a preliminary phase where a thread first reads the lock variable before attempting to modify it. This reduces the number of expensive atomic operations.

Another optimization is exponential backoff, where a thread waits for an increasingly longer duration after each failed attempt to acquire the lock. This helps reduce contention but requires careful tuning of parameters such as the base delay, maximum delay, and multiplier.

Fairness is another concern with basic spin locks. A thread that has been waiting for a long time may be repeatedly bypassed by newer threads. To address this, the ticket lock was introduced. In this approach, each thread receives a unique ticket number by performing a fetch-and-increment operation on a shared variable. Threads then wait until their ticket number is served, ensuring a first-come-first-served order.

The ticket lock uses two shared variables: next_ticket and now_serving. When a thread wants to acquire the lock, it fetches and increments next_ticket to get its own ticket number. It then spins on now_serving until its number is reached. A proportional backoff strategy can be used, where the thread waits for a duration proportional to how many tickets are ahead of it, reducing the number of probes to now_serving.

Queued spin locks take this idea further by organizing waiting threads into a queue. Each thread knows its position in the queue and waits only for its predecessor to finish. When a thread exits the critical section, it signals its successor. This reduces global contention by limiting cache coherence traffic to local interactions between adjacent threads in the queue.

In summary, mutual exclusion algorithms like Lamport's Bakery Algorithm and Peterson's Algorithm provide essential mechanisms for coordinating concurrent threads. These algorithms rely on precise memory ordering and synchronization techniques to ensure correctness. Optimizations such as ticket locks and queued spin locks further improve performance and fairness, especially under contention. Understanding these principles is crucial for developing efficient and reliable concurrent systems.
