120 7 Synchronization and Scheduling

run-to-completion (unblockable) tasks on top of user threads. System-level virtual machine
monitors may even multiplex the (virtual) hardware threads seen by guest operating systems
on top of some smaller number of physical hardware threads.

Regardless of the level of implementation, we can describe the construction of a scheduler
by starting with an overly simple system and progressively adding functionality. The details
are somewhat tedious (Scott 2009, Secs. 8.6, 12.2.4, and 12.3.4); we outline the basic
ideas here in the interest of having “hooks” that we can call in subsequent descriptions
of synchronization mechanisms. We begin with coroutines—each of which is essentially
a stack and a set of registers—and a single core (or kernel thread) that can execute one
coroutine at a time. To switch to a different coroutine, the core (or kernel thread) calls an
explicit transfer routine, passing as argument a pointer to the context block (descriptor) of
some other coroutine. The transfer routine (1) pushes all registers other than the stack pointer
onto the top of the (current) stack, (2) saves the (updated) stack pointer into the context block
of the current coroutine (typically found by examining a global current_thread variable),
(3) sets current_thread to the address of the new context block (the argument to transfer),
and (4) retrieves a (new) stack pointer from that context block. Because the new coroutine
could only have stopped running by calling transfer (and new coroutines are created in such
a way that they appear to have just called transfer), the program counter need not change—it
will already be at the right instruction. Consequently, the transfer routine simply (5) pops
registers from the top of the (new) stack and returns.

On top of coroutines, we implement non-preemptive threads (otherwise known as run-
until-block or cooperatively scheduled threads) by introducing a global ready list (often
but not always a queue) of runnable-but-not-now-running threads, and a parameterless
reschedule routine that pulls a thread off the ready list and transfers to it. To avoid monop-
olizing resources, a thread should periodically relinquish its core or kernel thread by calling
a routine (often named yield) that enqueues it at the tail of the ready list and then calls
reschedule. To block for synchronization, the thread can call reschedule after adding itself
to some other data structure (other than the ready list), with the expectation that another
thread will move it from that structure to the ready list when it is time for it to continue.

The problem with cooperatively scheduled threads, of course, is the need to cooperate—to
call yield periodically. At the kernel level, where threads may belong to mutually untrusting
applications, this need for cooperation is clearly unacceptable. And even at the user level,
it is highly problematic: how do we arrange to yield often enough (and uniformly enough)
to ensure fairness and interactivity, but not so often that we spend all of our time in the
scheduler? The answer 1s preemption: we arrange for periodic timer interrupts (at the kernel
level) or signals (at the user level) and install a handler for the timer that simulates a call
to yield in the currently running thread. To avoid races with handlers when accessing the
ready list or other scheduler data structures, we temporarily disable interrupts (signals) when
executing scheduler operations explicitly.

Given transfer, reschedule/yield, and preemption, we can multiplex concurrent kernel
or user threads on a single core or kernel thread. To accommodate true parallelism, we
Run to completion (unblockable) tasks on top of user threads. System level virtual machine monitors may even multiplex the (virtual) hardware threads seen by guest operating systems on top of some smaller number of physical hardware threads.

Regardless of the level of implementation, we can describe the construction of a scheduler by starting with an overly simple system and progressively adding functionality. The details are somewhat tedious (Scott two thousand nine, Sections eight point six, twelve point two point four, and twelve point three point four); we outline the basic ideas here in the interest of having “hooks” that we can call in subsequent descriptions of synchronization mechanisms. We begin with coroutines—each of which is essentially a stack and a set of registers—and a single core (or kernel thread) that can execute one coroutine at a time. To switch to a different coroutine, the core (or kernel thread) calls an explicit transfer routine, passing as argument a pointer to the context block (descriptor) of some other coroutine. The transfer routine number one pushes all registers other than the stack pointer onto the top of the (current) stack, number two saves the (updated) stack pointer into the context block of the current coroutine (typically found by examining a global current thread variable), number three sets current thread to the address of the new context block (the argument to transfer), and number four retrieves a (new) stack pointer from that context block. Because the new coroutine could only have stopped running by calling transfer (and new coroutines are created in such a way that they appear to have just called transfer), the program counter need not change—it will already be at the right instruction. Consequently, the transfer routine simply number five pops registers from the top of the (new) stack and returns.

On top of coroutines, we implement non preemptive threads (otherwise known as run until block or cooperatively scheduled threads) by introducing a global ready list (often but not always a queue) of runnable but not now running threads, and a parameterless reschedule routine that pulls a thread off the ready list and transfers to it. To avoid monopolizing resources, a thread should periodically relinquish its core or kernel thread by calling a routine (often named yield) that enqueues it at the tail of the ready list and then calls reschedule. To block for synchronization, the thread can call reschedule after adding itself to some other data structure (other than the ready list), with the expectation that another thread will move it from that structure to the ready list when it is time for it to continue.

The problem with cooperatively scheduled threads, of course, is the need to cooperate—to call yield periodically. At the kernel level, where threads may belong to mutually untrusting applications, this need for cooperation is clearly unacceptable. And even at the user level, it is highly problematic: how do we arrange to yield often enough (and uniformly enough) to ensure fairness and interactivity, but not so often that we spend all of our time in the scheduler? The answer is preemption: we arrange for periodic timer interrupts (at the kernel level) or signals (at the user level) and install a handler for the timer that simulates a call to yield in the currently running thread. To avoid races with handlers when accessing the ready list or other scheduler data structures, we temporarily disable interrupts (signals) when executing scheduler operations explicitly.

Given transfer, reschedule slash yield, and preemption, we can multiplex concurrent kernel or user threads on a single core or kernel thread. To accommodate true parallelism, we
The fundamental challenge in modern computing involves efficiently multiplexing a limited number of physical processing units among a much larger set of virtualized tasks. This discussion elaborates on the evolution of scheduling paradigms, beginning with basic run to completion models and progressing to sophisticated preemptive multitasking, which is essential for achieving concurrent and parallel execution.

Initially, a simplistic approach to task management involves *run to completion* operations, also known as unblockable tasks. In this model, once a task begins execution, it continues uninterrupted until its completion or until it voluntarily yields control. This paradigm offers simplicity but presents significant limitations in systems requiring responsiveness or where multiple tasks must share resources without one monopolizing the processor. System level virtual machines, for instance, might layer complex virtual hardware threads on top of a smaller count of physical hardware threads, necessitating more advanced scheduling.

To introduce a form of user level concurrency, the concept of *coroutines* emerges as a foundational building block. A coroutine is an execution context defined by its dedicated stack and a specific set of processor registers. Unlike true operating system threads, coroutines provide *cooperative* multitasking, meaning they explicitly yield control to one another. The core mechanism for this cooperative transfer is an explicit `transfer` routine. When one coroutine, say coroutine A, wishes to transfer control to coroutine B, the `transfer` routine performs a series of critical steps. First, it pushes all relevant registers, crucial for preserving the current execution state of coroutine A, onto coroutine A's stack. Second, it updates and saves coroutine A's stack pointer into its corresponding *context block* or descriptor. The context block serves as a persistent record of the coroutine's state when it is not active. Third, the routine then modifies a global `current_thread` variable to point to the address of coroutine B's context block. Fourth, it retrieves the saved stack pointer from coroutine B's context block. Finally, it pops the saved registers from coroutine B's stack, thereby restoring its previous execution state. An interesting consequence of this explicit transfer is that if a new coroutine is created by calling `transfer`, its program counter does not need to be explicitly modified, as the control flow inherently shifts to the new context where the program counter was last saved.

Building upon coroutines, a system can implement *non preemptive threads*, often referred to as *run until block* or *cooperatively scheduled threads*. This is achieved by introducing a *global ready list*, which is typically a queue, but not always, holding all threads that are runnable but not currently executing. A parameterless `reschedule` routine is introduced. Its function is to pull a thread from the head of this ready list and initiate a `transfer` operation to it, effectively switching execution to that selected thread. To prevent any single thread from monopolizing the processor, a `yield` routine is also provided. When a thread calls `yield`, it voluntarily relinquishes its claim to the core or kernel thread, enqueues itself at the tail of the ready list, and then immediately invokes the `reschedule` routine. This ensures that another thread from the ready list gets an opportunity to execute. For synchronization, such as waiting for a resource or an event, a thread can block itself by moving from the ready list to some other data structure, like a wait queue, and then calling `reschedule`, with the expectation that another thread will eventually move it back to the ready list when its awaited condition is met.

However, the cooperative scheduling paradigm inherently presents several significant drawbacks. The primary issue is the reliance on the application programmer to periodically call `yield`. If a thread fails to yield, it can starve other threads, leading to poor fairness and responsiveness. At the kernel level, where various applications and system components, often mutually untrusting, share resources, this cooperative model is completely unacceptable. Ensuring consistent fairness and interactivity becomes highly problematic, as there is no guarantee that threads will yield in a timely or uniform manner. The fundamental question becomes: how can the system ensure that threads yield often enough to maintain responsiveness without excessively frequent context switches that would consume disproportionate processing time?

The answer lies in *preemption*. Preemption is a mechanism where the operating system or scheduler forcibly interrupts a running task to give control to another. This is typically achieved through periodic *timer interrupts*. At the kernel level, a hardware timer is configured to generate an interrupt at fixed intervals, for example, every few milliseconds. When such an interrupt occurs, the system's interrupt handler is invoked. This handler effectively simulates a `yield` call by saving the context of the currently running thread and then invoking the scheduler to select another thread from the ready list to execute. To prevent *race conditions*—where multiple execution contexts might simultaneously access and corrupt shared scheduler data structures like the ready list—it is crucial to temporarily disable interrupts when the scheduler is performing critical operations. This ensures atomic updates to these data structures.

By combining the concepts of `transfer` for context switching, `reschedule` and `yield` for cooperative control, and crucially, preemption via timer interrupts, a system can effectively multiplex concurrent kernel or user threads on a single core or a set of kernel threads. This layered approach forms the basis for sophisticated operating system schedulers, ultimately providing the illusion of simultaneous execution and laying the groundwork necessary to accommodate true parallelism across multiple processing cores.
