6.3 Read-Copy Update 115

applications. Most revolve around a global counter C and a global set S of counters, indexed
by thread 1d. C is monotonically increasing (extensions can accommodate rollover): in the
simplest implementation, it 1s incremented at the end of each write operation. In a partial
violation of the no-shared-updates property, S is maintained by readers. Specifically, S[i]
will be zero if thread i is not currently executing a reader operation. Otherwise, S[i] will
be j if C was j when thread i’s current reader operation began. To ensure a grace period
has passed (and all old readers have finished), a writer iterates through S, waiting for each
element to be either zero or a value greater than or equal to the value just written to C.
Assuming that each set element lies in a separate cache line, the updates performed by
reader operations will usually be cache hits, with almost no performance impact. Moreover,
since each element 1s updated by only one thread, and the visibility of updates can safely be
delayed, no synchronizing instructions are required (assuming memory ordering issues are
handled as described in the following paragraph).

Memory Ordering. When beginning a read operation with grace periods based on the global
counter and set, a thread must update its entry in S using a ||R store, or follow the update
with a W||R fence. At the end of the operation, it must update its entry with a Rl store, or
precede the update with a R||W fence. When reading a pointer that might have been updated
by a writer, a reader must use a R|[load, or follow the read with a R||R fence. Among these
three forms of ordering, the W||R case is typically the most expensive (the others will in fact
be free on a TSO machine). We can avoid the overhead of W||R ordering in the common
case by requiring the writer to interrupt all potential readers (e.g., with a POSIX signal) at
the end of a write operation. The signal handler can then “handshake” with the writer, using
appropriate memory barriers, thereby ensuring that (a) each reader’s update of its element in
S is visible to the writer, and (b) the writer’s updates to shared data are visible to all readers.
Assuming that writers are rare, the cost of the signal handling will be outweighed by the
no-longer-required W||R ordering in the (much more numerous) reader operations.

For writers, the rules are similar to the seqlock case: to avoid causality loops when readers
inspect more than one RCU-updatable pointer, writers must use sequentially consistent
(write-atomic) synchronizing stores to modify those pointers.

Multi-write Operations. The single-pointer update property may become a single-word
update in data structures that use some other technique for traversal and space management.
In many cases, it may also be possible to accommodate multi-word updates, by exploiting
application-specific knowledge. In their original paper, for example, McKenney et al. (2001)
presented an RCU version of doubly-linked lists, in which a writer must update both forward
and backward pointers. The key to their solution is to require that readers search for nodes
only in the forward direction. The backward pointers can then be treated as hints that facilitate
constant-time deletion. Readers serialize before or after a writer depending on whether they
see the change to the forward pointer.

In subsequent work with a similar flavor, Clements et al. (2012) show how to implement
an RCU version of balanced external binary trees. They begin by noting that rebalancing is
a semantically neutral operation, and can be separated from insertion and deletion, allowing
Six point three Read Copy Update.

Applications most revolve around a global counter C and a global set S of counters, indexed by thread I D. C is monotonically increasing (extensions can accommodate rollover). In the simplest implementation, it is incremented at the end of each write operation. In a partial violation of the no shared updates property, S is maintained by readers. Specifically, S index i will be zero if thread i is not currently executing a reader operation. Otherwise, S index i will be j if C was j when thread i's current reader operation began. To ensure a grace period has passed (and all old readers have finished), a writer iterates through S, waiting for each element to be either zero or a value greater than or equal to the value just written to C. Assuming that each set element lies in a separate cache line, the updates performed by reader operations will usually be cache hits, with almost no performance impact. Moreover, since each element is updated by only one thread, and the visibility of updates can safely be delayed, no synchronizing instructions are required, assuming memory ordering issues are handled as described in the following paragraph.

Memory Ordering. When beginning a read operation with grace periods based on the global counter and set, a thread must update its entry in S using an Or Or store, or follow the update with a W Or R fence. At the end of the operation, it must update its entry with an R Or R store, or precede the update with an R Or W fence. When reading a pointer that might have been updated by a writer, a reader must use an R Or load, or follow the read with an R Or R fence. Among these three forms of ordering, the W Or R case is typically the most expensive. The others will in fact be free on a T S O machine. We can avoid the overhead of W Or R ordering in the common case by requiring the writer to interrupt all potential readers, for example, with a P O S I X signal, at the end of a write operation. The signal handler can then handshake with the writer, using appropriate memory barriers, thereby ensuring that, A, each reader’s update of its element in S is visible to the writer, and, B, the writer’s updates to shared data are visible to all readers. Assuming that writers are rare, the cost of the signal handling will be outweighed by the no longer required W Or R ordering in the much more numerous reader operations. For writers, the rules are similar to the seqlock case. To avoid causality loops when readers inspect more than one R C U updatable pointer, writers must use sequentially consistent, write atomic, synchronizing stores to modify those pointers.

Multi write Operations. The single pointer update property may become a single word update in data structures that use some other technique for traversal and space management. In many cases, it may also be possible to accommodate multi word updates, by exploiting application specific knowledge. In their original paper, for example, McKenney and others in two thousand one, presented an R C U version of doubly linked lists, in which a writer must update both forward and backward pointers. The key to their solution is to require that readers search for nodes only in the forward direction. The backward pointers can then be treated as hints that facilitate constant time deletion. Readers serialize before or after a writer depending on whether they see the change to the forward pointer.

In subsequent work with a similar flavor, Clements and others in two thousand twelve, show how to implement an R C U version of balanced external binary trees. They begin by noting that rebalancing is a semantically neutral operation, and can be separated from insertion and deletion, allowing.
The concept of Read Copy Update, or R C U, represents a sophisticated approach to concurrency control in multi-processor systems, particularly valuable for scenarios dominated by read operations. At its core, R C U is a non-blocking synchronization primitive that permits concurrent readers to access shared data without explicit locking, thereby minimizing overhead and enhancing scalability. The fundamental principle revolves around the notion of a "grace period," during which writers create a new version of a data structure, while existing readers continue to access the old version. Only after all readers that were active when the update began have completed their access to the old data is the old memory safely reclaimed, often through a garbage collection mechanism or explicit deallocation.

A common implementation strategy for enforcing this grace period, as described, utilizes a global counter, denoted as C, and a per-thread set of counters, S, indexed by thread I D. The global counter C is designed to be monotonically increasing, incrementing at the conclusion of each write operation. This increment serves as a timestamp for the completion of a writer's work. Concurrently, each thread maintains an entry in the S array, specifically S index I. This entry is set to zero if thread I is not actively executing a reader operation. If it is a reader, S index I stores the value of C that was current when thread I commenced its reader operation. To ensure that all pre-existing readers have completed their access to the old data, a writer performs a critical validation step: it iterates through the entire S array, waiting for each element S index I to either be zero, indicating no active reader, or to be a value greater than or equal to the C value that the writer just recorded. This waiting phase is the essence of the grace period, guaranteeing that all readers initiated prior to the writer's update have observed the state of the data before the writer's modifications became visible. The architectural design decision to place each element of S in a separate cache line is a strategic optimization to mitigate false sharing, a common issue in multi-core systems where unrelated data residing in the same cache line causes unnecessary cache invalidations. This separation ensures that updates to one thread's S entry do not inadvertently invalidate the cache line containing another thread's S entry, thereby improving cache coherency and overall performance.

The integrity of R C U hinges critically on precise memory ordering, particularly in weakly ordered memory models prevalent in modern C P U architectures. When a thread initiates a read operation under R C U, it must update its corresponding entry in the S array. This update can be performed using a Read Or Store (R bar R store) operation, or by explicitly following the update with a Write Or Read (W bar R) fence. The choice of barrier type dictates the strictness of the ordering guarantees. Similarly, upon concluding a read operation, the thread updates its S entry, typically using a Read Or Write (R bar W) store or by following the update with a Read Or Read (R bar R) fence. These fences are essential for ensuring that the writer observes the reader's entry and exit from its critical section in a causally consistent manner. Furthermore, when a reader accesses a pointer that might have been concurrently updated by a writer, it must employ a Read Or Load (R bar R load) or follow the read with a Read Or Read (R bar R) fence. This ensures that the pointer value read by the thread is consistent with its ongoing operation. The Write Or Read (W bar R) fence is generally the most expensive due to its strong ordering semantics, often involving a global memory barrier that flushes write buffers and ensures visibility across all processors. To circumvent this performance penalty in common scenarios, an alternative approach is to avoid the explicit W bar R ordering by employing mechanisms such as a P O S I X signal. This allows the writer to interrupt potential readers, effectively achieving the grace period via an operating system level inter-process communication rather than direct memory ordering constraints. For writers, the rules for managing R C U-updatable pointers are akin to those for `seq_lock` mechanisms, which often require careful handling of causality loops to prevent readers from observing inconsistent intermediate states. Writers must sequentially consistently inspect multiple R C U-updatable pointers and use `write-atomic` synchronizing stores to ensure that modifications to these pointers appear as a single, atomic operation to other concurrent actors.

Beyond single-pointer updates, R C U can be extended to manage more complex data structures and multi-word modifications, although this often introduces additional design challenges. The inherent "single-pointer update property" of basic R C U implies that the most straightforward application involves atomically changing a single pointer to point from an old version of data to a new one. However, for data structures that require simultaneous modification of multiple related fields or pointers, such as a doubly-linked list or a tree, application-specific knowledge becomes paramount for correct traversal and space management. For instance, in their seminal work from two thousand one, McKenney and colleagues demonstrated an R C U-compatible version of doubly-linked lists. Their innovation involved ensuring that readers traverse the list exclusively using forward pointers. Backward pointers, while essential for efficient deletion, were treated as mere hints, enabling constant-time deletion without requiring readers to synchronize on them. This design decouples the complex update of backward pointers from the high-concurrency read path. In a similar vein, Clements and collaborators in two thousand twelve explored the application of R C U to balanced external binary trees. They illustrated how complex operations like tree rebalancing, insertion, and deletion could be decomposed into semantically neutral steps. By separating the rebalancing process from the fundamental insertion and deletion operations, it becomes possible to perform the rebalancing as an asynchronous, R C U-safe operation, ensuring that concurrent readers always operate on a consistent, if not perfectly balanced, version of the tree, while the new balanced structure is gradually made available. This exemplifies a powerful design pattern in concurrent programming: separating computationally intensive or highly contended operations from the read path to maximize concurrency and throughput.
