9 Transactional Memory 187

Because they may encompass as little as two or three memory accesses, they cannot afford
the overhead of crash-surviving disk 1/0. At the same time, because they are intended mainly
for synchronization among threads of a single program (which usually live and die together),
durability is much less important than it is in the database world."

Given a correct sequential implementation of a data structure (a tree-based set, for exam-
ple), TM allows the author of a parallel program to reuse the sequential code, with guaranteed
correctness, in an almost trivial fashion:

class pset pset.insert(item x):
setS atomic
bool pset.lookup(item x): S.insert(x)
atomic pset.remove(item x):
return S.lookup(x) atomic

S.remove(x)

Moreover, unlike lock-based critical sections, transactions can safely be composed into
larger atomic operations:

pset PQ

atomic
if Plookup(x)
Premove(x)
Q.insert(x)

Here the fact that Plookup, Premove, and Q.insert contain nested transactions is entirely
harmless. Moreover, if some other thread attempts a concurrent, symmetric move from Q
to P, deadlock can never result.

The original intent of TM was to simplify the construction of library-level concurrent
data abstractions, with relatively small operations. Current hardware (HTM) and (to a lesser
extent) software (STM) implementations serve this purpose well. How much larger trans-
actions can get before they conflict too often to scale is still an open question.

The following two sections consider software and hardware transactions in turn; the
third takes a closer look at challenges—many of them initially unanticipated—that have
complicated the development of TM, and may yet determine the degree of its success.

While early STM implementations were provided simply as library packages—with entry
points to begin a transaction, read or write a shared location transactionally, and (attempt to)
commit a transaction—experience suggests that such libraries are too cumbersome for most
programmers to use (Dalessandro et al. 2007). We assume through the rest of this chapter
that TM is embedded in a programming language, and that all necessary hooks (including
instrumentation of STM loads and stores) are generated by the compiler.

I The prospect of byte-addressable nonvolatile memory (successor technologies to DRAM) has begun
to undermine this argument, leading concurrency and TM researchers to consider durable data struc-
tures and transactions. We do not consider such extensions here.
Nine Transactional Memory. One hundred eighty seven. Because they may encompass as little as two or three memory accesses, they cannot afford the overhead of crash surviving disk I O. At the same time, because they are intended mainly for synchronization among threads of a single program, which usually live and die together, durability is much less important than it is in the database world. Given a correct sequential implementation of a data structure tree based set, for example, T M allows the author of a parallel program to reuse the sequential code, with guaranteed correctness, in an almost trivial fashion.

Class pset.
Set S.
Bool pset dot lookup open parenthesis item x close parenthesis.
Atomic S dot insert x.
Return S dot lookup open parenthesis x close parenthesis.

Pset dot remove open parenthesis item x close parenthesis.
Atomic S dot remove x.

Moreover, unlike lock based critical sections, transactions can safely be composed into larger atomic operations.

Pset P comma Q.
...
Atomic.
If P dot lookup open parenthesis x close parenthesis.
P dot remove open parenthesis x close parenthesis.
Q dot insert open parenthesis x close parenthesis.

Here the fact that P dot lookup comma P dot remove comma And Q dot insert contain nested transactions is entirely harmless. Moreover, if some other thread attempts a concurrent, symmetric move from Q to P, deadlock can never result. The original intent of T M was to simplify the construction of library level concurrent data abstractions, with relatively small operations. Current hardware T M And to a lesser extent software S T M implementations serve this purpose well. How much larger transactions can get before they conflict too often to scale is still an open question. The following two sections consider software And hardware transactions in turn, the third takes a closer look at challenges many of them initially unanticipated that have complicated the development of T M, And may yet determine the degree of its success. While early S T M implementations were provided simply as library packages with entry points to begin a transaction, read or write a shared location transactionally, And attempt to commit a transaction experience suggests that such libraries are too cumbersome for most programmers to use Dalessandro et al. two thousand seven. We assume through the rest of this chapter that T M is embedded in a programming language, And that all necessary hooks including instrumentation of S T M loads And stores are generated by the compiler.

The prospect of byte addressable nonvolatile memory successor technologies to D Ram has begun to undermine this argument, leading concurrency and T M researchers to consider durable data structures and transactions. We do not consider such extensions here.
Transactional memory aims to simplify concurrent programming by providing atomic execution of code blocks, analogous to database transactions. Unlike traditional lock-based mechanisms, which can lead to complexities like deadlocks, transactional memory allows for composition of operations, potentially simplifying the construction of library-level concurrent data abstractions. Hardware transactional memory, or H T M, and software transactional memory, or S T M, are implementations that serve this purpose. The efficacy of these systems, particularly in handling a high volume of conflicts between concurrent transactions, remains an open area of research.

The provided code snippet illustrates a simplified transactional set, demonstrating how operations like `insert` and `remove` can be wrapped in an `atomic` block. This ensures that the operations within the block are executed indivisibly, meaning they either complete successfully or have no effect, thereby maintaining data consistency. The `lookup` operation in this example is also shown within an `atomic` context, returning the result of the underlying set's lookup. The snippet further illustrates the composability of transactions, showing a scenario where a lookup on one set `P` triggers a remove on `P` and an insert into another set `Q` within a single atomic transaction. This ability to combine multiple operations into a larger atomic unit is a key advantage of transactional memory, as it can prevent race conditions and deadlocks that might arise from fine-grained locking. For instance, if `P.lookup(x)` returns true, a transaction might attempt to remove `x` from `P` and insert `x` into `Q`. Without transactional semantics, if another thread concurrently modified `P` or `Q` between these operations, the program's state could become inconsistent. The use of the `atomic` keyword here guarantees that both `P.remove(x)` and `Q.insert(x)` either succeed together or are rolled back, maintaining atomicity and isolation.

The text further delves into the challenges and nuances of transactional memory. It points out that while the intent is to simplify concurrency, the practical implementation and performance can be intricate. The statement that "How much larger transactions can get before they conflict too often to scale is still an open question" highlights the trade-offs between the granularity of transactional operations and their performance characteristics in highly concurrent environments. When many threads attempt to access and modify shared data concurrently, conflicts are more likely to occur. In transactional memory, conflicts typically lead to transaction aborts and retries, which can degrade performance if they happen frequently. The text also contrasts hardware and software implementations, suggesting that hardware transactional memory (H T M) and software transactional memory (S T M) have different characteristics and challenges. Early S T M implementations are described as library packages that provide transactional semantics for operations like read and write, with the implication that these can be cumbersome for programmers. The discussion implies that the effectiveness of S T M relies heavily on how efficiently it can track memory accesses, detect conflicts, and manage transaction rollbacks and retries.

The footnote at the bottom of the page introduces a forward-looking perspective, referencing the prospect of byte-addressable nonvolatile memory as a potential successor to D Ram. It suggests that such persistent memory technologies could significantly impact concurrency control and transactional memory research, potentially enabling more durable data structures and transactions. However, the authors state they will not delve into these extensions within the current discussion, focusing instead on the more established aspects of transactional memory. This implies an understanding that advancements in memory technology can fundamentally alter the landscape of system design and concurrency primitives. The very nature of transactional memory, with its focus on atomicity and isolation, becomes even more relevant in scenarios where data persistence and crash recovery are primary concerns, as offered by nonvolatile memory.
