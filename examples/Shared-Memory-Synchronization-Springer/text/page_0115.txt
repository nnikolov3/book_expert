=

Check for
updates

Synchronization and Scheduling

So far in this monograph, we have emphasized busy-wait synchronization. In the current
chapter we turn to mechanisms built on top of a scheduler, which multiplexes some collection
of cores among a (typically larger) set of threads, switching among them from time to time
and—in particular—when the current thread needs to wait for synchronization.

We begin with a brief introduction to scheduling in Sec. 7.1. We then discuss the oldest
(and still most widely used) scheduler-based synchronization mechanism—the semaphore—
in Sec.7.2. Semaphores have a simple, subroutine-call interface. Many scheduler-based
synchronization mechanisms, however, were designed to be embedded in a concurrent pro-
gramming language, with special, non-procedural syntax. We consider the most important
of these—the monitor—in Sec. 7.3, and others—conditional critical regions, futures, and
series-parallel (split-merge) execution—in Sec. 7.4.

With these mechanisms as background, we return in Sec.7.5 to questions surrounding
the interaction of user- and kernel-level code: specifically, how to minimize the number of
context switches, avoid busy-waiting for threads that are not running, and reduce the demand
for kernel resources.

7.1 Scheduling

As outlined in Sec. 1.3, scheduling often occurs at more than one level of a system. The
operating system kernel, for example, may multiplex kernel threads on top of hardware
cores, while a user-level run-time package multiplexes user threads on top of the kernel
threads. On many machines, the processor itself may schedule multiple hardware threads
on the pipeline(s) of any given core (in which case the kernel schedules its threads on top
of hardware threads, not cores). Library packages (e.g., in Java) may sometimes schedule

© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 119
M. L. Scott and T. Brown, Shared-Memory Synchronization, Synthesis Lectures
on Computer Architecture, https://doi.org/10.1007/978-3-031-38684-8_7
The page shows a check for updates button. The main title of this section is Synchronization and Scheduling, which is Chapter seven.

So far in this monograph, we have emphasized busy wait synchronization. In the current chapter we turn to mechanisms built on top of a scheduler, which multiplexes some collection of cores among a typically larger set of threads, switching among them from time to time and, in particular, when the current thread needs to wait for synchronization.

We begin with a brief introduction to scheduling in Section seven point one. We then discuss the oldest, and still most widely used, scheduler based synchronization mechanism, the semaphore, in Section seven point two. Semaphores have a simple, subroutine call interface. Many scheduler based synchronization mechanisms, however, were designed to be embedded in a concurrent programming language, with special, non procedural syntax. We consider the most important of these, the monitor, in Section seven point three, and others, conditional critical regions, futures, and series parallel, split merge, execution, in Section seven point four.

With these mechanisms as background, we return in Section seven point five to questions surrounding the interaction of user and kernel level code. Specifically, how to minimize the number of context switches, avoid busy waiting for threads that are not running, and reduce the demand for kernel resources.

Seven point one Scheduling.

As outlined in Section one point three, scheduling often occurs at more than one level of a system. The operating system kernel, for example, may multiplex kernel threads on top of hardware cores, while a user level run time package multiplexes user threads on top of the kernel threads. On many machines, the processor itself may schedule multiple hardware threads on the pipelines of any given core, in which case the kernel schedules its threads on top of hardware threads, not cores. Library packages, for example, in Java, may sometimes schedule.

Copyright The Author S, under exclusive license to Springer Nature Switzerland A G two thousand twenty four. M. L. Scott and T. Brown, Shared Memory Synchronization, Synthesis Lectures on Computer Architecture, H T T P S colon slash slash D O I dot org slash ten point one zero zero seven slash nine seven eight dash three dash zero three one dash three eight six eight four dash eight underscore seven.
This monograph delves into the intricate realm of synchronization and scheduling within concurrent computing systems. A central theme is the distinction between busy wait synchronization and scheduler based synchronization. Busy waiting involves a process or thread continuously checking a condition, consuming C P U cycles in a loop until the condition is met. While simple, this approach is inherently inefficient as it wastes processing power that could be utilized by other tasks. In contrast, scheduler based synchronization mechanisms allow a waiting thread to yield the C P U, enabling the operating system's scheduler to allocate that resource to another ready thread. This design significantly improves system throughput and responsiveness by minimizing idle C P U time.

The core of this efficiency lies with the *scheduler*, a fundamental component of any modern operating system or runtime environment. Its primary function is to multiplex, or time share, the available C P U cores among a potentially large set of threads. This involves making critical decisions about which thread executes when, and for how long, managing the transitions between them. The scheduler ensures that threads requiring synchronization are handled efficiently, allowing a thread to be suspended if it needs to wait for a specific event or resource, and then reactivated only when the necessary conditions are satisfied.

The text introduces a progression of synchronization primitives and mechanisms. The semaphore, conceived by Edsger Dijkstra, stands as one of the earliest and most widely adopted scheduler based synchronization tools. Semaphores provide a simple, atomic subroutine call interface, typically encompassing `wait` (also known as `P` or `down`) and `signal` (also known as `V` or `up`) operations. These operations are used to control access to shared resources or to signal the occurrence of events between threads. `Wait` decrements a counter and blocks if the counter falls below zero, indicating resource unavailability. `Signal` increments the counter and potentially unblocks a waiting thread.

Beyond semaphores, the discussion extends to more advanced synchronization constructs. Monitors, for instance, are higher level synchronization mechanisms often integrated directly into programming languages. They encapsulate shared data and the procedures that operate on that data, ensuring mutual exclusion: only one thread can execute within a monitor's procedures at any given time. Monitors often incorporate condition variables, allowing threads to wait for specific conditions to become true while inside the monitor, and to signal other threads when those conditions change. Conditional critical regions represent another programming language construct for synchronization, allowing a block of code to be executed only when a specified Boolean condition is true, and ensuring mutual exclusion for that region. Futures are a different paradigm, representing the result of a computation that may not yet be complete. A thread can query a future, and if the result is not ready, it can suspend until the value becomes available, acting as an implicit synchronization point. Lastly, concepts like series parallel, or split merge, execution describe computational patterns where tasks can be dynamically created and executed in parallel, then explicitly synchronized and recombined, representing complex data flow or task parallelism models.

A crucial aspect of system design highlighted is the interaction between user level and kernel level code. Kernel level code, part of the operating system kernel, operates in a privileged mode, directly managing hardware resources. User level code, on the other hand, runs in a restricted environment. Efficient synchronization mechanisms are designed to minimize the need for transitions between these two modes, specifically by reducing the number of costly context switches. A context switch involves saving the complete state of the currently executing thread, including its C P U registers and memory map, and loading the state of another thread. This operation incurs significant overhead, measured in microseconds, and frequent context switching can degrade system performance. By employing scheduler based synchronization, threads that cannot proceed can be suspended, avoiding wasteful busy waiting and reducing the demand on kernel resources, thereby minimizing context switch overhead.

The concept of scheduling itself is multifaceted and hierarchical, operating at multiple levels within a computing system. At the lowest, fundamental level, the operating system kernel is responsible for scheduling *kernel threads* directly onto the available hardware cores of the C P U. This is the primary resource allocation layer. Above this, a user level run time package, such as a Java Virtual Machine or a language specific runtime, may manage its own *user threads*. These user threads are typically multiplexed onto a smaller set of kernel threads. For example, a Java application might create many lightweight user threads, but the J V M maps them onto a few kernel threads, which are then scheduled by the O S. This creates a many to one or many to many mapping between user threads and kernel threads.

Furthermore, modern processors themselves implement internal scheduling mechanisms. Processors with features like simultaneous multi threading, such as Intel's Hyper Threading technology, can present a single physical core as multiple *hardware threads*. This allows the processor's micro architecture to schedule multiple independent instruction streams concurrently on shared execution pipelines within a single core. Thus, even at the hardware level, there is a form of scheduling occurring, where the processor determines how to interleave or execute instructions from these different hardware threads to maximize pipeline utilization. Finally, some high level library packages, particularly in object oriented languages like Java, can also implement their own forms of thread management that, while ultimately relying on the underlying operating system and hardware, appear to schedule threads directly. This multi tiered approach to scheduling, from the hardware micro architecture to the operating system kernel and up to user level run times and libraries, is essential for achieving efficient resource utilization and responsive concurrent program execution.
