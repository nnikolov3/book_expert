2.2 Memory Consistency 27

Note that work migrating into a critical section cannot create correctness or progress
issues. If work was safe to perform outside of a critical section, it remains safe to perform
inside the critical section. Progress could potentially be threatened if the work that migrates
into the critical section involves acquiring additional locks. However, the ordering constraints
on lock acquire and release operations prevent this. At worst, work migrating into a critical
section may decrease the potential for parallel execution, leading to lower performance.

2.3 Atomic Primitives

To facilitate the construction of synchronization algorithms and concurrent data structures,
most modern architectures provide instructions capable of updating (i.e., reading and writ-
ing) a memory location as a single atomic operation. We saw a simple example—the
test_and_set instruction (TAS)—in Sec. 1.3. A longer list of common instructions appears
in Table 2.4. Note that for each of these, when it appears in our pseudocode, we per-
mit an optional, final argument that indicates local ordering constraints. The instruction
a.CAS(old, new, W||), for example, indicates a CAS that is ordered after all preceding write
accesses 1n its thread.

Originally introduced on mainframes of the 1960s, TAS and swap are still available on
several modern machines, including the x86. FAA and FAI were introduced for “combining
network” machines of the 1980s (Kruskal et al. 1988); the former is still supported today
on the x86. The semantics of TAS, swap, FAI, and FAA should all be self-explanatory. Note
that they all return the value of the target location before any change was made.

CAS and LL/SC are universal primitives, in a sense we will define formally in Sec. 3.3.
In practical terms, we can use them to emulate essentially arbitrary (single-word) read-
modify-write (fetch_and_®) operations (including all the other operations in Table 2.4).
CAS was originally introduced in the 1973 version of the IBM 370 architecture (IBM 1975;
Gifford et al. 1987; Brown and Smith 1975). It was also provided by IA-64 (Itanium) and
SPARC processors, and is still available on x86 and certain recent Arm machines. LL/SC
was originally proposed for the S-1 AAP Multiprocessor at Lawrence Livermore National
Laboratory (Jensen et al. 1987). It was also provided by Alpha processors, and is found on
modern Power, MIPS, and Arm machines. Interestingly, Arm’s AArch64 (v8/9) architecture
introduced CAS in addition to LL/SC, though in at least some implementations the latter
remains significantly faster (Jesus and Weiland 2023).

CAS takes three arguments: a memory location, an old value that is expected to occupy
that location, and a new value that should be placed in the location if indeed the old value 1s
currently there. The instruction returns a Boolean value indicating whether the replacement
occurred successfully.
Two point two, Memory Consistency.
Note that work migrating into a critical section cannot create correctness or progress issues. If work was safe to perform outside of a critical section, it remains safe to perform inside the critical section. Progress could potentially be threatened if the work that migrates into the critical section involves acquiring additional locks. However, the ordering constraints on lock acquire and release operations prevent this. At worst, work migrating into a critical section may decrease the potential for parallel execution, leading to lower performance.

Two point three, Atomic Primitives.
To facilitate the construction of synchronization algorithms and concurrent data structures, most modern architectures provide instructions capable of updating, that is, reading and writing, a memory location as a single atomic operation. We saw a simple example, the test and set instruction, abbreviated as T A S, in Section one point three. A longer list of common instructions appears in Table two point four. Note that for each of these, when it appears in our pseudocode, we permit an optional, final argument that indicates local ordering constraints. The instruction A C A S, open parenthesis old, new, W, or or, close parenthesis, for example, indicates a C A S that is ordered after all preceding write accesses in its thread.

Originally introduced on mainframes of the nineteen sixties, T A S and swap are still available on several modern machines, including the x eight six. F A A and F A I were introduced for combining network machines of the nineteen eighties, citing Kruskal et al. nineteen eighty eight. The former is still supported today on the x eight six. The semantics of T A S, swap, F A I, and F A A should all be self-explanatory. Note that they all return the value of the target location before any change was made.

C A S and L L slash S C are universal primitives. In a sense, we will define formally in Section three point three. In practical terms, we can use them to emulate essentially arbitrary, single word, read modify write, fetch and phi, operations, including all the other operations in Table two point four. C A S was originally introduced in the nineteen seventy three version of the I B M three seventy architecture, citing I B M nineteen seventy five, and Gifford et al. nineteen eighty seven, Brown and Smith nineteen seventy five. It was also provided by I A sixty four Itanium and S P A R C processors, and is still available on x eight six and certain recent Arm machines. L L slash S C was originally proposed for the S one A A P multiprocessor at Lawrence Livermore National Laboratory, citing Jensen et al. nineteen eighty seven. It was also provided by Alpha processors, and is found on modern Power, M I P S, and Arm machines. Interestingly, Arm's A Arch sixty four, version eight slash version nine, architecture introduced C A S in addition to L L slash S C, though in at least some implementations the latter remains significantly faster, citing Jesus and Weiland two thousand twenty three.

C A S takes three arguments: a memory location, an old value that is expected to occupy that location, and a new value that should be placed in the location if indeed the old value is currently there. The instruction returns a Boolean value indicating whether the replacement occurred successfully.
The discussion centers on the fundamental concepts of memory consistency and atomic operations, which are pivotal in the design and correctness of concurrent systems. Memory consistency models establish the rules for how memory operations, specifically reads and writes, are ordered and made visible across multiple processors or threads. Ensuring strict ordering for all operations maintains sequential consistency, where the result of any execution is the same as if the operations of all processors were executed in some sequential order, and the operations of each individual processor appear in the order specified by its program. However, this strictness can limit performance in parallel environments. The text highlights a common trade off: while moving work into a critical section, a code segment that accesses shared resources and must execute atomically, might not compromise program correctness, it can degrade overall system performance. This occurs because such migration may necessitate additional lock acquisitions, thereby reducing the potential for concurrent execution and true parallelism. The challenge lies in balancing the need for precise memory ordering, which ensures program correctness, with the desire for higher performance through increased parallelism.

To overcome these challenges and facilitate the construction of robust synchronization algorithms and concurrent data structures, modern computer architectures integrate atomic primitives. These are special instructions that execute as a single, indivisible unit, guaranteeing that their internal operations, such as a read and a subsequent write to a memory location, cannot be interrupted or observed in an inconsistent state by other concurrent threads. A basic example is the `test and set` instruction, which atomically reads a value from a memory location and then writes a new value, typically one, returning the original value. A more powerful and versatile primitive is `C A S`, or Compare And Swap. This instruction typically takes three arguments: a memory location, an expected old value, and a new value. The core principle is that the `C A S` operation atomically checks if the current value at the memory location matches the expected old value. If it does, the new value is written to that location. The instruction then returns a Boolean value indicating whether the swap was successful. Some implementations of `C A S` also incorporate an optional argument to specify local ordering constraints, ensuring that the `C A S` operation is ordered correctly relative to preceding memory accesses within the thread, which is vital for adhering to particular memory consistency models.

The origins of these atomic primitives trace back to early mainframe architectures of the nineteen sixties, with `test and set` and `swap` operations being among the earliest forms. Later, in the nineteen eighties, primitives like `F A A`, Fetch And Add, and `F A I`, Fetch And Increment, were introduced, particularly for "combining network" machines designed to efficiently aggregate concurrent updates. The key characteristic of these historical primitives is that they return the value that existed at the memory location *before* the atomic modification took place. `C A S` and the `L L / S C` (Load Linked / Store Conditional) instruction pair are widely recognized as universal primitives. This "universality" implies their capacity to emulate virtually any other atomic read modify write operation or to construct complex synchronization mechanisms, a concept formally defined within the theoretical framework of concurrent computing.

From an architectural standpoint, `C A S` was first introduced in the nineteen seventy three version of the I B M three seventy architecture. It has since been adopted by many contemporary processor architectures, including I A sixty four, a sixty four bit I S A, S P A R C processors, Alpha processors, Power, MIPS, and the Arm `A A R C H sixty four` version eight slash nine architecture. In many of these modern architectures, `C A S` is provided in addition to, or as an alternative to, `L L / S C` pairs. Empirical observations have shown that in certain implementations, `C A S` can execute significantly faster than `L L / S C`, demonstrating the continuous optimization efforts in hardware design for concurrent operations. The fundamental importance of `C A S` lies in its ability to enable the construction of highly efficient lock free and wait free concurrent data structures by allowing threads to optimistically attempt updates and then conditionally commit those changes based on whether the expected state of memory holds true, thereby gracefully handling concurrent modifications without resorting to traditional locking mechanisms.
