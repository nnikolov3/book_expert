34 2 Architectural Background

Several supercomputer-class machines have provided special network hardware (gener-
ally accessed via memory-mapped I/O) for near-constant-time barrier and “Eureka” oper-
ations. These amount to cross-machine AND (all processors are ready) and OR (some pro-
cessor 1s ready) computations. We will mention them again in Secs. 5.2 and 5.3.3.

In 1991, Stone et al. proposed a multi-word variant of LL/SC, which they called “Okla-
homa Update” (Stone etal. 1993) (in reference to the song “All Er Nuthin’ ” from the Rodgers
and Hammerstein musical). Concurrently and independently, Herlihy and Moss proposed
a similar mechanism for Transactional Memory (TM) 1993. Neither proposal was imple-
mented at the time, but TM enjoyed a rebirth of interest about a decade later. Like queued
locks, it can be implemented in software using CAS or LL / SC, but hardware implementations
enjoy a substantial performance advantage (Harris et al. 2010). As of 2023, hardware TM
has been implemented on the Azul Systems Vega 2 and 3 (Click 2019); the experimental
Sun/Oracle Rock processor (Dice et al. 2009); the IBM Blue Gene/Q (Wang et al. 2012);
IBM’s zEC12 mainframe (Jacobi et al. 2012) and its successors; IBM’s Power 8 and 9 pro-
cessors; Intel’s x86 processors starting with the “Haswell” generation; and Arm processors
that implement the v9-A instruction set. We will discuss TM in Chapter 9.
Several supercomputer class machines have provided special network hardware, generally accessed via memory mapped I O, for near constant time barrier and “Eureka” operations. These amount to cross machine And all processors are ready and Or some processor is ready computations. We will mention them again in Sections five point two and five point three point three.

In nineteen ninety one, Stone et al. proposed a multi word variant of L L slash S C, which they called “Oklahoma Update” Stone et al. nineteen ninety three, in reference to the song “All Er Nuthin’” from the Rodgers and Hammerstein musical. Concurrently and independently, Herlihy and Moss proposed a similar mechanism for Transactional Memory T M nineteen ninety three. Neither proposal was implemented at the time, but T M enjoyed a rebirth of interest about a decade later. Like queued locks, it can be implemented in software using C A S or L L slash S C, but hardware implementations enjoy a substantial performance advantage Harris et al. two thousand ten. As of two thousand twenty three, hardware T M has been implemented on the Azul Systems Vega two and three Click two thousand nineteen; the experimental Sun slash Oracle Rock processor Dice et al. two thousand nine; the I B M Blue Gene slash Q Wang et al. two thousand twelve; I B M’s Z E C twelve mainframe Jacobi et al. two thousand twelve and its successors; I B M’s Power eight and nine processors; Intel’s X eighty six processors starting with the “Haswell” generation; and Arm processors that implement the V nine A instruction set. We will discuss T M in Chapter nine.
The page delineates foundational architectural principles employed in high-performance computing to manage concurrency and communication efficiently across multiple processing units. It commences by highlighting that supercomputer-class systems integrate specialized network hardware designed to facilitate inter-processor communication with extremely low latency. This is achieved through mechanisms such as memory-mapped I O, where physical I O devices or remote memory are mapped directly into a processor's address space, enabling direct access without the need for traditional operating system calls, thereby significantly reducing communication overhead. The objective is to enable near-constant time operations for collective synchronization primitives, such as barrier operations, which ensure all participating processors reach a defined point in their execution before any can proceed. Furthermore, these systems support global logical operations like cross-machine And and Or. A cross-machine And operation, for instance, can collectively determine if a specific condition is true across all processors, while a cross-machine Or operation can ascertain if it is true for at least one. These primitives are indispensable for coordinating tasks and managing data dependencies in large-scale parallel algorithms.

The discussion then transitions to advanced atomic operations and concurrency control mechanisms. The "Oklahoma Update," proposed by Stone and colleagues in one nine nine three, represents a significant conceptual extension of the Load Link, Store Conditional, or L L S C, atomic primitive. L L S C is a pair of instructions critical for implementing non-blocking algorithms. A Load Link instruction reads a value from a memory location and creates a reservation, or "link," to that address. A subsequent Store Conditional instruction attempts to write a new value to the same address. This write succeeds atomically only if the memory location has not been modified by any other processor since the Load Link was performed, indicating that the reservation is still valid. If another processor modified the location, the Store Conditional fails, and the operation must be retried. While L L S C typically provides atomicity for a single memory word, the "Oklahoma Update" proposes a multi-word variant. Achieving atomic updates across multiple, arbitrary memory locations in hardware presents substantial design challenges related to maintaining cache coherence across multiple processor caches and ensuring a consistent memory order, as defined by the memory consistency model.

Building upon similar principles, Herlihy and Moss independently proposed Transactional Memory, or T M, also in one nine nine three. T M introduces an optimistic concurrency control paradigm, treating a sequence of memory operations as an atomic transaction, akin to those in database systems. Within a T M block, operations are performed speculatively. If no conflicts are detected with other concurrently executing transactions, the transaction commits, and all its changes become visible globally. If a conflict occurs, the transaction aborts, and all speculative changes are rolled back, requiring the transaction to be re-executed. This approach aims to simplify concurrent programming by abstracting away the complexities of traditional lock-based synchronization, such as deadlocks, livelocks, and performance bottlenecks caused by fine-grained locking.

While initially challenging to implement efficiently in hardware, Transactional Memory experienced a notable resurgence in interest a decade later, leading to practical hardware implementations. Early software T M, or S T M, relied on compiler and runtime support, often built atop existing atomic primitives like Compare and Swap or L L S C. However, these software-only approaches typically incurred significant overhead. The true performance advantages of T M are realized with hardware T M, or H T M, which offloads the complex logic for managing transaction state, detecting conflicts, and performing rollbacks to the processor's microarchitecture. This often involves extending or leveraging the existing cache coherence protocols within the processor's memory subsystem, resulting in substantially lower overhead and improved performance, especially for contended shared data.

The page cites several contemporary processor architectures that have incorporated hardware T M capabilities. These include the specialized Azul Systems Vega two and Vega three processors, and the experimental Sun/Oracle Rock processor, which were pioneering efforts in this domain. Major general-purpose processor vendors have also integrated H T M into their commercial offerings. I B M's enterprise-grade mainframe systems, such as the z E C twelve and its subsequent generations, along with their Power eight and Power nine processors, feature robust transactional memory support. Intel introduced its own form of H T M, known as Transactional Synchronization Extensions, or T S X, with its x eight six "Haswell" processor generation. Furthermore, the Arm v nine A instruction set architecture now includes provisions for transactional memory. This widespread adoption across diverse processor architectures underscores the growing recognition of T M as a crucial hardware-assisted mechanism for simplifying parallel programming and enhancing the scalability and performance of multi-core and many-core systems.
