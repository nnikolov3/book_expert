48 3 Essential Theory

Most wait-free algorithms—and many lock-free algorithms—employ some variant of
helping, in which a thread that has begun but not completed its operation may be assisted
by other threads, which need to “get it out of the way” so they can perform their own
operations without an unbounded wait. Other lock-free algorithms—and even a few wait-
free algorithms—are able to make do without helping. As a simple example, consider the
implementation of an increment-only counter. On a machine with a FAI instruction, the
natural single-word implementation of the counter will indeed be wait free. If FAI must be
emulated with CAS, however, the result will be merely lock free. The following array-based
implementation is also wait free. It takes more space than the FAI version, but may yield
better performance if inc operations are more common than val:!

atomic<int> C[7]:={0...}

int val():
void inc(): intrtn:=0
// C[self]++ foriin[1..N]
int newval := C[self].load(||W)+1 rtn +:= C[il.load(]|)
C[self].store(newval, ||R) return rtn

Here the aggregate counter value is taken to be the sum of a set of per-thread values.
Because each per-thread value is monotonically increasing (and always by exactly one), so
1s the aggregate sum. Given this observation, one can prove that the value returned by the
val method will have been correct at some point between its call and its return: it will be
bounded above by the number of inc operations that were called before val returned, and
below by the number that returned before it was called. In other words, val is linearizable,
though its linearization point cannot in general be statically determined. Because both inc
and val comprise a bounded (in this case, statically bounded) number of program steps, the
methods are wait free.

3.2.2 Fairness

Obstruction freedom and lock freedom clearly admit behavior that defies any notion of
fairness: both allow an individual thread to take an unbounded number of steps without
completing an operation. Even wait freedom allows an operation to execute an arbitrary
number of steps (helping or deferring to peers) before completing, so long as the number is
bounded in any given situation.

I Interestingly, performance may improve on an x86 machine if the programmer uses a C++
std::fetch_add operation (FAA) instead of separate load and store accesses to C[self] in inc. With
separate instructions, many compilers will issue an expensive mfence instruction to ensure the ||R
ordering on the store. By contrast, fetch_add will trigger the use of an atomic xadd instruction,
which ensures the ordering implicitly.
Most wait free algorithms, and many lock free algorithms, employ some variant of helping, in which a thread that has begun but not completed its operation may be assisted by other threads, which need to get it out of the way so they can perform their own operations without an unbounded wait. Other lock free algorithms, and even a few wait free algorithms, are able to make do without helping. As a simple example, consider the implementation of an increment only counter. On a machine with a F A I instruction, the natural single word implementation of the counter will indeed be wait free. If F A I must be emulated with C A S, however, the result will be merely lock free. The following array based implementation is also wait free. It takes more space than the F A I version, but may yield better performance if increment operations are more common than val. Interestingly, performance may improve on an x eighty six machine if the C plus plus programmer uses the standard fetch add operation, referred to as F A A, instead of separate load and store accesses to C index self in the increment function. With separate instructions, many compilers will issue an expensive memory fence instruction to ensure read ordering. In contrast, fetch add will trigger the use of an atomic x add instruction, which ensures the ordering implicitly.

The code defines an atomic integer array named C, indexed by T, and initialized to all zeros. It includes two functions. The first function, `increment`, is a void function. A commented line shows the concept of incrementing `C` at `self` by one. The actual implementation computes a `newval` by loading the current value of `C` at `self` with write ordering and then incrementing it by one. This `newval` is then stored back into `C` at `self` with read ordering. The second function, `val`, returns an integer. It initializes a return variable `rtn` to zero. It then iterates for an index `i` from one to N. In each iteration, `rtn` is incremented by the value of `C` at index `i`, loaded with implicit memory ordering. Finally, the function returns the accumulated `rtn` value.

Here the aggregate counter value is taken to be the sum of a set of per thread values. Because each per thread value is monotonically increasing, and always by exactly one, the aggregate sum is the value. Given this observation, one can prove that the value returned by the `val` method will have been correct at some point between its call and its return. It will be bounded above by the number of increment operations that were called before val returned, and below by the number that returned before it was called. In other words, `val` is linearizable, though its linearization point cannot in general be statically determined. Because both `increment` and `val` comprise a bounded, in this case, statically bounded number of program steps, the methods are wait free.

Three point two point two Fairness

Obstruction freedom and lock freedom clearly admit behavior that defies any notion of fairness. Both allow an individual thread to take an unbounded number of steps without completing an operation. Even wait freedom allows an operation to execute an arbitrary number of steps, whether helping or deferring to peers, before completing, so long as the number is bounded in any given situation.
In the realm of concurrent computing, the principles of wait free and lock free algorithms are foundational to designing robust and efficient multi threaded systems. Wait free algorithms guarantee that every operation invoked by a process will complete in a bounded number of its own steps, regardless of the execution speeds or failures of other processes. This implies a strong form of non blocking progress, where no single thread can indefinitely halt the progress of another. Many wait free algorithms, and indeed many lock free algorithms, employ a concept known as "helping," where a thread assists in completing an operation initiated by another thread if that operation has stalled or is not making progress. This cooperative paradigm ensures that if a thread begins an operation but is subsequently delayed, other threads can step in to complete it, allowing the system to move forward.

Contrast this with other lock free algorithms, which achieve non blocking properties without resorting to helping. While these still guarantee system wide progress—meaning at least one thread will make progress—they do not necessarily guarantee that every individual thread will complete its own operation in a bounded number of steps. Such algorithms are susceptible to individual thread starvation, where a specific thread might repeatedly be preempted or delayed, taking an unbounded number of steps to complete its task.

Consider a practical illustration: the implementation of a simple increment only counter in a concurrent environment. On a machine that natively supports a Fetch And Increment, or F A I, instruction, a single word implementation of such a counter can indeed be wait free. The F A I instruction atomically reads a value from memory, increments it, and writes the new value back, all as a single, indivisible operation. However, if such an F A I operation must be emulated using a Compare And Swap, or C A S, primitive, the resulting counter implementation often becomes merely lock free, rather than wait free. This is because C A S operations, while atomic, are conditional: they succeed only if the memory location still holds an expected value. If multiple threads attempt a C A S simultaneously, only one can succeed, and the others must retry. This retry loop, while guaranteeing system wide progress, can lead to individual threads retrying indefinitely, thus violating the bounded step guarantee of wait freedom.

Let us analyze a specific array based implementation of a concurrent counter, as exemplified by the provided code snippet. We have an `atomic<int>` array, `C`, indexed by `T`, representing a distributed counter where each thread maintains its own count in `C[self]`. The `inc()` method demonstrates a per thread increment operation. The commented line `C[self] increment by one` hints at a simpler, direct atomic increment if available. The more explicit implementation involves `int newval is C[self].load(or or W) increment by one`, followed by `C[self].store(newval, or or R)`. Here, `C[self].load(or or W)` performs an atomic load operation with "acquire" semantics, meaning that all memory operations that logically follow this load in program order will appear to happen after the load has completed. Similarly, `C[self].store(newval, or or R)` performs an atomic store operation with "release" semantics, ensuring that all memory operations that logically precede this store in program order will appear to happen before the store is completed. These memory orderings are crucial for maintaining memory consistency across multiple threads, preventing problematic reordering of operations by the compiler or the underlying hardware.

The `val()` method, shown in the code, aggregates the sum of these per thread values. It initializes `int rtn is zero`, then iterates through `i` from `one` to `N`, where `N` represents the total number of threads or counter segments. In each iteration, it atomically loads `C[i]` and adds it to `rtn`, using `rtn increment by C[i].load(or or l)`. The `or or l` indicates a relaxed atomic load, implying that the ordering of this specific load relative to other memory operations is not strictly enforced, though the atomicity of the load itself is guaranteed. Finally, the accumulated sum `rtn` is returned.

The behavior of this aggregate counter `val` and the per thread increment `inc` operations merits closer examination in terms of correctness and concurrency properties. The text states that the aggregate value is derived from a set of per thread values, each of which is monotonically increasing. Given this property, it is provable that the value returned by the `val` method will be correct at some point between its invocation and its return. Specifically, the value returned by `val` will be bounded from above by the actual number of `inc` operations that were called before `val` returned, and bounded from below by the number that returned before `val` was called. This demonstrates the critical concept of linearizability. An operation is linearizable if it appears to take effect instantaneously at some point between its invocation and its return. For `val`, this means that even though its internal execution might involve a loop reading multiple distributed counters, its effect on the system's state can be mapped to a single, atomic point in time. The statement that both `inc` and `val` can comprise a bounded number of program steps implies that, with proper implementation, these methods can indeed be wait free. The footnote further clarifies that `std::fetch_add` in C++, often used for atomic increments, can offer better performance on an x86 architecture compared to separate load and store operations that might necessitate explicit memory fences, such as `mfence`. The `fetch_add` primitive typically compiles down to an atomic `xadd` instruction, which inherently provides the necessary memory ordering guarantees without the overhead of explicit barriers.

Moving to the discussion of fairness, this property is distinct from obstruction freedom and lock freedom, though related to wait freedom. Obstruction freedom asserts that if a thread executes its operation without interference from any other threads, it will complete in a finite number of steps. Lock freedom, as discussed, guarantees that at least one thread will make progress in a concurrent system. However, neither obstruction freedom nor lock freedom inherently guarantees fairness. An individual thread may still starve, repeatedly failing to acquire resources or complete its operation, even as the system as a whole continues to advance. Fairness, in contrast, specifically addresses this issue by ensuring that every thread attempting an operation will eventually complete it. Wait freedom is a stronger guarantee than lock freedom, as it implies that every individual operation completes within a bounded number of steps, thereby inherently ensuring fairness for individual threads. The text rightly points out that even wait freedom permits an operation to execute an arbitrary number of steps—potentially involving helping other threads or deferring to peers—before completion, so long as this arbitrary number remains bounded in any given situation. This distinction is crucial: the boundedness is the key to wait freedom and thus to a strong form of fairness, preventing indefinite individual starvation.
