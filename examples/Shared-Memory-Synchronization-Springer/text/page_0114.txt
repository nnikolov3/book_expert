6.3 Read-Copy Update 117

This relaxation of the rules introduces a variety of synchronization challenges. For exam-
ple: a fault handler that overlaps in time with a major update (e.g., an munmap operation
that invalidates a broad address range) may end up modifying the about-to-be-reclaimed
version of a page table entry, in which case it should not return to the user program as if
nothing had gone wrong. If each major update acquires and updates a (per-address-space)
sequence lock, however, then the fault handler can check the value of the lock both before
and after its operation. If the value has changed, it can retry, using the new version of the
data. (Alternatively, if starvation is a concern, it can acquire the lock itself.) Similarly, if
fault handlers cannot safely run concurrently with one another (e.g., if they need to modify
more than a single word in memory), then they need their own synchronization—perhaps a
separate sequence lock in each page table entry. If readers may inspect more than one word
that 1s subject to in-place update, then they, too, may need to inspect such a local sequence
lock, and repeat their operation if they see a change. This convention imposes some on the
(presumably dominant) read-only code path, but the overhead is still small—in particular,
readers still make no updates to shared data.
Six point three, Read Copy Update.

This relaxation of the rules introduces a variety of synchronization challenges. For example: a fault handler that overlaps in time with a major update, for example, an munmap operation that invalidates a broad address range, may end up modifying the about to be reclaimed version of a page table entry, in which case it should not return to the user program as if nothing had gone wrong. If each major update acquires and updates a per address space sequence lock, however, then the fault handler can check the value of the lock both before and after its operation. If the value has changed, it can retry, using the new version of the data. Alternatively, if starvation is a concern, it can acquire the lock itself. Similarly, if fault handlers cannot safely run concurrently with one another, for example, if they need to modify more than a single word in memory, then they need their own synchronization, perhaps a separate sequence lock in each page table entry. If readers may inspect more than one word that is subject to in place update, then they, too, may need to inspect such a local sequence lock, and repeat their operation if they see a change. This convention imposes some on the presumably dominant read only code path, but the overhead is still small. In particular, readers still make no updates to shared data.
The principle of Read-Copy Update, or R C U, represents a sophisticated approach to concurrency control in computer systems, particularly within operating system kernels where data structures are frequently read but only occasionally modified. This mechanism is fundamentally about relaxing traditional synchronization rules to achieve high scalability for read operations, offering a significant advantage over pessimistic locking schemes such as mutexes or semaphores.

At its core, R C U operates on an optimistic concurrency model for readers. Unlike traditional locking, where readers would acquire a shared lock before accessing data, R C U allows read-only operations to proceed without explicit locks, thereby eliminating contention and context switching overhead for the dominant read path. This is achieved by ensuring that writers never modify data in place. Instead, a writer creates a new, updated copy of the data structure, performs its modifications on this private copy, and then atomically publishes the pointer to the new version. The original version of the data remains accessible for any readers that might have been in progress at the time of the update.

To manage consistency, R C U introduces a mechanism often involving "sequence locks" or similar versioning counters. When a writer initiates an update, it increments a sequence number before modifying the data structure and again after the new version is fully published. A reader, before accessing the shared data, first records the current value of this sequence lock. It then proceeds to read the data. Upon completing its read, the reader checks the sequence lock value again. If the initial and final sequence lock values are identical, it indicates that no writer interfered with the data during the reader's critical section, and the read is valid. However, if the sequence lock value has changed, it signifies that a writer performed an update concurrently. In this scenario, the reader's optimistic assumption was violated, and it must retry its entire operation using the newly published version of the data. This retry mechanism is crucial for maintaining correctness without blocking readers.

Consider the illustrative example of a fault handler interacting with page table entries. A `munmap` operation, which deallocates a range of virtual memory, necessitates changes to the page table. In an R C U context, this `munmap` operation would create a new version of the relevant page table entries or the entire table structure. A concurrent fault handler, upon encountering a page fault, might attempt to translate a virtual address using the page table. Under R C U, it could initially access an "about to be reclaimed" version of a page table entry. If the `munmap` operation completes while the fault handler is still processing, the sequence lock would be updated. The fault handler's post-read check of the sequence lock would then reveal the change, prompting it to retry its address translation using the updated page table information. This approach ensures the fault handler never returns an incorrect translation to the user program, even though it temporarily operated on stale data. It avoids the complexities and performance penalties of a full lock acquisition by the reader in this critical path.

The trade-off for this high read concurrency is borne by the writers and the system's memory management. Writers must manage the creation of new data copies and ensure atomic publication. Furthermore, the system must determine a "grace period" – a time after which all readers that might have accessed the old data version are guaranteed to have completed their operations. Only after this grace period can the old data version be safely reclaimed and freed. This reclamation process adds complexity and can involve garbage collection-like mechanisms.

The text highlights an interesting edge case: if a fault handler cannot safely run concurrently with another because it needs to modify more than a single word in memory, it then requires its own synchronization. This underscores that R C U is primarily a read-side optimization for non-modifying readers. If a "reader" process itself needs to perform multi-word modifications, it fundamentally acts as a writer and must employ conventional synchronization primitives, perhaps even per-page-table entry sequence locks for fine-grained control, to ensure atomicity and consistency of its own modifications.

The mention of starvation concern for readers acquiring the lock themselves is a nuanced point. While R C U is designed to eliminate reader blocking, a continuously retrying reader in a highly contended scenario could theoretically "starve" by never completing its operation successfully. However, in practice, the more common starvation concern with R C U is writer starvation, where a continuous stream of readers might perpetually delay the grace period required for old data to be freed. If a reader *must* complete its operation without retrying, it could, as an alternative, temporarily acquire the writer's sequence lock, thus serializing its operation and ensuring a stable view of the data. This strategy, however, deviates from the primary R C U benefit of lock-free reads and would typically be reserved for exceptional cases demanding absolute consistency without retries.

Ultimately, R C U offers a compelling design pattern for systems requiring extreme read performance on shared data. The overhead for the dominant read-only code path is minimal, involving only a few instructions for sequence number checks. The fundamental strength of R C U lies in the premise that readers do not modify shared data, allowing them to proceed with remarkable concurrency, with writers bearing the burden of copy and atomic publication, managed through careful memory reclamation policies.
