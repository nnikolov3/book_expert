5.3 Barrier Extensions 97

Threads Threads

phase 0:

phase 1:

phase 2:

voy

Figure 5.6 Impact of variation across threads in phase execution times, with normal barriers (left)
and fuzzy barriers (right). Blue work bars are the same length in each version of the figure. Fuzzy
intervals are shown as outlined portions of the bars. With fuzzy barriers, threads can leave the barrier
as soon as the last peer has entered its fuzzy interval. Overall performance improvement is shown by
the double-headed arrow at center.

in parallel forie 7
repeat
// do i's portion of the work of a phase
b.cycle()
until terminating condition

becomes

in parallel forie 7

repeat
// dois critical work for this phase
b.arrive()
// do i's non-critical workâ€”its fuzzy interval
b.depart()

until terminating condition

As illustrated on the right side of Figure 5.6, the impact on overall run time can be a dramatic
improvement.

A centralized barrier 1s easily modified to produce a fuzzy variant (Figure 5.7). Unfor-
tunately, none of the logarithmic barriers we have considered has such an obvious fuzzy
version. We address this issue in the following subsection.

5.3.2 Adaptive Barriers

When all threads arrive at about the same time, tree and dissemination barriers enjoy an
asymptotic advantage over the centralized barrier. The latter, however, has an important
advantage when thread arrivals are heavily skewed: if all threads but one have already
finished their arrival work, the last thread is able recognize this fact in constant time in a
Section five point three, Barrier Extensions.

Figure five point six illustrates the impact of variation across threads in phase execution times. The figure presents two diagrams: one on the left showing normal barriers, and one on the right showing fuzzy barriers. In both versions, blue work bars are of the same length. Fuzzy intervals are depicted as outlined portions of the bars. With fuzzy barriers, threads can leave the barrier as soon as the last peer has entered its fuzzy interval. The overall performance improvement is indicated by a double headed arrow at the center of the figure.

The left diagram, representing normal barriers, shows four threads, labeled zero, one, two, and three, progressing through three phases: phase zero, phase one, and phase two. Each thread performs its work, represented by a solid blue vertical bar, and then waits at a horizontal line, which is the barrier, until all other threads have also completed their work for that phase. This means the start of the next phase is determined by the slowest thread.

The right diagram, representing fuzzy barriers, also shows four threads, zero, one, two, and three, and three phases. Here, the work for each phase is divided into a critical work portion, shown as a solid blue bar, and a non-critical or fuzzy interval portion, shown as an outlined blue bar. Threads arrive at the barrier, indicated by a dashed horizontal line, and can proceed to the next phase once all other threads have entered their fuzzy interval, potentially before completing their own non-critical work for the current phase. This mechanism allows for earlier progression and shows a performance improvement over normal barriers.

The process described by the pseudocode can be understood as follows:

In parallel for i is an element of T:
  Repeat:
    Do i's portion of the work of a phase.
    Call the b dot cycle function.
  Until a terminating condition is met.

This process then becomes:

In parallel for i is an element of T:
  Repeat:
    Do i's critical work for this phase.
    Call the b dot arrive function.
    Do i's non-critical work, which is its fuzzy interval.
    Call the b dot depart function.
  Until a terminating condition is met.

As illustrated on the right side of Figure five point six, the impact on overall run time can be a dramatic improvement. A centralized barrier is easily modified to produce a fuzzy variant, as shown in Figure five point seven. Unfortunately, none of the logarithmic barriers we have considered have such an obvious fuzzy version. We address this issue in the following subsection.

Section five point three point two, Adaptive Barriers.

When all threads arrive at about the same time, tree and dissemination barriers enjoy an asymptotic advantage over the centralized barrier. The latter, however, has an important advantage when thread arrivals are heavily skewed: if all threads but one have already finished their arrival work, the last thread is able to recognize this fact in constant time in a
The concept of barrier synchronization is fundamental in parallel computing, ensuring that all threads in a computational ensemble reach a designated point in execution before any are allowed to proceed. This mechanism is crucial for maintaining data consistency and correct program flow in phased parallel algorithms. However, a significant challenge arises from execution time variations, or "load imbalance," among threads.

Consider a graphical representation of thread execution over time, often referred to as a Gantt chart. On the left side, we observe four threads, labeled zero through three, executing three distinct computational phases, designated "phase zero," "phase one," and "phase two." The vertical dimension represents time, flowing downwards, while the horizontal axis delineates the individual threads. Within each phase, a solid blue bar illustrates the actual work performed by each thread. Notice that the lengths of these blue bars vary within each phase, indicating differing work completion times. For instance, in "phase zero," thread zero completes its work relatively quickly, whereas thread three takes considerably longer.

With traditional, or "normal," barriers, a rigid synchronization point is enforced at the end of each phase. This is depicted by the solid horizontal black lines. Every thread, regardless of when it finishes its current phase's work, must wait at this barrier until the *slowest* thread has also completed its work and reached the barrier. This necessitates that all subsequent work for the next phase begins simultaneously across all threads. This waiting period, visible as empty vertical space between the shorter blue bars and the horizontal barrier lines, represents idle time. This idle time is a direct consequence of load imbalance and significantly impacts overall parallel efficiency, as the total execution time is dictated by the slowest-performing thread in each phase. The underlying principle here is strict sequential consistency between phases.

The code snippet for this normal barrier behavior illustrates a parallel loop where each thread `i` within the set `T` repeatedly performs its "portion of the work of a phase" followed by a `b.cycle()` call. This `b.cycle()` function encapsulates the blocking synchronization: a thread will pause its execution until all other threads have also invoked `b.cycle()`, thereby ensuring all threads have completed the current phase's work before any can proceed. The loop continues "until terminating condition." This model, while simple and effective for correctness, does not inherently mitigate the performance penalties associated with variations in thread execution times.

In contrast, the right side of the visual representation introduces the concept of "fuzzy barriers," which aim to mitigate this load imbalance penalty. Here, the intrinsic work duration for each thread in each phase, represented by the overall length of the blue bars, remains identical to the normal barrier case. However, the synchronization mechanism is fundamentally altered. Instead of a single, absolute rendezvous point, a "fuzzy interval" is introduced. This interval is depicted by the outlined portions within the blue work bars. The core idea is to differentiate between "critical work" that absolutely requires global synchronization and "non-critical work" that can be performed more flexibly.

The modified code snippet reveals this distinction. The `b.cycle()` operation is replaced by a two-stage process: `b.arrive()` and `b.depart()`. A thread first performs its "critical work for this phase" and then calls `b.arrive()`. This signifies that the thread has completed all operations that *must* be globally consistent before any other thread can progress significantly. After `b.arrive()`, the thread then executes its "non-critical work," which constitutes its "fuzzy interval." This non-critical work can proceed even while other threads are still completing their critical work or arriving at their own fuzzy intervals. Finally, the thread calls `b.depart()`. The critical relaxation with fuzzy barriers is that threads are permitted to "leave the barrier as soon as the last peer has entered its fuzzy interval." This means a thread can proceed to the next phase as soon as all other threads have at least completed their critical work, not necessarily their non-critical work.

The performance benefit is visualized by the double-headed arrow spanning between the two diagrams, indicating an "overall performance improvement." By allowing threads to perform non-critical work during what would otherwise be idle waiting time, fuzzy barriers effectively reduce the total wall clock time required to complete the entire set of phases. This is achieved by overlapping the non-critical work of faster threads with the critical work of slower threads, thereby reducing the cumulative idle time. The design of fuzzy barriers embodies a trade-off between strict synchronization and exploiting parallelism through latency hiding.

While centralized barriers can be adapted to this fuzzy variant, more complex logarithmic barriers, such as tree or dissemination barriers, have not historically lent themselves to an obvious fuzzy implementation. This observation introduces a deeper discussion on "Adaptive Barriers," which are designed to dynamically adjust their synchronization strategy based on real-time execution conditions.

Section five point three point two delves into adaptive barriers. A key insight in parallel system design is that no single barrier mechanism is optimal under all conditions. Tree and dissemination barriers, which exhibit logarithmic time complexity with respect to the number of threads, offer asymptotic performance advantages over centralized barriers when threads arrive at the synchronization point at approximately the same time, indicating a well-balanced workload. However, their overhead can be substantial when thread arrivals are highly skewed. In such scenarios, where, for instance, nearly all threads have completed their work but one, a centralized barrier can recognize the arrival of the last straggler in constant time. This is because a centralized barrier typically involves a single shared variable or counter, allowing for very rapid detection of the final participant. The objective of adaptive barriers is to synthesize the strengths of different barrier types: leveraging the scalability of logarithmic barriers for balanced loads and exploiting the low overhead of centralized barriers for highly imbalanced, or skewed, arrival patterns. Such adaptive mechanisms dynamically choose the most efficient barrier implementation based on runtime profiling of thread arrival statistics, optimizing for overall throughput and minimizing synchronization overhead.
