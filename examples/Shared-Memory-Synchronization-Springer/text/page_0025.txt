26 2 Architectural Background

(1993), Gharachorloo (1995), or Lustig (2015), or Sewell et al.’s attempt (2010) to formalize
the behavior specified informally in Intel’s architecture manual (Intel 2023, Vol. 3, Sec. 9.2).

A few instruction sets—notably the IA64 (Itanium) and AArch64 (Arm v8 and v9, but
not v7)—provide explicit synchronizing loads and stores. Other machines typically pro-
vide separate fence instructions, which do not, themselves, modify memory. Though less
common, synchronizing accesses have two principal advantages. First, they can be designed
to ensure write atomicity, even on a machine that does not otherwise provide it, thereby
precluding the sort of circularity shown in Figure 2.4. Second, they can allow an individ-
ual access (the load or store itself) to be ordered with respect to preceding or subsequent
accesses. A fence, by contrast, must order all preceding and subsequent accesses of the
specified kind.

One particular form of local ordering is worthy of special mention. As noted in Sec. 2.2.2,
a lock acquire operation must ensure that a thread cannot read or write shared data until
it has actually acquired the lock. The appropriate guarantee will typically be provided by a
IRW load or a load that is followed by a R||RW fence. In a similar vein, a lock release must
ensure that all reads and writes within the critical section have completed before the lock is
actually released. The appropriate guarantee will typically be provided by a RW|| store or
a store that is preceded by a RW||W fence. These combinations are common enough that
they are sometimes referred to as acquire and release orderings. They are used not only
for mutual exclusion, but for most forms of condition synchronization as well. The [A-64
(Itanium), AArch64, and several research machines—notably the Stanford Dash (Lenoski
et al. 1992)—support acquire and release orderings directly in hardware. In the mutual
exclusion case, particularly when using synchronizing accesses rather than fences, acquire
and release orderings allow work to “migrate” into a critical section both from above (prior
to the lock acquire) and from below (after the lock release). They do not allow work to
migrate out of a critical section in either direction.

Speculation in Compilers and in Hardware

When reasoning about ordering, it is tempting to assume that compilers and processors will always
respect control and data dependences—that they won’t, for example, access shared memory on the
far side of a not-yet-resolved conditional test, or guess the address from which to read a value. Such
assumptions are a recipe for disaster: modern compilers and processors can be surprisingly aggressive,
and programmers who assume otherwise are likely to write code that doesn’t work. In this monograph,
we assume only that no reasonable compiler or processor will speculate the address or value of a write.

In a similar vein, we assume that a reasonable compiler and processor will eventually (in finite time)
verify speculated values. So, for example, in the code if (x.load(||)) while (true) {}, if x is speculated
to be true but its value is really false, it should not be possible for a thread to spin forever in the while
loop. Likewise, in the code repeat a := x.load() until a, where x is an atomic variable, the compiler
should never optimize away the spin loop by hoisting a single load of x outside. (We shall have more
to say about spinning when we consider the subject of fairness in Sec. 3.2.2.)
Gharachorloo, one nine nine three, or Lustig, two thousand fifteen, or Sewell et al.'s attempt, two thousand ten, formalize the behavior specified informally in Intel's architecture manual, Intel two thousand twenty three, Volume three, Section nine point two. A few instruction sets—notably the I A sixty four, Itanium, and A Arch sixty four, Arm version eight and version nine, but not version seven—provide explicit synchronizing loads and stores. Other machines typically provide separate fence instructions, which do not, themselves, modify memory. Though less common, synchronizing accesses have two principal advantages. First, they can be designed to ensure write atomicity, even on a machine that does not otherwise provide it, thereby precluding the sort of circularity shown in Figure two point four. Second, they can allow an individual access, the load or store itself, to be ordered with respect to preceding or subsequent accesses. A fence, by contrast, must order all preceding and subsequent accesses of the specified kind.

One particular form of local ordering is worthy of special mention. As noted in Section two point two point two, a lock acquire operation must ensure that a thread cannot read or write shared data until it has actually acquired the lock. The appropriate guarantee will typically be provided by a R double vertical bar W load or a load that is followed by a R double vertical bar W fence. In a similar vein, a lock release must ensure that all reads and writes within the critical section have completed before the lock is actually released. The appropriate guarantee will typically be provided by a R double vertical bar W store or a store that is preceded by a R double vertical bar W fence. These combinations are common enough that they are sometimes referred to as acquire and release orderings. They are used not only for mutual exclusion, but for most forms of condition synchronization as well. The I A sixty four, Itanium, A Arch sixty four, and several research machines—notably the Stanford Dash, Lenoski et al. one nine nine two—support acquire and release orderings directly in hardware. In the mutual exclusion case, particularly when using synchronizing accesses rather than fences, acquire and release orderings allow work to “migrate” into a critical section both from above, prior to the lock acquire, and from below, after the lock release. They do not allow work to migrate out of a critical section in either direction.

Speculation in Compilers and in Hardware

When reasoning about ordering, it is tempting to assume that compilers and processors will always respect control and data dependencies—that they won’t, for example, access shared memory on the far side of a not yet resolved conditional test, or guess the address from which to read a value. Such assumptions are a recipe for disaster: modern compilers and processors can be surprisingly aggressive, and programmers who assume otherwise are likely to write code that doesn’t work. In this monograph, we assume only that no reasonable compiler or processor will speculate the address or value of a write.

In a similar vein, we assume that a reasonable compiler and processor will eventually, in finite time, verify speculated values. So, for example, in the code, if X dot load or or, while true, if X is speculated to be true but its value is really false, it should not be possible for a thread to spin forever in the while loop. Likewise, in the code, repeat A is assigned X dot load until A, where X is an atomic variable, the compiler should never optimize away the spin loop by hoisting a single load of X outside. We shall have more to say about spinning when we consider the subject of fairness in Section three point two point two.
In the realm of modern computer architecture, the precise ordering of memory operations is a foundational challenge, particularly in multi core and multi threaded environments. While sequential consistency, where all operations appear to execute in program order, offers the simplest programming model, it often comes at a significant performance cost. To mitigate this, processors adopt relaxed memory models, allowing hardware to reorder loads and stores for improved parallelism and latency hiding. However, this reordering necessitates explicit synchronization mechanisms to maintain correctness in concurrent programs.

Memory fences, also known as memory barriers, are crucial low level instructions that enforce specific ordering constraints on memory operations. Architectures like Intel's I A sixty four, also known as Itanium, and A Arch sixty four, spanning A R M version eight and version nine, provide such explicit fence instructions. These instructions ensure that a particular sequence of loads and stores completes before another sequence begins, thereby preventing problematic reorderings that could lead to data corruption or logical errors. Fences offer two primary advantages: first, they can be meticulously designed to guarantee write atomicity, ensuring that a series of writes to shared memory appears as a single, indivisible operation to all observing processors, even on systems that intrinsically offer a weak consistency model. This atomicity is paramount for maintaining data integrity in shared memory paradigms. Second, fences prevent the type of circular dependencies in memory operations that can arise from aggressive reordering, thereby averting potential deadlocks or incorrect state transitions. A fence, fundamentally, imposes a strict ordering constraint, demanding that all preceding memory accesses complete before any subsequent accesses of the specified kind are allowed to proceed.

A critical application of memory ordering and fences lies in the implementation of locks and other synchronization primitives. A lock acquire operation mandates that a thread cannot read or write shared data until it has successfully obtained the lock. The architectural guarantee for this is typically provided by a specialized memory fence, often denoted as a Read double pipe Read Write fence. This fence ensures that all reads and writes performed within the critical section guarded by the lock become visible only after the lock itself is acquired, and conversely, that any reads and writes *before* the lock is released have completed before the release operation. Similarly, a lock release operation, which signals the availability of shared resources, must ensure that all modifications made by the thread while holding the lock are definitively visible to any other thread that subsequently acquires the lock. This guarantee is commonly provided by a Read Write double pipe Write fence, which ensures that all prior memory operations are completed and visible before the lock release. These acquire and release orderings are fundamental to mutual exclusion, facilitating correct and robust concurrent programming. Historical research machines, such as the Stanford Dash from Lenoski and collaborators in one thousand nine hundred ninety two, and commercial architectures like I A sixty four and A Arch sixty four, explicitly support these acquire and release orderings. While full fences provide strong ordering guarantees, acquire and release operations specifically allow for the "migration" of work into and out of critical sections, meaning certain operations can be reordered relative to the lock operation itself, provided the essential data visibility and atomicity properties are preserved. However, they strictly prohibit work from crossing the critical section boundary in either direction in a manner that would violate sequential consistency or atomicity.

The interplay between compilers and hardware introduces complexities, particularly concerning speculative execution and optimization. When compilers reason about program order, especially with respect to control and data dependencies, there is a temptation to assume predictable behavior regarding shared memory access. For instance, a compiler might assume it can aggressively reorder memory accesses across a conditional test or speculate on the memory address from which to read a value. Such aggressive assumptions, if unverified or incorrect, can lead to subtle yet catastrophic failures in concurrent systems. A reasonable compiler and processor are expected to perform speculation, but also to eventually verify these speculatively computed values, discarding results if the speculation proves false.

Consider a common pattern for busy waiting in concurrent programming: a spin loop. This often involves repeatedly loading the value of a shared variable until it satisfies a condition, such as `if (x.load() or or y)` or `while (true) {}` where `x` is an atomic variable. A common compiler optimization, known as loop invariant code motion or hoisting, aims to improve performance by moving computations that do not change within a loop body to a point outside the loop. For instance, a compiler might attempt to hoist `x.load()` outside the loop, performing the load once and then repeatedly checking a cached value `a := x.load()` in the loop condition. However, if `x` is an atomic variable, its value can be modified by another thread at any time. If the compiler hoists `x.load()` outside the loop, the value of `a` will become stale, and the loop might continue to spin indefinitely, even if another thread has updated `x` to a value that should terminate the loop. This constitutes a live lock. Therefore, for atomic variables, a reasonable compiler and processor must *not* optimize away the load from within the spin loop by hoisting it outside. The underlying principle is that optimizations must preserve the observable behavior of atomic operations, ensuring that changes made by other threads are promptly reflected when an atomic variable is accessed. This fundamental principle extends to discussions of fairness in concurrent algorithms, where incorrect optimizations can lead to starvation or lack of progress for certain threads.
