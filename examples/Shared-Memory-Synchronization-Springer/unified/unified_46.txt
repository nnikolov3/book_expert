The discussion revolves around the intricacies of linked list operations, particularly focusing on deletion and its potential pitfalls in concurrent environments, drawing upon concepts from Harris's algorithm and its subsequent refinements by Michael. A critical aspect highlighted is the problem of deletion in a singly linked list, specifically when a node is removed but its successor is still being traversed by another thread. If a thread is examining the node containing twenty, and the node containing ten preceding it is deleted, the deletion operation involves swinging the next pointer of the node before the deleted one to point to the node after the deleted one.

The described scenario posits that the deletion of the node containing twenty requires swinging the next pointer of the node containing ten. The concern arises if an insertion operation occurs immediately after the node containing ten, but before the deletion of the node containing twenty is fully committed. Specifically, if a deletion in Figure eight point three b happens, and then an insertion occurs in Figure eight point three a before the deletion is fully processed, the node containing twenty-five, which should have been inserted after twenty, might inadvertently be linked to the wrong predecessor. This situation, where a node is logically deleted but its pointer is still in use, can lead to data loss or corruption if not handled carefully.

The presented solution, adapted from an early queue algorithm and generalized to linked lists by Harris, involves a two-step deletion process. The first step marks the node for deletion, often by flipping a bit in its next pointer, indicated by shading in a diagram. This marking signals to other threads that the node is being removed, without immediately severing its connection. The second step, which might not be explicitly shown, involves the actual removal. This two-step approach ensures that lookups and traversals can still proceed past the marked node, preventing premature termination or incorrect path following. If the list remains consistent during this marking phase, subsequent operations like lookups are safe.

However, the text notes that deletion linearization occurs at the initial pointer-marking C A S, while insertion linearizes at the C A S that adds its node. This subtle difference can lead to issues if node reuse is permitted, as the A B A problem can manifest. The A B A problem occurs when a thread reads a value, then another thread modifies it to a different value, and then back to the original value before the first thread performs its intended operation. In this context, if a node is deleted and its memory is immediately reused for a new node, a thread might mistakenly believe it is operating on the original node.

Harris's algorithm, as described, relies on the availability of general-purpose garbage collection, such as reference counting, to reclaim nodes that are no longer referenced. Michael's refinement addressed this by developing an algorithm that does not require garbage collection for correctness. This refinement utilized counted pointers and a type-preserving allocator, or more commonly, hazard pointers. A key observation enabling this was that as long as threads do not dereference marked pointers, they can safely traverse the list even if nodes are in a transitional deletion state. Michael's algorithm, therefore, avoids the need for garbage collection by ensuring safe traversal.

The text also introduces the concept of counted pointers, which are augmented with synchronization instructions, and are illustrated in Figures eight point four and eight point five. A double-width C A S, which atomically updates a pointer and a count simultaneously, is employed. This mechanism provides the advantage of easily moving a node from one list to another without causing race conditions, particularly when dealing with stale references.

The core of the Harris and Michael algorithm, as detailed in Figure eight point five, is a search routine that supports insert, delete, and lookup operations. For notational convenience, this routine returns three values: P R E V p, C U R R, and N E X T. P R E V p represents a counted-pointer reference to the predecessor of the node being searched for, C U R R is a counted-pointer reference to the first node, and N E X T is a counted-pointer reference to the node following C U R R. If C U R R itself is the node being searched for, then P R E V p is a counted-pointer reference to the node that refers to C U R R. This structured return of information is crucial for the correct implementation of atomic operations on the linked list.

The provided pseudocode defines a linked list structure and a search operation. The node structure contains an atomic value named val and an atomic pointer named next. The use of atomic types is fundamental to concurrent programming, ensuring that operations on these members are indivisible and maintain memory consistency even when accessed by multiple threads simultaneously. The list class encapsulates this structure with an atomic pointer named head, pointing to the first node.

The pseudocode introduces thread-private variables: P R E V p, C U R R, and N E X T. The P R E V p variable, a pointer to an atomic pointer, is particularly interesting as it hints at a mechanism for tracking the previous node during traversal, crucial for operations like deletion. The C U R R and N E X T pointers will be used to navigate the list. The comment indicates these variables are local to each thread's search operation, preventing interference.

The search algorithm aims to find a node whose value is greater than or equal to a target value. The algorithm employs a loop that begins by initializing P R E V p to point to the head and C U R R to the node pointed to by head. The inner loop continues as long as C U R R is not null. Inside the inner loop, a check signifies that the end of the list has been reached, and if the value is greater than the target, it implies the target is larger than all elements in the list.

A critical operation occurs with a compare-and-swap, or C A S, operation, likely on the P R E V p pointer itself. The C A S attempts to atomically update the P R E V p if it hasn't changed since it was last read. If the C A S fails, it implies the list has been modified by another thread, necessitating a restart of the search from the beginning of the outer loop. This is a common technique in nonblocking algorithms to handle concurrent modifications.

If the C A S succeeds, the code checks if the next pointer is marked as deleted. If the next pointer is marked as deleted, it means the node pointed to by the next pointer is no longer valid. In this scenario, the algorithm attempts to link the current node to the node after the next pointer using a C A S operation. This operation essentially removes the deleted node from the list by bypassing it. If this C A S fails, indicating concurrent modification of the predecessor's next pointer, the outer loop is restarted.

The pseudocode also handles the case where the target value is found to be less than or equal to the current node's value. In this situation, if the current node's value is equal to the target, the search has found an exact match. If the current node's value is greater than the target, the search has found the first node that satisfies the condition. In both cases, the loop terminates, and the algorithm returns.

The figure caption attributes this implementation to a well-known paper on lock-free data structures. It highlights that counted pointers were added to address the A B A problem. The A B A problem arises in concurrent algorithms when a memory location is read, then modified, then restored to its original value. If another thread observes the original value without seeing the intermediate modification, it might incorrectly assume no change has occurred. Counted pointers, by including a version or modification counter along with the pointer, provide a more robust way to detect such problematic sequences.

The subsequent paragraph discusses the context of these algorithms, mentioning a run-time check that allows a method to determine linearization at some previous instruction. This relates to the concept of linearizability, a correctness condition for concurrent objects that requires that every operation appears to take effect atomically at some instant between its invocation and its completion. The H and M list is presented as an example where insertions and deletions linearize at C A S operations. Unsuccessful insertions and deletions, as well as lookups, are also discussed in terms of their linearization points.

The discussion also touches upon the scenario where the list is empty, further emphasizing the robust handling of edge cases in concurrent data structure design. One downside of the Harris and Michael algorithm is that operations must restart their searches from the head after failing a C A S. To avoid this inefficiency, Fomitchev and Ruppert augmented each list node with a back pointer. When a node is removed from the list, its back pointer is set to point to the node just before it. Of course, that previous node might also get deleted, but an operation can always follow back pointers until it finds an unmarked nodeâ€”one that has not been deleted. This process eliminates the need to restart from the head.

Heller et al. took a different approach, designing a list whose updates acquire fine-grained locks, but whose searches are wait-free. The presented code snippet illustrates the insert and delete operations within a lock-free context. The insert function first checks if the value already exists. If not, it allocates a new node. The core of the lock-free mechanism here is the use of Compare And Swap, or C A S, operations. A C A S operation atomically checks if a memory location holds an expected value and, if so, updates it with a new value.

The delete operation employs a similar strategy. It first attempts to mark the node as logically deleted. This is achieved by another C A S operation. If this succeeds, the node is logically removed. Subsequently, another C A S operation attempts to physically unlink the logically deleted node by updating the previous node's next pointer. The use of version numbers is crucial for ensuring atomicity and detecting interference from other threads. If any C A S operation fails, the entire operation restarts, ensuring correctness in a highly concurrent environment.

The lookup function, in contrast, is straightforward, simply returning the result of a search operation. The text also introduces the concept of "More Recent Linked Lists," highlighting a performance consideration in lock-free algorithms. The Harris and Michael algorithm requires threads to restart their search if a node is removed from the list while they are traversing. To address this inefficiency, Fomitchev and Ruppert augmented each list node with a back pointer. This back pointer points to the preceding node, allowing operations to follow these pointers if a node is deleted. This strategy eliminates the need for immediate restarts when a node is removed, as operations can potentially find the correct predecessor and continue the traversal.
