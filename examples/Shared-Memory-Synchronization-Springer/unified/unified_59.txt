Nonblocking algorithms have been a subject of extensive research, with various techniques being developed to improve their performance and efficiency. One such technique is elimination, which is essentially implemented as a filtering step that sits on top of the data structure. Recently, a more fine-grained variant of elimination called publishing elimination has been implemented in alpha beta trees, which are a type of search tree with many keys per node. Publishing elimination attempts to improve performance in skewed workloads, where some keys are much more popular than others. When threads contend on the same key and thus the same node, they communicate with each other through an elimination record in the node, allowing some concurrent inserts and deletes on the same key to be eliminated.

In the realm of higher-level constructions, researchers have been exploring ways to automatically derive concurrent, nonblocking implementations from sequential ones. The ideal scenario involves feeding a sequential data structure implementation into an automated tool to yield a performant, nonblocking version. This approach is rooted in properties that are both nonblocking and inherently concurrent. Herlihy's foundational work in wait-free synchronization, particularly his nineteen ninety-one paper, provided crucial insights that enable the creation of wait-free concurrent implementations for virtually any sequential object.

A subsequent paper by Herlihy in nineteen ninety-three introduced alternative construction methodologies suitable for both wait-free and lock-free paradigms. The core principle behind these lock-free constructions is the ability to access the data structure without requiring exclusive locks. This is typically achieved by employing a distinguished root pointer. A read-only operation simply dereferences this pointer to locate the required data. Modifications, however, necessitate a more complex atomic update. The process involves creating a copy of the relevant part of the structure, verifying its integrity, and then atomically updating the root pointer to point to this new copy.

Numerous universal constructions have since been developed, with a survey of these efforts provided by Fatourou and Kallimanis in two thousand twenty. While these universal constructions simplify the process of obtaining concurrent, nonblocking algorithms, they often introduce substantial overhead. The inherent inefficiency of these universal constructions is a key area of research. The development of synchronization constructs that balance performance and simplicity is a significant ongoing effort. Notable examples of such constructs include the k-compare-single swap, or k-CSS, developed by Luchangco et al. in two thousand three. Additionally, load-link extended or store-conditional extended operations, such as those implemented by LLX/SCX by Brown et al. in two thousand thirteen, and PathCAS by Brown et al. in two thousand twenty-two, represent advancements in this domain.

K-CSS takes k addresses, k expected values, and one new value; it then atomically checks if all addresses contain their expected values and, if so, changes the first address to its new value and returns true. Otherwise, it simply returns false. Unfortunately, because k-CSS updates only a single location, it cannot easily be used to, say, remove a node and mark it as deleted so it is not subsequently changed erroneously. The more expressive k-CAS operation takes k addresses, k expected values, and k new values; it then atomically checks if all addresses contain their expected values and, if so, changes each address to its new value and returns true. Otherwise, it simply returns false.

LLX and SCX are slightly different in that they operate on nodes rather than individual addresses. LLX of u returns a snapshot of the contents of node u. SCX of D comma F comma f comma n takes a sequence D of dependency nodes that LLX has been invoked on, a sequence F of nodes to finalize, a field f and a value n; it then atomically checks whether any of the nodes in D have been changed since LLX was last invoked on them and, if not, stores n in f and finalizes the nodes in F, preventing them from being changed by future SCX operations. LLX and SCX are less expressive than k-CAS, but can be implemented more efficiently.

Recently, a generalization of k-CAS, called PathCAS, was introduced to make it easier to use k-CAS to design lock-free data structures. Note that using k-CAS to design, for example, a lock-free tree is not trivial. Although k-CAS makes it easy to change multiple addresses atomically, one still needs to reason about the atomicity of searches and, as weâ€™ve seen in Section eight point six point one, even atomic updates in an internal binary search tree can foil naive searches. PathCAS provides operations to visit a node; validate that no node has changed since it was visited returning true if there were no changes and false otherwise; and add a triple consisting of an address, expected value, and new value to the accumulated path. It then provides an exec operation, which performs a k-CAS using the added triples, and a vexec operation, which atomically performs the combined operation of validate if validate is true then exec.

Transactional memory is perhaps the most expressive synchronization primitive yet to be proposed. It allows an arbitrary sequence of reads and writes to be encapsulated in a transaction that either commits and takes effect atomically, or aborts and does not perform any of its writes. The term software transactional memory is coined by Shavit and Touitou in nineteen ninety-five. Inspired by the hardware proposal of Herlihy and Moss nineteen ninety-three, the original Shavit and Touitou-style software transactional memory is very similar to a lock-free software implementation of k-CAS. Most subsequent software transactional memory systems have extended this functionality to accommodate dynamic operations, in which the full set of locations to be accessed is not known in advance.

Transactional memory has been one of the most active areas of synchronization research over the past two decades, spanning hundreds of published papers. At its core, transactional memory represents the fusion of two complementary ideas: first, that we should raise the level of abstraction for synchronization, allowing programmers to specify what should be atomic without needing to specify how to make it atomic; second, that we should employ an underlying implementation based on speculation. The user-level construct is typically simply an atomic label attached to a block of code. The speculative implementation allows transactions to proceed in parallel unless and until they conflict with one another, accessing the same location, with at least one of them performing a write. At most one conflicting transaction is allowed to continue; the other(s) abort, roll back any changes they have made, and try again.

Ideally, the combination of atomic blocks and speculation should provide the scalability of fine-grain locking with the simplicity of coarse-grain locking, thereby sidestepping the traditional tradeoff between clarity and performance. The combination also offers a distinct semantic advantage over lock-based critical sections, namely composability. An atomicity mechanism is said to be composable if it allows smaller atomic operations to be combined into larger atomic operations without the possibility of introducing deadlock. Critical sections based on fine-grain locks are not composable: if operations are composed in different orders in different threads, they may attempt to acquire the same set of locks in different orders, and deadlock can result. Speculation-based implementations of atomic blocks break the irrevocability required for deadlock: when some transactions abort and roll back, others are able to make progress.

As noted, transactional memory was originally proposed by Herlihy and Moss in nineteen ninety-three. A similar mechanism was proposed concurrently by Stone et al. in nineteen ninety-three, and precursors can be found in earlier work. The concept of transactional memory has since evolved, with significant advancements in its design and implementation. The current state of the art in transactional memory is characterized by a wide range of approaches, from hardware-based implementations to software-based solutions, each with its strengths and weaknesses.

In conclusion, nonblocking algorithms and transactional memory are essential components of modern concurrent systems. The development of efficient and scalable synchronization primitives, such as k-CAS, LLX/SCX, and PathCAS, has enabled the creation of high-performance concurrent data structures. Transactional memory, with its ability to encapsulate arbitrary sequences of reads and writes in atomic transactions, offers a promising approach to simplifying concurrent programming. As research in this area continues to evolve, we can expect to see significant advancements in the design and implementation of concurrent systems, leading to improved performance, scalability, and reliability.
