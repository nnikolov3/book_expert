In the realm of parallel computing, synchronization barriers serve as a critical primitive to ensure correct program execution. A barrier defines a point in a parallel algorithm where all participating threads must arrive before any thread is allowed to proceed. This mechanism guarantees that certain computations have completed and their results are visible across all threads, thereby establishing a global state and preventing race conditions or data inconsistencies.

One class of these primitives is the tournament barrier. This design organizes threads into a conceptual tree structure. Threads begin at the leaves of this tree and propagate their arrival upward towards the root. At each level, a subset of threads competes, and only one, designated as the winner or specific thread, continues its ascent. The remaining threads, upon signaling their arrival, simply enter a busy-wait state. Once the ultimate winner reaches the root, indicating all threads have arrived, it initiates a wakeup phase, broadcasting a signal or setting a global flag. This signal then propagates back down the tree, releasing the waiting threads.

The temporal complexity of such a barrier is logarithmic with respect to the number of threads, specifically O of logarithm base two N, where N is the number of threads, reflecting the height of the tree. However, on NRC-NUMA machines, the physical distribution of memory can introduce significant performance penalties. To mitigate the high cost of remote spinning on non-local memory locations, tournament barriers may increase their space requirements to O of N logarithm base two N. This is a design trade-off where memory consumption is increased to improve temporal performance by minimizing expensive remote memory accesses and exploiting local cache coherence, especially on systems supporting broadcast-based cache coherence protocols where a global flag can be efficiently disseminated.

Inspired by these early designs, the static tree barrier was proposed, building upon the principles of hierarchical synchronization. This barrier also exhibits logarithmic time complexity and linear space complexity. A key advantage of the static tree barrier lies in its locality-aware design: it ensures that threads spin exclusively on memory locations within their local cache, thereby circumventing the performance overhead associated with remote memory accesses on NUMA architectures. This characteristic allows it to achieve the theoretical minimum number of memory accesses, which is two N minus two, on machines that lack explicit broadcast mechanisms.

In this static tree structure, each thread is assigned a unique node, and the synchronization process involves a carefully choreographed exchange of signals. Threads at the leaves of the conceptual arrival tree signal their parents. An internal node, upon receiving signals from all its designated children, in turn signals its own parent. This hierarchical aggregation continues until the root of the arrival tree is reached, signifying that all threads have arrived. Concurrently, a separate wakeup tree operates. Once the root of the arrival tree has processed all arrivals, a signal propagates down the wakeup tree, releasing the waiting threads.

The structure of these trees, specifically their fan-in for arrival and fan-out for departure, can be tuned for optimal performance. For instance, early experiments on thirty-two-bit machines suggested an arrival fan-in of four and a departure fan-out of two. Later work on sixty-four-bit machines demonstrated that leveraging single-byte writes and single-word spins for eight arrival-tree children simultaneously could be beneficial. The optimal departure fan-out, like many architectural parameters, is often machine-dependent. In scenarios where hardware supports broadcast-based cache coherence, the wakeup mechanism can be simplified by using a single global flag, leveraging the underlying hardware capabilities for efficient dissemination.

When considering the various types of barriers for a parallel application, one must carefully weigh the inherent trade-offs. The combining tree barrier, for example, often proves uncompetitive due to the substantial overhead associated with remote spinning and atomic fetch and phi operations on contemporary NUMA machines. These operations, while fundamental for shared memory synchronization, can incur significant latency if they involve off-chip or remote memory accesses. Similarly, the tournament barrier, while conceptually elegant, generally offers little performance advantage over the static tree barrier, which typically provides superior locality and reduced memory contention.

The centralized barrier, despite its simplicity where all threads synchronize against a single shared variable, often faces scalability challenges due to high contention as the number of threads increases. However, its simplicity makes it highly adaptable to applications where the number of threads may vary dynamically from one barrier episode to another. Furthermore, the centralized barrier is often the only viable option when implementing more advanced, less rigid synchronization paradigms, such as the fuzzy barrier technique, which permits some degree of temporal flexibility in thread synchronization.

The choice between the dissemination and static tree barriers comes down to a question of architectural features and costs. The dissemination barrier has the shortest critical path, but induces asymptotically more total network traffic. Given broadcast-based cache coherence, nothing is likely to outperform the static tree barrier, modified to use a global departure flag. In the absence of broadcast, the dissemination barrier will do better on a machine with high cross-sectional bandwidth; otherwise, the static tree barrier with explicit departure tree is likely to do better. When in doubt, practitioners would be wise to try both and measure their performance.

Table five point one presents tradeoffs among leading software barriers. Critical path lengths are expressed in remote memory references, assuming a broadcast on a CC-NUMA machine. These lengths may not correspond precisely to wall-clock time. Space needs are measured in words. Constants a and d in the static tree barrier represent arrival fan-in and departure fan-out, respectively. Fuzzy barriers are further discussed in section five point three point one.

The table details performance characteristics across three barrier types: central, dissemination, and static tree, for both CC-NUMA and NRC-NUMA architectures. For space needs, on a CC-NUMA architecture, the central barrier requires n plus one words, dissemination requires n plus two n times floor of log base two of n words, and the static tree needs four n plus one words. For NRC-NUMA, a central barrier requires infinity words, dissemination uses floor of log base two of n words, and the static tree needs five plus d times n words.

Regarding critical path length, for CC-NUMA, the central barrier has a length of n plus one, dissemination has floor of log base two of n, and the static tree has floor of log base a of n plus one. On an NRC-NUMA architecture, the central barrier's critical path length is infinity, dissemination is floor of log base two of n, and the static tree is floor of log base a of n plus floor of log base d of n.

For total remote references, under CC-NUMA, the central barrier ranges from n plus one to two n references, dissemination requires n times floor of log base two of n references, and the static tree needs n references. Under NRC-NUMA, the central barrier uses infinity references, dissemination uses n times floor of log base two of n, and the static tree uses two n minus two references.

In terms of fuzzy barrier suitability, for both CC-NUMA and NRC-NUMA architectures, the central barrier is suitable, indicated by a plus sign, while both dissemination and static tree barriers are not suitable, indicated by a minus sign.

Finally, for tolerance of changes in n, for both CC-NUMA and NRC-NUMA architectures, the central barrier shows tolerance, indicated by a plus sign, whereas dissemination and static tree barriers show no tolerance, indicated by a minus sign.

One of the principal performance problems associated with barriers is skew in thread arrival times, often caused by irregularities in the amount of work performed between barrier episodes. If one thread consistently does more work than the others, its arrival will naturally be delayed, and all other threads will wait. If variations in work are more randomly distributed, this situation is illustrated on the left side of Figure five point six, where the time between barrier episodes is repeatedly determined by a different slowest thread. If T subscript i comma r is the time thread i consumes in phase r of the computation, then the total execution time is the sum over r of the quantity t subscript b plus the maximum over all threads i in set T of T subscript i comma r, where t subscript b is the time required by a single barrier episode.

Fortunately, it often turns out that the work performed in one algorithmic phase depends on only some of the work performed by peers in previous phases. Imagine, for example, a program that repeatedly updates all the elements of a complex simulated system. It collects and logs information about the current state for subsequent off-line analysis, and then proceeds to the next step. If these logs are kept on a per-thread basis, we can start the next phase of simulation in a fast thread as soon as its peers have finished their local updates; we do not have to wait for them to finish their logging. This observation, first noted by Gupta in nineteen eighty-nine, leads to the design of a fuzzy barrier, in which arrival and departure are separate operations. This concept represents the standard idiom.

The fuzzy barrier mitigates the performance degradation caused by skew in thread arrival times at a synchronization point. In a strict barrier, the total execution time of a parallel phase is dictated by the slowest thread in that phase. This is formalized by the expression sum over r of parentheses t sub b plus max over i in T of T sub i comma r parentheses, where t sub b is the base time for a single barrier episode and T sub i comma r is the time consumed by thread i in phase r. The max operator clearly illustrates that the overall performance is bottlenecked by the maximum T sub i comma r across all threads.

Fuzzy barriers allow threads to proceed to subsequent work phases under certain conditions, even if not all peers have arrived. This is particularly valuable when the subsequent work is independent of the precise arrival of all other threads, or when threads can perform local updates that do not require global consistency immediately. The critical insight is the ability to separate the arrival and departure operations of a barrier. A thread might arrive by completing its current phase of work, but its departure, commencing the next phase, can be delayed or decoupled. This separation allows faster threads to perform useful work that might otherwise be idle waiting for slower threads, thereby enhancing overall system throughput and resource utilization by reducing the impact of arrival skew.
