The concept of atomicity in concurrent systems is built upon the foundational idea of sequential consistency. Sequential consistency requires that all memory accesses across multiple processing units appear to execute in a single, global total order, as if they were executed one at a time. Within this global order, each individual processing core's or thread's memory operations must still appear in the order specified by its own program.

A key question arises when considering high-level operations on a concurrent object: can sequential consistency simplify the design of such operations? The answer is generally negative in the absence of proper synchronization. Without explicit coordination mechanisms, sequentially consistent memory alone does not guarantee correct execution for concurrent code that modifies shared data. In fact, many modern systems employ more relaxed memory consistency models for performance, relying on well-defined synchronization primitives, such as fences or locks, to enforce necessary ordering and visibility among concurrent operations.

The correct construction of high-level concurrent objects often necessitates careful application of these synchronization mechanisms, even on memory systems with strong consistency guarantees. From the perspective of a memory architect, who is responsible for designing the complex interplay of load and store instructions across distributed cache-coherent interconnects, the goal is to provide a memory model that simplifies reasoning for software developers. Similarly, a designer of concurrent objects aims to make high-level operations appear atomic.

An operation is considered atomic if its internal steps, which might involve multiple low-level memory accesses, appear to complete instantaneously and indivisibly from the viewpoint of any other concurrent operation. This means that either all of its effects are visible, or none are; there is no intermediate state observable by other threads. Achieving this "all or nothing" property for high-level operations, even when composed of many smaller steps, is the essence of atomicity in this context.

Any execution trace where a high-level operation appears to complete as a single, indivisible unit, consistent with the per-thread program order, signifies an atomically executed operation. Expanding on this, a concurrent object, denoted as O, is said to be sequentially consistent if, for every possible execution trace, the sequence of operations applied to O can be reordered into some hypothetical total order. This total order must preserve the individual program order of operations within each thread and must yield the same return values and final state as if the operations had actually occurred in that exact, reconstructed total order.

However, a significant challenge arises when composing multiple high-level concurrent objects, even if each individually satisfies sequential consistency. This is the problem of lack of composability. Consider a multiprocessor memory system itself as a large, complex concurrent object; its individual memory access instructions are its fundamental methods. While such a system might be designed to enforce sequential consistency for its primitive memory operations, this property does not automatically extend to higher-level abstractions.

For instance, imagine one has implemented a concurrent object A, and rigorously proven that all its operations appear sequentially consistent when A is used in isolation within any program. Suppose a second concurrent object B also possesses this same guarantee when used alone. The critical and often problematic issue is that when a program utilizes both object A and object B concurrently, there is no inherent guarantee that the operations across both A and B will collectively appear in a single, overarching total order that remains consistent with each thread's individual program flow.

This non-composability of sequential consistency for high-level objects means that simply combining individually correct, sequentially consistent components does not automatically result in a correct, sequentially consistent composite system, often necessitating more complex verification or additional synchronization mechanisms. To address these complexities, the notion of linearizability was introduced. Linearizability establishes a stringent standard for consistency, requiring that an operation on a concurrent object appear to occur instantaneously at some unique point in time between its invocation and its return.

This means that despite the actual interleaving of concurrent operations, the system must behave as if all operations executed atomically, one after another, in a global total order. Critically, this total order must be consistent with the real-time ordering of non-overlapping operations and also preserve the program order of operations within each individual thread. The "instantaneously" clause of linearizability is crucial, as it precludes the visibility of partial updates.

For example, in a shared counter scenario, if threads observe different views of an update, it signifies a violation of linearizability. Similarly, if a `put` operation by a thread buffers data in software such that it is not immediately visible to other threads until much later, this also fails the linearizability criterion. It is essential to recognize that in parallel and distributed systems, there is no absolute, objective notion of global physical time.

What truly matters for consistency models, therefore, is the observable ordering of events. For an event to be said to occur at a single instant in time, it implies an impossibility for any other thread to observe that event has not yet occurred, while another thread simultaneously observes that the event has occurred. This is often exemplified by writing to a shared variable: if one thread writes a value, and another thread reads it, then the reading thread observing the new value means the write must have logically completed before the read, from a system-wide perspective.

To facilitate reasoning about the linearizability of a concurrent object, a conceptual construct known as the linearization point is typically identified within each method's execution. This linearization point marks the precise instant within the operation's call and return interval at which the operation logically takes effect and becomes atomic and globally visible. If we correctly assign these points, then whenever the linearization point of operation A precedes that of operation B, we can definitively state that operation A, as a whole, linearizes before operation B.

In the most straightforward case, where a method's entire execution is bracketed by the acquisition and subsequent release of a common object lock, the linearization point can conceptually be placed anywhere within that critical section. However, in more complex algorithms employing fine-grain locks, the determination of the linearization point becomes more nuanced, often corresponding to the specific moment of release of the particular lock that protects the critical part of the operation's state change.
