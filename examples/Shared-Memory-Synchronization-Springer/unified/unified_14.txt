In the realm of concurrent computing, ensuring data integrity and predictable behavior in shared memory systems is paramount, yet inherently complex. The concept of safety is central to this endeavor, particularly concerning how operations on shared data appear to be ordered. While two-phase locking is a well-established concurrency control mechanism that often suffices to achieve strict serializability, certain other implementations of what is termed "plain" serializability may not inherently provide the same strong guarantees.

Strict serializability is a demanding consistency model. It posits that all operations, even across different threads or processes, appear to execute in a single, total order, and this order must be consistent with the real time in which the operations actually occur. This means if operation A completes before operation B begins in real time, then A must appear before B in the serialized execution history. Plain serializability, on the other hand, only requires that a concurrent execution is equivalent to some serial execution, but does not impose the real time ordering constraint.

Two-phase locking enforces a strict order by requiring transactions to acquire all necessary locks before starting their "growing" phase and releasing them only after all operations are complete in their "shrinking" phase, thus preventing deadlocks and ensuring isolation. However, in certain high-performance applications, system designers may be willing to accept a limited amount of unintuitive behavior in return for greater performance. This leads to weaker consistency models.

One such model is quiescent consistency. The fundamental criterion for quiescent consistency dictates that operations on a concurrent object should appear to occur in some total order such that if operations A and B are separated by a quiescent interval, then A should appear to occur before B. A quiescent interval is defined as a period during which no operations on the object are in progress. This means that for a system to be quiescently consistent, it must present an observation where all activities related to prior operations on the object have completed before any subsequent operations begin.

Unlike the strong global guarantees of linearizability, quiescent consistency is fundamentally a local property. This means that if two concurrent objects each individually exhibit quiescent consistency, any program that interacts with both objects will observe their operations occurring in some single total order that respects the quiescent intervals of each object. However, a key limitation of quiescent consistency arises during non-quiescent intervals. In these periods, the model may fail to respect either the program order of operations within a single thread or the real time order of operations across multiple threads.

To illustrate how such a system might work, consider an object implementation that utilizes a lock to serialize its operations. When a thread acquires this lock, it might not immediately execute its requested operation. Instead, it could "stage" the operation, queuing it for later execution. If results are not immediately required, this staging allows the system to defer the actual execution. Suppose further that before releasing the lock, a thread is obligated to execute all pending, or "staged", operations. This convention can be viewed as a variation on the concept of flat combining, a technique that batches operations to be processed together by a single thread, often the one holding the lock.

A comprehensive understanding of these consistency models often benefits from a comparative analysis. Such an analysis would typically categorize relationships among the ordering criteria, differentiating them based on their guarantees for high-level object operations versus individual transactions across threads. In terms of a mental model for this comparison, one would envision rows representing these five consistency criteria, and columns detailing their specific properties or relationships.

For instance, a cell might indicate whether a criterion provides a total order on operations, respects program order, or guarantees real-time ordering. A system that correctly implements any of the first four criteria—sequential consistency, linearizability, serializability, or strict serializability—will inherently provide the appearance of a total order on operations. However, for quiescent consistency, the total order is specifically consistent with per-thread program order.

Specifically, linearizability and strict serializability inherently enforce consistency with real-time order, meaning the observed order of operations aligns with their actual wall clock timings. This is a critical distinction from plain serializability or sequential consistency, which only guarantee some valid serial ordering but not necessarily one that matches real time. Furthermore, quiescent consistency adds this critical property of consistency with real-time order, but only within the limited context of its quiescent intervals.

Serializability and strict serializability are the models that provide the crucial ability to define and guarantee multi-object atomic operations. This means that a series of operations across multiple distinct data objects can be treated as a single, indivisible unit. In contrast, linearizability and quiescent consistency are primarily local properties; they guarantee that operations on separate objects will always occur in a mutually consistent order, but they effectively "decline" to natively address or guarantee atomicity across multiple distinct objects as a primary feature.

Therefore, while quiescent consistency offers performance advantages through its relaxed ordering during non-quiescent periods, it sacrifices the strong, real-time, and multi-object atomicity guarantees provided by more stringent models like strict serializability, offering a trade-off that system designers carefully evaluate based on the application's specific requirements for correctness and performance.

The discussion then transitions to the concept of composability, particularly in the context of concurrent operations. In distributed systems and transactional memory environments, composability refers to the ability to combine individual, atomic or serializable operations into larger units that maintain their overall atomicity or serializability. This is a non-trivial challenge. While individual operations might guarantee certain consistency properties, their arbitrary composition does not automatically extend these guarantees to the composite operation.

For instance, combining two individually linearizable operations does not necessarily yield a linearizable composite operation without careful design. The text highlights that systems relying on speculation, such as some transactional memory implementations, find it difficult to support composable operations without resorting to more conservative locking strategies, which can limit concurrency. Conversely, a seemingly strong property like linearizability, while ensuring individual atomic operations, does not inherently facilitate the arbitrary composition of such operations into larger atomic units without explicit mechanisms.

The subsequent section introduces the critical concept of liveness, which, alongside safety properties, forms the bedrock of correctness in concurrent system design. Safety properties dictate that "bad things never happen," ensuring that the system always remains in a valid state. Examples include the absence of deadlocks, the preservation of atomicity for critical operations, and the maintenance of system invariants. While safety guarantees that nothing incorrect occurs, liveness ensures that "good things eventually happen," focusing on the forward progress of the system.

This includes ensuring that programs eventually produce results, operations complete, and threads continue to make progress rather than becoming stalled indefinitely. A method in a concurrent system is termed "blocking" if its execution can lead to a state where the thread invoking it is unable to proceed until some other thread performs a specific action. This often arises when a thread attempts to acquire a shared resource, such as a lock, that is currently held by another thread.

In such scenarios, the calling thread enters a waiting state, suspending its execution. Lock-based algorithms, by their very nature, are often inherently blocking because they rely on threads waiting for exclusive access to shared resources. Such blocking can lead to several undesirable outcomes, including deadlocks, where multiple threads are perpetually waiting for each other, and starvation, where a thread repeatedly loses contention for a resource and is never able to make progress.

To mitigate these risks, liveness proofs are crucial for lock-based algorithms. These proofs mathematically demonstrate that the algorithms are free of infinite loops and deadlocks, and that critical sections will eventually be entered, ensuring ongoing progress. In contrast, a method is considered "nonblocking" if there is no reachable state in which its invocation would prevent it from completing its execution and returning, irrespective of the actions or failures of other threads.

Nonblocking algorithms, typically built upon atomic primitives like compare and swap operations, ensure that at least one thread makes progress, even in the face of contention or the failure of other threads. This offers stronger guarantees for system robustness and availability, as the failure of one thread does not halt the progress of others. The theoretical underpinnings of nonblocking paradigms involve sophisticated techniques to manage shared state without relying on mutual exclusion locks, thereby enhancing concurrency and fault tolerance.
