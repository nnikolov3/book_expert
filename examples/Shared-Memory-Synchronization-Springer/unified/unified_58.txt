The text discusses advancements in Version-based Reclamation, or VBR, techniques, as presented by Sheffi et al. in two thousand twenty-one. These methods aim to enable concurrent read and write operations without the need for hardware prefetching, or HP techniques. It also references the work of Brown, specifically a two thousand seventeen publication, which provides a more comprehensive review of SMR algorithms. The latter publication delves into the compatibility of SMR algorithms with common data structures.

Section eight point eight introduces the concept of Dual Data Structures, focusing on the challenges of nonblocking progress in concurrent operations. When an operation is attempted on a data structure that is not in a valid state, or where a precondition is not met, progress might be hindered. The text posits that for operations like removal from containers, such as stacks or queues, the operation must be considered total. This means it should be valid and well-defined under any circumstances, including attempts to remove elements from an empty container. A typical approach to signal failure in such scenarios is by returning a special null or bottom symbol, denoted as perp.

However, the scenario becomes more complex when a thread encounters an empty container or an account with insufficient funds, leading to a need for condition synchronization. The question arises as to how to achieve condition synchronization without blocking progress. A straightforward, albeit potentially inefficient, approach is spinning. This involves repeatedly checking a condition until it is met. The pseudocode illustrates this with a loop: datum v repeat v is equal to my container remove until v is not equal to perp. This means a variable v is repeatedly assigned the result of my container remove until v is not equal to perp, indicating a successful removal.

The primary drawback of spinning is the consumption of CPU cycles and the potential for increased contention, especially when the data structure is frequently accessed or modified. A significant disadvantage arises when a new datum is finally inserted into an otherwise empty container. A thread that was spinning and successfully removes the datum might, due to the scheduler's interleaving of operations, inadvertently remove the newly inserted item. This behavior, occurring more or less accidentally based on the scheduler's timing, is a consequence of the data structure's methods not adequately handling synchronization.

To mitigate these issues, research by Scherer and Scott, cited as two thousand four, introduced the notion of nonblocking dual data structures. These structures are designed to avoid the performance penalties associated with spinning and to provide a more robust approach to concurrent operations. Their work proposes a formal framework for managing concurrent data structures. In this framework, when an operation encounters a precondition that is not met, it can insert a reservation. This reservation signifies an intent to complete the operation later. Another thread, upon detecting this reservation, is expected to notify the waiting thread when the precondition is eventually satisfied. The authors' framework aims to ensure that both the initial reservation and the subsequent successful completion of an operation are handled in a manner that is both nonblocking and linearizable. Intermediate activities, such as spinning or blocking, are designed to be effectively harmless, contributing to the overall correctness and efficiency of the concurrent data structure.

The text delves into the intricacies of nonblocking data structures, specifically focusing on dual-structure implementations and the concept of elimination. Initially, it references the work of Scherer and Scott concerning nonblocking dual versions of the Treiber stack and the M and S queue. The core challenge in these structures, whether performing a remove or an insert operation, is to achieve atomicity. An insert operation, for example, must correctly decide whether to insert a datum or fulfill a reservation. This decision-making process needs to be atomic, meaning it appears as a single, indivisible operation from the perspective of other threads. A critical aspect is ensuring that if an operation x on a data structure satisfies a precondition dependent on its successor t, then x must complete its operation within a bounded number of time steps, irrespective of other concurrent operations that might linearize between x's initial access and its completion.

The discussion then shifts to nonblocking dual queues. Atomicity in this context necessitates a mechanism to associate a tag with each node, indicating whether it contains a datum or a reservation. This tag is crucial for the consistent snapshotting of the queue. The fulfillment of a reservation in a dual queue is described as a process where a waiting thread attempts to change a field in the queue's data structure from a null value to the reservation itself, effectively using a compare and swap, or CAS, operation. This operation aims to update the queue state without blocking other threads. An alternative approach involves signaling a condition variable if a waiting thread is blocked.

The analysis extends to nonblocking dual stacks. Here, next pointers are also tagged, but the key difference from dual queues lies in the fact that insertions and deletions occur at the same end of the list. The absence of a dummy node introduces an additional step to ensure correct operation, requiring a request before popping an element. A potential issue arises if the thread performing the pop operation stalls after linearization but before the pop is fully executed. In such scenarios, an unbounded number of steps might be required for the pop, and other operations could potentially linearize in the interim. A push operation therefore pushes a data node regardless of the state of the stack. If the previous top of stack node was a reservation, the adjacent nodes then annihilate each other: any thread that finds a data node and an underlying reservation at the top of the stack attempts to write the address of the former into the latter, and then pop both nodes from the stack. In subsequent work, Izraelevitz and Scott, two thousand seventeen, presented dual versions of the lock-free, cache-oblivious, reservation-based queue, demonstrating their applicability to generic nonblocking containers for data paired with reservations.

Nonblocking dual data structures have proven quite useful in practice. In particular, the Executor framework of Java six used dual stacks and dual queues to replace the lock-based task pools of Java five, resulting in improvements of two to ten times in the throughput of thread dispatch. Scherer et al., two thousand nine, provide further insights into this implementation.

The principle of nonblocking elimination, as discussed in this text, is a sophisticated technique employed in concurrent data structures to reduce contention and enhance scalability. At its core, elimination leverages a temporary, auxiliary data structure, often referred to as an elimination array or elimination buffer, to facilitate the resolution of conflicts between operations that would otherwise require blocking or involve expensive atomic primitives like Compare And Swap, or CAS.

Consider a nonblocking stack implementation. When contention is low, threads can perform standard CAS operations on the top of the stack, which typically succeed with high probability. However, under high contention, these operations can frequently fail, leading to repeated attempts and wasted cycles. To mitigate this, Hendler et al. proposed an elimination strategy. When a thread attempting an operation, say a push, finds the top of the stack occupied and thus its CAS operation fails, it doesn't immediately retry. Instead, it can attempt to find a matching operation in the elimination array. For instance, a push operation could look for a pending pop operation, or vice versa.

The elimination array is conceptually partitioned. A thread might first attempt to access a slot at the beginning of this array, perhaps representing a small prefix. If it finds an empty slot, it parks its operation there for a limited duration, say time t. During this time, it hopes that another thread performing a complementary operation will find its parked operation and perform a hand-off, thereby resolving both operations without recourse to the main data structure. This hand-off typically involves an atomic exchange of data. If a matching operation is found, the two operations are eliminated, and the threads can complete successfully.

If, after a certain period t, no matching operation is found, the thread might dynamically adjust its strategy. This adjustment could involve shrinking the range of slots it probes in the elimination array, or conversely, expanding it if recent attempts to find a match were successful. Repeated failures to find a matching operation within the time interval t suggest that contention might be too high for the current strategy, prompting the thread to increase the size of the array prefix it searches or to employ a different back-off strategy. Conversely, if a thread successfully eliminates its operation, it might reduce the prefix size it searches in subsequent attempts. The frequency of failures and the factor by which the search range is adjusted are critical tuning parameters.

This concept of elimination is not limited to stacks. It has been applied to other data structures, such as queues. In queue implementations, an enqueue operation might be paired with a dequeue operation. If a dequeue operation is sufficiently old, meaning it was initiated some time ago and has not yet completed, it can be combined with a new enqueue operation. This temporal aspect is often managed using monotonically increasing serial numbers. When the count of operations at the head of the queue exceeds a certain threshold, an enqueue operation can be eliminated. This strategy, while potentially introducing a slight latency disadvantage for dequeues in certain scenarios, offers significant improvements in overall throughput and scalability, especially under heavy load. The First In, First Out, or FIFO, elimination mechanism is a notable example of this. Furthermore, elimination techniques have been adapted for priority queues, where operations with very small keys might eliminate other operations. The core idea remains the same: to provide a localized, temporary conflict resolution mechanism that bypasses the need for contention on the primary data structure. The efficiency of this approach hinges on the probability of finding a matching operation within the elimination buffer before its parking timeout expires.

In the context of nonblocking elimination, Hendler et al., two thousand four, use elimination in a nonblocking stack to back off adaptively in the wake of contention. As in a Treiber stack, Section eight point one, a thread can begin by attempting a CAS on the top of stack pointer. When contention is low, the CAS will generally succeed. If it fails, the thread chooses a slot in some subset of a separate elimination array. If it finds a matching operation already parked in that slot, the two exchange data and complete. If the slot is empty, the thread parks its own operation in it for some maximum time t, in hopes that a matching operation will arrive. Modifications to a slot—parking or eliminating—are made with CAS to resolve races among contending threads.

If a matching operation does not arrive in time, or if a thread finds a nonmatching operation in its chosen slot, for example, a push encounters another push, the thread attempts to access the top of stack pointer again. This process repeats—back and forth between the stack and the elimination array—until either a push pop CAS succeeds in the stack or an elimination CAS succeeds in the array. If recent past experience suggests that contention is high, a thread can go directly to the elimination array at the start of a new operation, rather than beginning with a top of stack CAS.

To increase the odds of success, threads dynamically adjust the subrange of the elimination array in which they operate. Repeated failure to find a matching operation within the time interval t causes a thread to use a smaller prefix of the array on its next iteration. Repeated failure to eliminate successfully given a matching operation, as can happen when some other operation manages to eliminate first, causes a thread to use a larger prefix. The value of t, the overall size of the array, the number of failures required to trigger a subrange change, and the factor by which it changes can all be tuned to maximize performance.

Similar techniques can be used for other abstractions in which operations may cancel out. Scherer et al., two thousand five, describe an exchange channel in which threads must pair up and swap information; a revised version of this code appears as the Exchanger class in the standard Java concurrency library. With care, elimination can also be applied to abstractions like queues, in which operations cannot naively eliminate in isolation. As shown by Moir et al., two thousand five, one can delay an enqueue operation until its datum, had it been inserted right away, would have reached the head of the queue: at that point it can safely combine with any arriving dequeue operation. To determine when an operation is sufficiently old, it suffices to augment the nodes of an M and S queue with monotonically increasing serial numbers. Each enqueue operation in the elimination array is augmented with the count found at the tail of the queue on the original failed CAS attempt. When the count at the head of the queue exceeds this value, the enqueue can safely be eliminated. This FIFO elimination has the nontrivial disadvantage of significantly increasing the latency of dequeue operations that encounter initial contention, but it can also significantly increase scalability and throughput under load. Elimination has also been implemented in priority queues, allowing delete Min operations and insert operations on very small keys to eliminate one another. Braginsky et al., two thousand sixteen, Calciu et al., two thousand fourteen.
