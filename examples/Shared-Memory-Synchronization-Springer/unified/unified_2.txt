In the context of concurrent systems, the terms spinning and blocking refer to two distinct strategies that threads may employ when waiting for a condition to become true. Spinning, also known as busy waiting, involves a thread continuously checking a condition in a tight loop, consuming central processing unit, or C P U, cycles without performing useful work. This approach avoids the overhead associated with context switching, which is the process of saving the current state of a running thread and loading the state of another thread. However, spinning can be highly inefficient if the condition remains false for an extended period, as it monopolizes a C P U core that could otherwise be utilized by another runnable thread.

On the other hand, blocking involves a thread explicitly yielding the C P U when a condition is not met. The thread is then moved to a waiting queue associated with that condition or resource, and the scheduler selects another thread to run. When the condition later becomes true, the blocked thread is made runnable again and eventually re-scheduled. This approach conserves C P U cycles by avoiding busy waiting but inevitably incurs the overhead of context switching.

The choice between spinning and blocking depends on the average time that a thread expects to wait. If the wait time is less than twice the context switch time, spinning will actually be faster than blocking. Additionally, spinning is the obvious choice if there is only one thread per core, as is sometimes the case in embedded or high-performance systems.

In modern operating systems, scheduling occurs at two different levels: kernel level and user level. The kernel-level scheduler implements kernel-level threads on top of a smaller number of processor cores, while the user-level scheduler implements user-level threads on top of a smaller number of kernel threads. Both kernel and user-level schedulers have similar internal structures and mechanisms for managing thread states and transitions.

The efficient and correct operation of any scheduler, whether kernel level or user level, critically depends on robust synchronization mechanisms. The internal data structures used by the scheduler itself, such as run queues, waiting lists, and thread control blocks, are shared resources that must be accessed atomically and consistently. This necessity for synchronization within the scheduler implies that even the scheduler's internal operations might involve a form of spinning or blocking to ensure data integrity.

To clarify the nomenclature, it is essential to distinguish between the closely related but distinct concepts of processes, threads, and tasks. A process is an independent execution environment that encapsulates its own dedicated virtual address space, containing the program code, data, and stack, along with system resources such as open files and I O channels. A thread, in its most common definition, is an active computation, representing a lightweight unit of execution within a process. Threads within the same process share the process's address space and system resources, allowing for efficient communication and data sharing. A task is a more general term, often used to describe a well-defined, typically small, unit of work to be accomplished.

In the realm of concurrent systems, ensuring correctness necessitates the careful consideration of two fundamental classes of properties: safety and liveness. Safety properties are invariants that assert "bad things never happen." For instance, in a critical section protected by a lock, a safety property dictates that at no point in time will two distinct threads concurrently occupy the critical section associated with the same lock. Liveness properties, on the other hand, address the concept of "good things eventually happening." They guarantee progress within a system, ensuring that if a lock is available, a thread waiting to acquire it will eventually succeed.

One of the simplest and most crucial liveness properties is livelock freedom, which asserts that threads will not execute indefinitely without making forward progress. A stronger notion of liveness is starvation freedom, which builds upon livelock freedom by introducing a fairness guarantee. It insists that if a thread attempts to acquire a lock, and the lock is eventually released by its current owner, then the thread will eventually be the one to acquire the lock.

The term "blocking" itself carries multiple meanings depending on the context within computer science. In an implementation-oriented sense, "blocking" is synonymous with "de-scheduling," where a thread gives up its C P U context, and its state is saved by the scheduler. Within a broader "systems" context, "blocking" often refers to an operation that waits for a response from another system component. However, to a theoretician, a thread that is actively spinning on a condition is not considered "blocked," as it is still consuming computational resources.

Most discussions of correctness will focus on safety properties, as they are typically easier to define and verify. Interestingly, deadlock freedom, which one might initially imagine to be a matter of liveness, is actually one of safety. Because deadlock can be described as a predicate that takes the current system state as input, deadlock freedom simply insists that the predicate be false in all reachable states. This classification stems from how the absence of deadlock is formally defined, emphasizing the importance of understanding the nuances between safety and liveness properties in the context of concurrent systems.
