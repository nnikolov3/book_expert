The K forty two variant of the M C S, or Mellor Crummey Scott, queued spin lock is a fundamental synchronization primitive in concurrent programming. This lock mechanism is designed to provide mutual exclusion for critical sections in multi-processor environments, while addressing the scalability issues inherent in simpler spin locks. The core concept revolves around a qnode record, which represents an individual participant in the lock's queue. Each qnode contains atomic pointers, including a tail pointer and a next pointer, as well as a special sentinel value, waiting, indicating that a thread associated with a particular qnode is still waiting for the lock.

The class lock encapsulates this mechanism, with an atomic qnode pointer representing the global lock state, initialized to null, null, signifying an initially free lock. The lock acquire function orchestrates the process of obtaining the lock, beginning with a continuous loop. Inside this loop, the current thread attempts to load the current tail of the queue, and if it is null, the function attempts to acquire the lock directly via a Compare And Swap operation. If this operation is successful, the current thread has acquired the lock and exits the loop.

If the lock is contended, the current thread creates a new qnode, which will serve as its entry in the waiting queue, and attempts to atomically compare and swap the global tail pointer from its previously observed value to point to the current thread's node. If this operation is successful, the current thread has successfully added itself to the end of the queue and then enters a while loop, spinning until its qnode's tail field is no longer equal to waiting, indicating that it has acquired the lock.

After acquiring the lock, the code prepares for the subsequent release by identifying its successor and updating the global lock state accordingly. The lock release function reverses the acquisition process, loading the next thread in the queue and attempting to atomically compare and swap the global tail pointer to null if there are no successors. If there are successors, the releasing thread updates the tail field of the successor's qnode, effectively releasing it.

Figure four point ten displays the K forty two variant of the M C S queued lock, highlighting the standard interface used for acquiring and releasing the lock. The lock's operation is further illustrated in Figure four point eleven, which shows the lock's state transitions as threads acquire and release the lock. The figure depicts a free lock, a single thread active in the critical section, and multiple threads arriving and waiting in the queue.

The C L H lock, or Craig, Landin, and Hagersten lock, represents a sophisticated approach to managing concurrent access in multi-processor systems. Its core innovation lies in bounding the number of remote memory accesses during lock acquisition, achieving a constant overhead regardless of system scale. The fundamental principle behind the C L H lock is the use of qnodes, where each thread attempting to acquire the lock is associated with its own dedicated qnode data structure.

When a thread seeks to acquire the lock, it conceptually enqueues itself by atomically linking its qnode into a system-wide logical queue. The thread then enters a localized spin loop, continuously monitoring a flag within its predecessor's qnode. This spinning is highly efficient because the predecessor's qnode resides either in the current thread's local cache or a closely associated memory region, minimizing costly remote cache line transfers and maintaining cache coherence locally.

To release the lock, a thread simply modifies the state of its own qnode, specifically setting its succ must wait flag to false. This action inherently notifies the next thread in the queue, its successor, which has been spinning on this very qnode, that the lock is now available. The C L H lock ensures F I F O fairness, as threads acquire the lock in the strict order they requested it.

Early analyses highlighted a potential efficiency concern, an extra handshake involved in the initial C L H design. This handshake referred to the requirement for a newly arriving thread to write the address of its qnode into its predecessor's qnode, and for the predecessor to wait for that write to complete before it could release the lock. To address this, an optimization was proposed, allowing each thread to spin on a memory location local to its processor, a crucial consideration for Numa architectures.

Further evolutions of the C L H lock, notably the L H and M lock variants, refine its behavior. The L H lock is presented as being conceptually identical to the C L H, with distinctions primarily arising in the precise mechanisms of passing qnodes during acquire and release operations. The M lock variant further optimizes the uncontended lock acquisition scenario by reducing the number of cache misses. While the M lock may utilize an atomic Compare And Swap operation to resolve contention during acquisition, the C L H lock, in its core form, primarily relies on atomic swap operations for pointer manipulation, demonstrating its efficiency without the need for the more complex C A S primitive in all cases.

Craig's original work explored several extensions to the C L H lock, including introducing an additional level of indirection to eliminate remote spinning, making it suitable even for N R C-Numa machines, without requiring C A S and without compromising strict F I F O ordering or wait-free entry properties. By maintaining a doubly linked list of qnodes, allowing both forward and backward traversal, the system can arrange to grant the lock based on an external notion of order or priority, moving beyond simple F I F O if desired, while still preserving the localized spinning advantage that defines the C L H lock's efficiency. These enhancements underscore the versatility and robustness of queue-based lock designs in managing concurrency challenges in highly scalable multi-core environments.
