A data race free program exhibits sequential consistency, meaning all memory accesses appear to occur in a global total order that respects the program order within each thread and is also consistent with the order of any given transaction.

Challenges in language integration for transactional memory systems arise from several sources. One significant aspect is the enforcement of transactionality at compile time or runtime. The question of whether a subroutine is transaction safe, and how this safety is communicated, is crucial. If a subroutine is not transaction safe, its calls within a transaction could lead to unpredictable behavior. Furthermore, surprise interactions with other language features, particularly exceptions, present a complex problem. When an exception occurs within a transaction, the propagation of that exception across the transaction boundary must be carefully handled. If the exception signals an error that requires state restoration, the transaction commit might not be able to guarantee the restoration of program invariants. This leads to the question of how to raise an exception within a transaction if it's considered to have never happened. Some researchers argue that transactions should fundamentally be atomic and that speculation, a technique often used in transactional memory to improve performance by executing tentative operations, should be an implementation detail rather than a core semantic concern.

Guerraoui and Kapalka, in their work from two thousand nine, formalized the issue of forward progress, which essentially means the absence of deadlocks or livelocks in a transactional system. They introduced the concept of progressiveness. A transactional memory system is considered weakly progressive if a transaction that encounters no conflicts is always able to commit. A system is deemed strongly progressive if conflicts among transactions are limited to a single variable, or perhaps a single object in a more general sense, and if progressiveness is guaranteed, it implies that speculation is not only a mechanism for performance but also a component of the formal semantics. The definition of conflict itself becomes important, as does the potential for false conflicts arising from the granularity of transactional operations and their mappings to underlying hardware structures.

The distinction between strong and weak isolation in transactional memory is significant. As noted earlier, most hardware transactional memory, or H T M, systems aim for strong atomicity, serializing not only transactions but also individual loads and stores. However, some researchers suggest that language level transactional memory should guarantee strong atomicity as well, though its implementation can be challenging, especially within software. The difference between strong and weak atomicity becomes apparent when considering data races, which are problematic interleavings of accesses by different threads that can lead to incorrect program behavior. In particular, data races between transactional and non transactional accesses, or even between two non transactional accesses, are considered bugs. If these data races are a primary concern and difficult to manage, then the distinction between strong atomicity and other forms becomes crucial for diagnosing and debugging concurrent programs.

Interestingly, progressiveness is largely orthogonal to nonblocking progress. A nonblocking T M system may allow a thread to make progress within the S T M implementation while still not providing progressiveness at the level of complete transactions. Perhaps the strongest argument for nonblocking S T M is its ability to accommodate event based code, in which a handler may need to make progress, again, within the S T M implementation, even when a main program thread is in the middle of a conflicting transaction.

When adding transactions to a programming language, one may want or need to include a variety of features not yet discussed in this chapter. One significant extension is nesting. The concept of composability, a key advantage of transactions over lock based critical sections, necessitates allowing transactions to nest. The simplest way to achieve this is through a flattening approach, where inner transactions are subsumed into the outer transaction, allowing them to commit or abort together. All current commercial H T M implementations support a form of subsumption nesting, often with a bounded depth limit.

For performance reasons, it may be desirable to permit transactions to abort and retry while retaining the work done so far. This is known as true or closed nesting, and it requires that any system supporting it must also allow a transaction to abort and not retry. Such behavior can also arise in languages that provide explicit abort commands. Furthermore, for both performance and generality, it is beneficial to enable concurrent transactions to cooperate in computationally demanding operations, committing their results atomically. In certain scenarios, it might even be advantageous to allow an inner transaction to commit independently of the outer transaction, a concept that can potentially violate serializability, and thus requires careful handling with appropriate safeguards.

The final topic discussed is condition synchronization. Similar to lock based critical sections, transactions can depend on preconditions that may or may not be met. However, a transaction cannot wait for a precondition in the same way a critical section can, because it is isolated and changes to the world made by other threads will not be visible to it. There is an analogy here to nonblocking operations, which cannot wait and still be nonblocking. The analogy suggests a potential solution: insist that transactions be total, that their preconditions always be true, but allow them to commit reservation notices in the style of dual data structures. If, say, a dequeue operation on a transactional queue finds no data to remove, it can enqueue a reservation atomically instead, and return an indication that it has done so. The surrounding code can then wait for the reservation to be satisfied in normal, non-transactional code.

A second alternative, suggested by Smaragdakis et al. in two thousand seven, is to suspend, or punctuate, a transaction at a conditional wait, and to make the sections of the transaction before and after the wait individually, but not jointly, atomic. This alternative requires, of course, that any invariants maintained by the transaction be true at the punctuation point. If a wait may be nested inside called routines, the fact that they may wait probably needs to be an explicit part of their interface.

Perhaps the most appealing approach to transactional condition synchronization is the retry primitive of Harris et al. in two thousand five. When executed by a transaction, it indicates that the current operation cannot proceed, and should abort, to be retried at some future time. Exactly when to retry is a question reminiscent of conditional critical regions. There is a particularly elegant answer for S T M: The transaction is sure to behave the same the next time around if it reads the same values from memory. Therefore, it should become a visible reader of every location in its read set, and wait for one of those locations to be modified by another transaction. Modification by non-transactional code would imply the existence of a data race. The wakeup mechanism for condition synchronization is then essentially the same as the abort mechanism for visible readers, and can share the same implementation.

Transactional memory is not the only potential use of speculation. Given a speculation and rollback mechanism implemented for T M, we may consider using it for other things as well. Possibilities include try blocks that roll back to their original state instead of stopping where they are when an exception arises. Shinnar et al. in two thousand four refer to such blocks as try all. They are supported explicitly in hardware on Power eight, as rollback only, non-isolated transactions.

Another application of speculation is in automatic or semi-automatic, programmer-hint-driven, parallelization of semantically sequential loops. In such a loop, each iteration is essentially a transaction, but conflicts are always resolved in favor of the earlier iteration, and no iteration is permitted to commit until all its predecessors have done so. Blue Gene Q provided explicit hardware support for such ordered speculation. It can also be used for other purposes, such as improving the performance of certain types of computations by allowing them to proceed speculatively and then rolling back if necessary.

The text delves into techniques for managing transactional memory, particularly addressing the challenge of transactions that cannot immediately proceed due to dependencies on other operations. One proposed solution involves a form of conditional waiting, drawing an analogy to nonblocking operations. In this approach, transactions would ensure their preconditions are always met, but they could reserve access to data. If a dequeue operation, for instance, finds no data, it could atomically enqueue a reservation for the data it expects, allowing other threads to proceed. The surrounding code would then wait for this reservation to be satisfied, in contrast to normal, non-transactional execution.

A second strategy suggests suspending a transaction conditionally, making sections of the transaction atomic without being jointly atomic. The key here is that the wait condition itself needs to be evaluated at a precise point in execution. If a wait is necessary, the transaction could effectively punctuate itself, recording its state and requiring a subsequent reevaluation. This approach necessitates that any invariants maintained by the transaction are managed, possibly by nesting the waiting logic within called routines.

The most compelling approach to transactional condition synchronization is attributed to Harris et al. in two thousand five. When a transaction encounters a condition that prevents its immediate progress, it can defer its execution and schedule a retry at a later time. This mechanism is akin to conditional critical regions, providing an elegant solution for Software Transactional Memory, or S T M. The transaction can proceed optimistically, assuming its read set will not be modified by other transactions. Should a modification occur to data it has read, or if the condition it was waiting for is still not met, the transaction can abort and reschedule. This abort and retry mechanism is essential for ensuring correctness. The concept of a visible reader is also introduced, implying that modifications made within a transaction might not be immediately visible to other threads until the transaction commits. The synchronization mechanism for conditional execution is described as being similar to, and potentially sharing implementation details with, the abort mechanism.

The document then transitions to other uses of speculation, noting that transactional memory's speculative capabilities extend beyond simple condition synchronization. It highlights the potential for implementing speculative execution in other scenarios. One such possibility involves try blocks that rollback to their original state upon an exception, rather than simply stopping. These are supported in hardware, as seen in systems like Power eight.

Another application of speculation is in automatic or semi-automatic parallelization of semantically sequential loops, as described by Ding et al. in two thousand seven and Berger et al. in two thousand nine. In these loops, each iteration is treated as a transaction. Conflicts between iterations are resolved in favor of the earlier iteration, and a transaction is not permitted to commit until all its predecessor transactions have successfully committed. This creates an ordered execution flow. The text mentions that systems like Blue Gene Q provided explicit hardware support for such ordered speculation, enabling efficient parallel execution of inherently sequential loop structures through transactional mechanisms.
