Four point five Special Case Optimizations. 

In the context of multi-processor environments, particularly those exhibiting Non-Uniform Memory Access, or N U M A, characteristics, the efficient management of shared data structures is a fundamental challenge. When a thread accesses a shared data structure, the relevant cache lines must be brought into the acquiring core's cache, incurring a performance cost that is substantially lower if the destination core is within the same N U M A cluster as the data's current owner. This highlights the critical importance of data locality.

Early observations by Radovic and Hagersten in two thousand two emphasized the significance of maintaining locality when dealing with locks, which are synchronization primitives used to protect shared data from concurrent access. They proposed the R H lock, designed for a two-cluster N U M A machine, which conceptually consists of a pair of test and set operations, one for each N U M A cluster. A test and set is an atomic instruction that reads the current value of a memory location and simultaneously writes a new value to it, ensuring that this read-modify-write sequence appears as a single, indivisible operation to other concurrent processors.

The R H lock could easily be adapted to larger numbers of clusters, but space consumption would be linear in the number of such clusters, a property considered undesirable by Radovic and Hagersten. Their subsequent Hierarchical Backoff, or H B O, lock relies on statistics instead, implementing a test and set lock with compare and swap, or C A S, in such a way that the lock variable indicates the cluster in which the lock currently resides. Nearby and remote threads then use different backoff parameters, so that nearby threads are more likely than remote threads to acquire the lock when it is released.

While a test and set lock is naturally unfair and subject to the theoretical possibility of starvation, the R H and H B O locks are likely to be even less fair in practice. Ideally, one should like to be able to explicitly balance fairness against locality. Toward that end, Dice and colleagues in two thousand twelve presented a general N U M A-aware design pattern that can be used with almost any underlying locks, including First In First Out, or F I F O, queued locks. Their cohort locking mechanism employs a global lock that indicates which cluster currently owns the lock, and a local lock for each cluster that indicates the owning thread.

The global lock needs to allow release to be called by a different thread from the one that called acquire, and the local lock needs to be able to tell, at release time, whether any other local thread is waiting. Apart from these requirements, cohort locking can be used with any known form of lock. Experimental results indicate particularly high throughput and excellent fairness, subject to locality, using Mellor-Crummey-Scott, or M C S, locks at both the global and cluster level.

The techniques discussed here improve locality only by controlling the order in which threads acquire a lock. However, it is also possible to control which threads perform the operations protected by the lock and to assign operations that access similar data to the same thread, minimizing cache misses. Such locality-conscious allocation of work can yield major performance benefits in systems that assign fine-grain computational tasks to worker threads.

In addition to these optimizations, another important concept is lazy initialization, where the creation of an object or resource is deferred until it is first accessed or required. This approach conserves system resources, such as memory and C P U cycles, by avoiding the allocation and construction of objects that might never be utilized during program execution. A basic, thread-safe lazy initialization pattern involves a shared pointer initialized to null and a lock. The function that returns an instance of the object first acquires the lock, checks if the object is null, and if so, creates it and assigns it to the shared pointer before releasing the lock.

However, this naive approach imposes the overhead of acquiring the lock on every call, even after the object has been successfully initialized. To mitigate this performance overhead, the double-checked locking idiom was devised. This pattern attempts to reduce contention on the lock by performing an initial check for null outside the synchronized block. If the initial check reveals that the object is not null, the thread can proceed without acquiring the lock, thereby avoiding the synchronization overhead.

The synchronizing accesses in this idiom are critical, and without explicit ordering, the initializing thread may set the pointer to point to not-yet-initialized space, or a reading thread may use fields of the object that were prefetched before initialization completed. On machines with highly relaxed memory models, the cost of the synchronizing accesses may be comparable to the cost of locking in the original version of the code, making the optimization of limited benefit. On machines with a Total Store Order, or T S O, memory model, the optimization is much more appealing, since read-read, read-write, and write-write orderings are essentially free.

Used judiciously, double-checked locking can yield significant performance benefits, but it has proven to be a significant source of bugs, particularly when dealing with nuances of different memory models and compiler optimizations. Modern programming languages and operating systems often provide higher-level, safer abstractions for one-time initialization, such as the InitOnce A P I in Windows Vista, which encapsulates all necessary synchronization and memory ordering guarantees.

Another optimization technique is asymmetric locking, which addresses the common challenge in multi-threaded applications where many data structures are predominantly accessed by a single, designated thread, yet require robust synchronization to handle occasional access from other threads. The core principle behind asymmetric locking is to introduce a bias towards this preferred thread, ensuring that its lock acquisition and release operations are extremely fast, ideally involving minimal overhead or even being completely lock-free in the common case.

The proposed solution often adapts classic software-based synchronization algorithms, such as Peterson's Algorithm, which fundamentally guarantees mutual exclusion for two concurrent processes using only atomic load and store operations. In this asymmetric context, Peterson's algorithm is conceptualized to arbitrate between the preferred thread and a representative of all other non-preferred threads. The key to the efficiency and correctness of the slow or non-preferred path is described as a handshake operation, which ensures proper synchronization and memory visibility even with relaxed memory models, typically involving specific read-modify-write atomic operations or sequences of memory barrier-protected loads and stores.

This complex interaction on the slow path guarantees global consistency while preserving the performance advantage for the common, fast path of the preferred thread. Asymmetric locking is particularly relevant in contexts like the Java Virtual Machine, where objects may belong to specific threads or require synchronization during garbage collection re-entry or interactions with native code. The HotSpot Java Virtual Machine uses biased locks to accommodate objects that appear to belong to a single thread and to control re-entry to the J V M by threads that have escaped to native code and may need to synchronize with a garbage collection cycle that began while they were absent.
