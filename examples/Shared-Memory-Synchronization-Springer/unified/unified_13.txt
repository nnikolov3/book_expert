In the realm of concurrent systems, ensuring correctness amidst numerous possible thread interleavings is a foundational challenge. Nonblocking algorithms, a class of concurrent algorithms, are designed to guarantee system-wide progress, meaning a halt in one thread does not impede the progress of others. Achieving provable correctness for such algorithms necessitates a rigorous formal framework, one that can account for the myriad ways operations from different threads might interleave. This often involves associating a specific linearization point with each operation. The linearization point is a conceptual instantaneous moment during an operation's execution where its effect on the shared state becomes globally visible and takes effect atomically. Any memory updates visible before this point are considered preparatory, while those observed afterwards are viewed as cleanup, effectively ensuring that the operation appears as an indivisible action.

For instance, in a nonblocking stack, a successful push or pop operation might linearize at the point of its final Compare And Swap instruction, whereas an unsuccessful pop might linearize at the load operation that identifies its failure. In more complex concurrent methods, the process of identifying these linearization points can become intricate, potentially requiring dynamic run-time checks or a determination based on the behavior of other threads. The ultimate goal is to establish a total order of all linearized operations that remains consistent with the object's sequential semantics.

This concept of linearizability is powerful because it allows us to prove that in every possible concurrent program execution, the operations appear to occur in some single total order that is consistent with each individual thread's program order and with any other ordering that threads are able to observe. This property, often described as local linearizability, implies that the linearizability of a composite system can be reasoned about directly from the linearizability of its constituent parts, a principle that significantly aids in the modular design and verification of complex concurrent data structures.

As a practical illustration of achieving linearizability through fine-grain synchronization, consider the technique known as Hand Over Hand Locking, or Lock Coupling. This approach is particularly effective when parallelizing operations on dynamic data structures like a sorted, singly-linked list, which supports operations such as insertion, removal, and lookup. Without proper synchronization, concurrent modifications could easily corrupt the list's integrity. While a global lock would trivially ensure correctness by serializing all operations, it would severely limit concurrency.

Hand Over Hand Locking offers a more performant alternative by allowing threads to operate on different parts of the list concurrently. The core mechanism of Hand Over Hand Locking involves a thread acquiring a lock on a successor node before releasing the lock on its current node during list traversal or modification. This ensures that at any given moment, a thread typically holds at most two locks: one on the current node and one on the next node it intends to access or modify. This careful acquisition and release pattern is critical for maintaining the list's structural consistency.

It guarantees that as a thread navigates or alters the list, no other thread can break the chain of nodes it is operating upon. By retaining the lock on the current node while acquiring the lock on the successor, a thread prevents any race conditions that might lead to a loss of data or structural corruption, such as when one thread attempts to delete a node that another thread is simultaneously trying to insert or link to.

To elaborate on this through a conceptual scenario, imagine thread one is inserting a new node with value C, while concurrently thread two is attempting to remove a node with value D from the same list. A simple, unsynchronized interleaving could lead to a situation where if thread one executes its final linking operation between thread two's acquisition of a lock on node D and its subsequent removal, the node C might become detached or lost. The Hand Over Hand protocol explicitly prevents this by mandating that thread one, for instance, would need to hold locks on both its predecessor node, say A, and the node that will become C's successor, D, throughout the insertion process.

Similarly, thread two, to safely remove D, would be required to acquire locks on the nodes immediately preceding and succeeding D. This locking discipline ensures that the structural integrity of the list is preserved throughout the concurrent operations, providing the strong guarantee of linearizability for these complex list manipulations. Any lookup operation on C by another thread would also necessarily acquire the appropriate locks to traverse the path to C, reinforcing the coordinated access.

The discussion then transitions to the broader concept of Serializability, a critical property for ensuring correctness in concurrent systems, particularly for transactional operations. The core idea is atomicity, which dictates that an operation must appear to complete instantaneously at a single, distinct point in time, and its effects become immediately visible to all other threads. This prevents partial or inconsistent states from being observed.

Linearizability is introduced as a stronger consistency model that builds upon atomicity. It applies to concurrent objects and guarantees that the observable order of operations on a given concurrent object is consistent with their real-time execution order. In other words, if an operation A completes before operation B begins, then A's effects must be visible to B. This provides a strong guarantee of "real-time" ordering for individual object operations.

However, the text critically highlights that linearizability for individual operations does not necessarily extend to operations that manipulate more than one object. This distinction is paramount in the context of transactions, which often involve operations across multiple distinct data elements. The banking system example starkly illustrates this. In a scenario where thread one transfers one hundred dollars from account A to account B, and concurrently thread two attempts to calculate the sum of balances in accounts A and B.

Initially, suppose both A's balance and B's balance are five hundred. Thread one's transfer would involve withdrawing one hundred from account A, making A's balance four hundred, and depositing one hundred into account B, making B's balance six hundred. The correct total system balance should remain one thousand. Thread two, however, reads A's balance and B's balance sequentially. If thread one's withdrawal completes, making A's balance four hundred, and thread two then reads A's balance as four hundred, but thread one's deposit has not yet completed, and thread two reads B's balance as its original five hundred, then thread two calculates a sum of four hundred plus five hundred, which is nine hundred. This sum is one hundred dollars "too low" compared to the true total of one thousand, which would be observed if the entire transfer operation by thread one were atomic from thread two's perspective.

The inconsistency arises because the "transfer" itself is not treated as a single atomic unit spanning multiple objects, even if individual withdrawal and deposit operations on their respective accounts are linearizable. This problem underscores the need for transactional atomicity or serializability at a higher level, encompassing multiple objects or operations, to maintain global system invariants. Such guarantees often necessitate more sophisticated concurrency control mechanisms like two-phase locking or multi-version concurrency control.

Multi-object atomic operations are the hallmark of database systems, which refer to them as transactions. Transactional memory adapts transactions to shared-memory parallel computing, allowing the programmer to request that a multi-object operation execute atomically. The simplest ordering criterion for transactions is known as serializability. Transactions are said to serialize if they have the same effect they would have had if executed one at a time in some total order.

For transactional memory, and sometimes for databases as well, we can extend the model to allow a thread to perform a series of transactions and require that the global order be consistent with program order in each thread. It turns out to be NP-hard to determine whether a given set of transactions is serializable. Fortunately, we seldom need to make such a determination in practice. Generally, all we really want is to ensure that the current execution will be serializable, something we can achieve with conservative measures.

A global lock is a trivial solution but admits no concurrency. Databases and most transactional memory systems employ more elaborate fine-grain locking mechanisms or nonblocking techniques to maximize parallelism. When dealing with concurrent access to shared resources, it becomes clear that a transaction might attempt to access resources before it fully knows which ones it will ultimately require. Implementing serializability through fine-grain locks introduces complexities such as exclusive use, hold and wait, and circularity, all of which are direct precursors to deadlock.

To mitigate deadlock, a transactional memory system or a lock-based database system must be engineered to detect and resolve such conditions. This often involves mechanisms like releasing held locks, rolling back partial transactions, and reattempting conflicting transactions. Some advanced transactional memory systems leverage a strategy akin to branch prediction or compare-and-swap-based fetch-and-increment, allowing transactions to proceed with operations even if they are temporarily non-serializable, with the optimistic hope that these operations will ultimately converge to a serializable state.

If conflicts arise, the system recovers by rolling back the affected transactions. So-called lazy transactional memory systems take this concept further, deferring the commit decision for potentially conflicting transactions and allowing them to execute in parallel until one of them is ready to commit, at which point any conflicting transactions are aborted and rolled back. An illustrative example of fine-grain locking for achieving serializability is Two-Phase Locking. In this protocol, a transaction operates in two distinct phases: a growing phase, where it acquires all necessary locks but cannot release any, and a shrinking phase, where it releases locks but cannot acquire any new ones.

To visualize this in a simple scenario, consider two transactions, both attempting to read and then update shared, symmetric variables. Under Two-Phase Locking, each transaction would acquire locks on the variables it needs to read and update during its growing phase. Once all required locks are obtained, it enters the shrinking phase, performing its updates and then releasing the locks. The strict separation of lock acquisition and release ensures that any concurrent execution of transactions following this protocol will be equivalent to some serial order, thus guaranteeing serializability and maintaining data consistency.
