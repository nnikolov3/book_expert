Memory consistency represents a fundamental challenge in the design and operation of modern parallel computing systems. It defines the rules governing the order in which memory operations, specifically reads and writes, appear to complete to different processors in a multi-core or multi-processor system. Without strong consistency guarantees, the intuitive behavior of programs written for a single processor can break down when executed concurrently.

Figure two point three, titled 'An apparent ordering loop', depicts a scenario involving two threads and their interaction with shared variables x and y, and local variables i and j. Initially, both x is equal to zero and y is equal to zero. Thread one consists of two operations: first, line one assigns the value one to variable x; second, line two assigns the value of variable y to variable i. Thread two also consists of two operations: first, line one assigns the value one to variable y; second, line two assigns the value of variable x to variable j. The diagram includes crossed arrows between the threads, signifying an inquiry or problem related to the concurrent execution order. The final state indicated is that both i is equal to zero and j is equal to zero, implying a specific, potentially unexpected, execution outcome for the loop.

When executing a load instruction, a core checks the contents of its reorder and store buffers before forwarding a request to the memory system. This check ensures that the core always sees its own recent writes, even if they have not yet made their way to cache or memory. At the same time, a load that accesses a location that has not been written recently may make its way to memory before logically previous instructions that wrote to other locations. This fact is harmless on a uniprocessor, but consider the implications on a parallel machine, as shown in Figure two point three. If the write to x is delayed in thread one's store buffer, and the write to y is similarly delayed in thread two's store buffer, then both threads may read a zero at line two, suggesting that line two of thread one executes before line one of thread two, and line two of thread two executes before line one of thread one. When combined with program order, line one in each thread should execute before line two in the same thread, this gives us an apparent "ordering loop," which "should" be logically impossible.

Similar problems can occur deeper in the memory hierarchy. A modern machine can require several hundred cycles to service a miss that goes all the way to L one, L two, L three to bus, and so on. Pending requests may be buffered in a queue. If multiple requests may be active simultaneously, as is common, at least, on the global interconnect, and if some requests may complete more quickly than others, then memory accesses may appear to be reordered. So long as accesses to the same location by the same thread are forced to occur in order, single-threaded code will run correctly. On a multiprocessor, however, sequential consistency may again be violated.

On a N U M A machine, or a machine with a topologically complex interconnect, differing distances among locations provide additional sources of circular ordering. If variable x in Figure two point three is close to thread two but far from thread one, and y is close to thread one but far from thread two, the reads on line two can easily complete before the writes on line one, even if all accesses are inserted into the memory system in program order. With a topologically complex interconnect, the cache coherence protocol itself may introduce variable delays, for example, to dispatch invalidation requests to the various locations that may need to change the state of a local cache line, and to collect acknowledgments. Again, these differing delays may allow line two of the example, in both threads, to complete before line one.

In all the explanations of Figure two point three, the ordering loop results from reads bypassing writes, executing in order from the perspective of the issuing core, but out of order from the perspective of the memory system, or of threads on other cores. On N U M A or topologically complex machines, it may also be possible for reads to bypass reads, writes to bypass reads, or writes to bypass writes. Worse, circularity may arise even without bypassing, that is, even when every thread executes its own instructions in strict program order. Consider the "independent reads of independent writes" example shown in Figure two point four. If thread one is close to thread two but far from thread three, and thread four is close to thread three but far from thread two, the reads on line one in threads two and three may see the new values of x and y, while the reads on line two see the old. Here the problem is not bypassing, but a lack of write atomicity, one thread sees the value written by a store and another thread subsequently sees the value prior to the store.

The concept of memory consistency is further complicated by the fact that compilers also reorder instructions. In any program not written in machine code, compilers perform a variety of optimizations in an attempt to improve performance. Simple examples include reordering computations to expose and eliminate redundancies, hoisting invariants out of loops, and scheduling instructions to minimize processor pipeline bubbles. Such optimizations are legal so long as they respect control and data dependencies within a single thread. Like the hardware optimizations discussed, compiler optimizations can lead to inconsistent behavior when more than one thread is involved. As we shall see, a language designed for concurrent programming must provide a memory model that explains allowable behavior, and some set of primitives, typically special synchronization operations or reads and writes of special atomic variables, that serve to order accesses at the language level.

To address these memory inconsistencies and enable reliable synchronization, specialized instructions are crucial. These instructions, often termed memory barriers or fences, enforce an explicit ordering on memory operations, preventing the hardware from reordering them across the barrier. For instance, in a flag-based programming idiom, such barriers ensure that updates to shared data are fully visible before a flag indicating readiness is set, and that the flag is observed before the data is accessed. Without these explicit ordering mechanisms, synchronization primitives built on simple read and write operations can fail catastrophically.

The term "barrier" itself is overloaded in computer science, leading to potential confusion. While "memory barrier" or "memory fence" refers specifically to instructions that enforce memory operation ordering, the word "barrier" is also used more broadly. For instance, in parallel programming, a "synchronization barrier" is a point in a program where all participating threads must arrive before any thread is allowed to proceed. This is a higher-level synchronization primitive often built upon memory barriers. In programming language runtimes, particularly for garbage collection, "write barriers" or "read barriers" are mechanisms that track pointer changes or object accesses to ensure the consistency of the memory heap during garbage collection cycles. Similarly, in the context of atomic transactions, "barriers" may refer to the explicit boundaries of transactions, where atomicity, consistency, isolation, and durability properties are enforced. Finally, in modern CPU architectures, "barriers" can relate to internal processor mechanisms that manage speculative execution and ensure correct state recovery after branch mispredictions. The precise meaning of "barrier" therefore heavily depends on its specific technical context.

In the context of Figure two point five, a simple example of flag-based synchronization is presented. To avoid a spurious error, the update to x must be visible to thread two before the update to f. If the write at line two in thread one can bypass the write in line one, however, thread two may read x too early, and see a value of zero. Similarly, if the read of x at line three in thread two can bypass the read of f in line one, a divide-by-zero may again occur, even if the writes in thread one complete in order. While thread two's read of x is separated from the read of f by a conditional test, the second read may still issue before the first completes, if the branch predictor guesses that the loop will never iterate.

Any machine that is not sequentially consistent will provide special instructions that allow the programmer to force consistent ordering in situations in which it matters, but in which the hardware might not otherwise guarantee it. Perhaps the simplest such instruction is a synchronizing access, typically a special load or store, that is guaranteed to be both locally and globally ordered. Here, "locally ordered" means that the synchronizing access will appear to occur after any preceding, ordinary or synchronizing, accesses in its own thread, and before any subsequent, ordinary or synchronizing, accesses in its thread, from the perspective of all threads. "Globally ordered" means that the synchronizing access will appear to occur in some consistent, total order with respect to all other synchronizing instructions in the program, from the perspective of all threads.

To avoid the spurious error in Figure two point five, it is sufficient, though not necessary, to use fully, locally and globally, ordered accesses to f in both threads, thereby ensuring that thread one's update of x happens before its update of f, and thread two's read of x happens after it sees the update to f. This can be achieved by inserting a memory barrier between the x write and the f write in thread one, and another memory barrier between the read of f and the read of x in thread two, effectively synchronizing the visibility of x across threads. These barriers force the memory system to complete and make visible all preceding operations before any subsequent operations can proceed past the barrier, from the perspective of other processors.
