Section five point two, Barrier Algorithms, describes a crucial synchronization mechanism in parallel computing. The algorithm defines a node record type with several fields, including const int k, which represents the fan-in of this node, atomic int count initialized to k, atomic bool sense initialized to false, and node pointer const parent, initialized appropriately for the tree structure.

A barrier class is also defined, which includes a boolean array called local sense, indexed by T, initialized with true values, and an array of node pointers called my leaf, indexed by T, serving as a pointer to the starting node for each thread. The initialization process for the barrier must create a tree of nodes, with each node residing in its own cache line, and nodes linked by parent pointers.

The barrier dot cycle method performs several steps. First, it issues a memory fence for read write or write operations. Then, it calls a combining helper function, passing my leaf index self and local sense index self as arguments, intended to join the barrier. Next, it updates local sense index self by inverting its current boolean value for the next barrier cycle. Finally, it issues another memory fence for read or read write operations.

The combining helper function takes a node pointer n and a boolean my sense as arguments. Inside this function, it checks if the result of a fetch and decrement operation on n's count, with write or write memory order, is equal to one. If this condition is true, it indicates that the current thread is the last one to reach this node. In this case, it performs further actions, including recursively calling combining helper on n's parent, storing n's k value into n's count with a write memory order, and storing the inverted value of n's sense into n's sense with a write memory order.

If the initial condition is false, the thread enters a spin loop, continuously loading n's sense with a read memory order, and continues spinning as long as n's sense is not equal to my sense. This loop causes the thread to wait until the node's sense variable flips to match its own my sense value, indicating that the barrier has been released by the last thread that traversed up to the root and propagated the release signal back down.

Figure five point two shows a software combining tree barrier. The concept of a software combining tree barrier is useful in a p processor machine where near simultaneous requests to the same memory location can occur. For example, if two operations, fetch and add a and fetch and add b, land in the same internal queue at about the same point in time, they would be forwarded on as a single fetch and add a plus b operation.

While hardware combining tends not to appear on modern machines, Yew and others, in nineteen eighty seven, observed that similar benefits could be achieved with an explicit tree in software. A shared variable that is expected to be the target of multiple concurrent accesses is represented as a tree of variables, with each node in the tree assigned to a different cache line. Threads are divided into groups, with one group assigned to each leaf of the tree. Each thread updates the state in its leaf, and if it discovers that it is the last thread in its group to do so, it continues up the tree and updates its parent to reflect the collective updates to the child.

Using a software combining tree, Tang and Yew, in nineteen ninety, showed how to create a log time barrier. Writes into one tree are used to determine that all threads have reached the barrier, and reads out of a second are used to allow them to continue. The tree based structure offers significant performance advantages by distributing the contention across multiple nodes in a tree, instead of concentrating it on a single shared variable.

The dissemination barrier algorithm, described in section five point two point three, reduces barrier latency by eliminating the separation between arrival and departure. The algorithm proceeds through log base two n unsynchronized rounds. In round k, each thread i signals thread i plus two to the power of k, modulo n. This resulting pattern ensures that by the end of the final round, every thread has heard, directly or indirectly, from every other thread.

The code for the dissemination barrier appears in figure five point four. The algorithm uses alternating sets of variables, selected based on parity, in consecutive barrier episodes, avoiding interference without requiring two separate spins in each round. It also uses sense reversal to avoid resetting variables after every episode. The flags on which each thread spins are statically determined, allowing them to be local even on an N R C Numa machine, and no two threads ever spin on the same flag.

Interestingly, while the critical path length of the dissemination barrier is log base two n, the total amount of interconnect traffic, representing remote writes, is n times log base two n. Space requirements are also big O of n log n. This is asymptotically larger than the big O of n space and bandwidth of the centralized and combining tree barriers, and may be a problem on machines whose interconnection networks have limited cross sectional bandwidth.

Section five point two point four, Non combining Tree Barriers, discusses an alternative approach. While the potential cost of fetch and phi operations is an argument against the combining tree barrier, it turns out not to be an argument against tree barriers in general. Hensgen and others, in nineteen eighty eight, and Lubachevsky, in nineteen eighty nine, observe that one can eliminate the need for fetch and phi by choosing the winner at each tree node in advance. As in a combining tree, this allows simpler, non combining operations to suffice, highlighting a fundamental design principle in parallel algorithm optimization: transforming expensive, contention prone atomic operations into less contention prone, more distributed patterns by carefully orchestrating communication and state transitions.
