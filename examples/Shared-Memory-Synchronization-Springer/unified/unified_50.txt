The A B P algorithm is both simple and clever, with very low constant time overhead in the common case, and is very widely used. However, it has two important limitations. First, it uses a bounded array, which limits the number of tasks that can be pushed into the deque at any given time. Second, when tasks are stolen via pop left operations, the space cannot be reclaimed until the deque is empty, that is, the left end index resets to zero only when the local thread bumps into it in the course of a pop right operation. 

Chase and Lev present an A B P extension that addresses both limitations. It treats the array as circular, avoiding the reset problem, and it allows the array to be resized on overflow, much like an extensible hash table. In an attempt to improve performance when workloads are unevenly distributed, Hendler and Shavit describe another A B P extension in which a thread can, in a single operation, steal up to half the elements from a peer's deque. Many additional extensions and alternatives can be found in the literature, as work stealing remains an active topic of research.

The discussion then transitions to hash tables, focusing on Michael's contribution of a nonblocking hash table with external chaining. In this design, each bucket within the hash table points to a linked list, enabling efficient lookup, insertion, and deletion operations without requiring mutual exclusion locks. A significant challenge for such an approach is the lack of an a priori estimate for the table size, necessitating an extensible hash table. To manage concurrency and ensure safety during table resizing, a single sequence lock is employed to serialize ordinary lookup, insert, and delete operations. However, resizing operations themselves are considered writer operations. While this strategy maintains safety, it introduces blocking for other operations during a resize.

To mitigate this, the use of read copy update, or R C U, is proposed. R C U allows lookup operations to proceed concurrently with resizing, deferring the reclamation of old table memory until all readers have finished. Insert and delete operations, however, must still wait for resizing to complete. The ideal scenario, as described by Shalev and Shavit, is a nonblocking resizing operation that allows all other operations to continue unimpeded. Their algorithm achieves this objective by spreading the cost of resizing across multiple insert, delete, and lookup operations, while maintaining an expected constant time for these operations.

The underlying principle involves organizing data within buckets using sorted lists, where the order is determined by a specific ordering attribute of the nodes. For instance, a hash function generating values within a range of zero to two to the power of n minus one can be used to determine the bucket, with the order determined by the bitwise representation of the hash key. Accessing elements within these lists can be further optimized by using lazily initialized buckets, where the indexing mechanism can dynamically adjust based on the number of elements, potentially increasing at runtime.

This illustration demonstrates a nonblocking, extensible Sieve and Search, or S and S, hash table. The core concept here is how to manage dynamic resizing and concurrent access without traditional locking mechanisms, which can lead to performance bottlenecks. Shaded nodes represent dummy nodes, which serve as markers or placeholders in the linked lists that form the buckets of the hash table. Data nodes, depicted as white boxes, contain actual keys, which are associated with hash values shown above them.

The diagrams illustrate the progression of operations on this data structure. The initial state shows a hash table with a certain number of bits used for hashing, thereby determining the number of buckets. The buckets are indexed from zero to three. The linked list within each bucket is ordered based on the hash values of the keys. The insertion of a data node with a hash value of nine is depicted, which falls in bucket one and is inserted according to its order number between the nodes with hash values seventeen and five.

The expansion of the hash table's addressing capability is shown, indicated by a three bit hash. This implies that more bits of the hash values are now being used to determine the bucket index, effectively doubling the number of conceptual buckets. The insertion of a node with hash value twenty one is also shown, demonstrating how it finds its place in the expanded table. The search for a node with hash value thirty is illustrated, which requires initialization of bucket six, and recursively, bucket two.

The accompanying text provides further technical details, explaining that when the number of elements grows beyond a certain limit, the hash table can dynamically increase its capacity. This is often achieved by doubling the number of buckets. The process involves re-distributing existing elements into the newly created buckets. The text mentions the concept of a dummy node linked into the list immediately before the data nodes whose top j bits, when reversed, determine the bucket index. This dummy node's value is derived by reversing the j bits of the bucket's hash, padding with zeros to match the total number of bits, and then adding an extra least significant zero bit.

This bit manipulation is a key technique for ensuring that during a resize operation, all old buckets continue to point to the correct locations, and new buckets are properly initialized. The approach is central to building robust concurrent data structures that can scale efficiently. The S and S hash table can only increase in size, as it offers no mechanism for contracting the table. This limitation is lifted by Liu et al., who present two new resizable hashing algorithms, one lock free and one wait free. The key idea is to implement each bucket as a freezable set, which offers operations to attempt an insertion or deletion and to permanently freeze a bucket, preventing further modifications.

An auxiliary data structure, such as a list or array, is then used to index the buckets, and a collaborative helping mechanism is used to resize the auxiliary data structure. More sophisticated hashing algorithms have also been implemented in a lock free way, such as hopscotch hashing, which outperforms traditional probing based hashing algorithms. The text concludes by highlighting the importance of efficient and scalable hash table designs, particularly in the context of concurrent and parallel computing systems. 

Section eight point four delves into the intricacies of Hash Tables, specifically examining techniques for managing dynamic growth and efficient insertion operations. The text begins by illustrating a scenario with nodes interleaved, leading to a demonstration of a hash table containing elements with hash values five, fifteen, sixteen, and seventeen. The assumed range of hash values is zero to thirty one. For simplicity, the analysis focuses on a node with hash value five, represented by the binary order number binary one zero one zero one two.

This node's hash value, specifically its two low order bits, are used to index into an array of two squared, or four, buckets. Buckets at indices zero, one, and three are shown to contain pointers to dummy nodes, indicating they are allocated but not yet holding active data. All data nodes encountered are described as contiguous within a node list, and immediately follow the dummy node corresponding to their bucket. The process of inserting four initial nodes is then detailed, with the first node having a hash value of nine, which is inserted into bucket one.

Subsequently, nodes with hash values seventeen and five are inserted. Following this, an increment operation is performed, and the number of buckets in use is doubled. Crucially, the presented approach emphasizes lazy initialization of buckets, which is particularly beneficial given that the number of buckets can grow exponentially. The discussion then refers to a simplified scheme by Shalev and Shavit, which employs noncontiguous bucket arrays. This scheme utilizes a single, second level directory, which serves to provide access to bucket arrays. These arrays grow in size exponentially, with the first two arrays being of size two to the power of i, where i is two, and the subsequent array being of size two to the power of i plus one.

The growth pattern is presented in the context of handling a key k, where the bucket index b is computed as k modulo two to the power of j, and d is determined by shifting b right by i bits. If d is zero, the bucket is located at directory index zero, b mod two to the power of i. Otherwise, the bucket is found at directory index m plus one, where m is the most significant bit in d's binary representation. The text then details the insertion of a new data node with hash value twenty one, which requires initialization of bucket twenty one mod two to the power of three, which equals five.

The parent of bucket five, identified by its order number binary one zero zero zero zero two, is located between buckets with hash values nine and five. After inserting this node, a search is performed for a node with hash value thirty, which maps to bucket six using the operation thirty modulo two cubed, resulting in six. This bucket is described as recursively initiating bucket two, which itself requires initialization by zeroing the parent's portion of bucket one. The authors, Shalev and Shavit, are credited with proving that the entire algorithm, under reasonable assumptions regarding the hash function h, results in constant amortized cost for insert, delete, and lookup operations.

The section concludes with a discussion of More Recent Developments, highlighting a limitation in the described hash table: it can only increase in size, lacking a mechanism for contracting. This is contrasted with work by Liu and colleagues, who introduced two new resizable hashing algorithms. Their key innovation involves implementing each bucket as a freezable set, which allows operations to freeze a bucket, preventing further modifications during insertion or deletion. An auxiliary data structure, such as a list or array, is then used to index these buckets. The mechanism for resizing this auxiliary data structure is employed to facilitate table resizing.

Furthermore, more sophisticated hashing algorithms, such as hopscotch hashing, have been implemented in a lock free manner, outperforming traditional probing based methods, as demonstrated by Kelly and colleagues. The importance of efficient and scalable hash table designs is emphasized, particularly in the context of concurrent and parallel computing systems, where dynamic growth and efficient insertion operations are crucial for maintaining performance and responsiveness.
